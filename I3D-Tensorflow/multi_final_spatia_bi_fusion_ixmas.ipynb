{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/requests/__init__.py:83: RequestsDependencyWarning: Old version of cryptography ([1, 2, 3]) may cause slowdown.\n",
      "  warnings.warn(warning, RequestsDependencyWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import tensorflow as tf\n",
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import pdb\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "print(tf.__version__)\n",
    "import scipy.io as sio\n",
    "import math\n",
    "\n",
    "############### Global Parameters ###############\n",
    "# path\n",
    "video_list  = sio.loadmat('trainlist_ix2.mat')\n",
    "test_list = sio.loadmat('testlist_ix2.mat')\n",
    "\n",
    "# train_path = './experiments/ixmas/rgb_data_split2/train/'\n",
    "train_path = './experiments/ixmas/rgb_scratch_split2/train/'\n",
    "# train_path_2 = './experiments/ixmas/flow_data_split2/train/'\n",
    "train_path_2 = './experiments/ixmas/flow_scratch_split2/train/'\n",
    "# test_path = './experiments/ixmas/rgb_data_split2/test/'\n",
    "test_path = './experiments/ixmas/rgb_scratch_split2/test/'\n",
    "# test_path_2 = './experiments/ixmas/flow_data_split2/test/'\n",
    "test_path_2 = './experiments/ixmas/flow_scratch_split2/test/'\n",
    "train_path_3 = './experiments/ixmas/flowt_data_split2/train/'\n",
    "\n",
    "test_path_3 = './experiments/ixmas/flowt_data_split2/test/'\n",
    "\n",
    "# Model Path\n",
    "default_model_path = './multi_model/ixmas_fusion_bi_scratch/final_model'\n",
    "save_path = './multi_model/ixmas_fusion_bi_scratch/'\n",
    "# default_model_path = './multi_model/ixmas_fusion_bi/final_model'\n",
    "# save_path = './multi_model/ixmas_fusion_bi/'\n",
    "# Output Path\n",
    "output_path = './ixmas_fusion_bi_scratch_res/'\n",
    "\n",
    "# Train Test Split\n",
    "train_num = video_list['video'][0].shape[0]-1\n",
    "test_num = test_list['video'][0].shape[0]-1\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([u'chiara3_01_check-watch_cam2_frames_0029_0093'], dtype='<U44')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_list ['video'][0][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Train Parameters #################\n",
    "# Parameters\n",
    "learning_rate = 0.00001\n",
    "n_epochs = 100\n",
    "batch_size = 1\n",
    "# Network Parameters\n",
    "n_input = 1024 # input dimension\n",
    "n_hidden = 1024 # hidden layer num of multi_head\n",
    "n_classes = 12 # has 51 classes\n",
    "n_frames = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_dist(A, B):\n",
    "    assert A.shape.as_list() == B.shape.as_list()\n",
    "\n",
    "    row_norms_A = tf.reduce_sum(tf.square(A), axis=1)\n",
    "    row_norms_A = tf.reshape(row_norms_A, [-1, 1])  # Column vector.\n",
    "\n",
    "    row_norms_B = tf.reduce_sum(tf.square(B), axis=1)\n",
    "    row_norms_B = tf.reshape(row_norms_B, [1, -1])  # Row vector.\n",
    "\n",
    "    return row_norms_A - 2 * tf.matmul(A, tf.transpose(B)) + row_norms_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(inputs, \n",
    "              epsilon = 1e-8,\n",
    "              scope=\"ln\",\n",
    "              reuse=None):\n",
    "    '''Applies layer normalization.\n",
    "    \n",
    "    Args:\n",
    "      inputs: A tensor with 2 or more dimensions, where the first dimension has\n",
    "        `batch_size`.\n",
    "      epsilon: A floating number. A very small number for preventing ZeroDivision Error.\n",
    "      scope: Optional scope for `variable_scope`.\n",
    "      reuse: Boolean, whether to reuse the weights of a previous layer\n",
    "        by the same name.\n",
    "      \n",
    "    Returns:\n",
    "      A tensor with the same shape and data dtype as `inputs`.\n",
    "    '''\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        inputs_shape = inputs.get_shape()\n",
    "        params_shape = inputs_shape[-1:]\n",
    "    \n",
    "        mean, variance = tf.nn.moments(inputs, [-1], keep_dims=True)\n",
    "        beta= tf.Variable(tf.zeros(params_shape))\n",
    "        gamma = tf.Variable(tf.ones(params_shape))\n",
    "        normalized = (inputs - mean) / ( (variance + epsilon) ** (.5) )\n",
    "        outputs = gamma * normalized + beta\n",
    "        \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multihead_attention(queries, \n",
    "                        keys,\n",
    "                        wkeys, \n",
    "                        num_units=None, \n",
    "                        num_heads=8, \n",
    "                        dropout_rate=0,\n",
    "                        is_training=True,\n",
    "                        causality=False,\n",
    "                        trainable=True,\n",
    "                        scope=\"multihead_attention\", \n",
    "                        reuse=None):\n",
    "    '''Applies multihead attention.\n",
    "    \n",
    "    Args:\n",
    "      queries: A 3d tensor with shape of [N, T_q, C_q].\n",
    "      keys: A 3d tensor with shape of [N, T_k, C_k].\n",
    "      num_units: A scalar. Attention size.\n",
    "      dropout_rate: A floating point number.\n",
    "      is_training: Boolean. Controller of mechanism for dropout.\n",
    "      causality: Boolean. If true, units that reference the future are masked. \n",
    "      num_heads: An int. Number of heads.\n",
    "      scope: Optional scope for `variable_scope`.\n",
    "      reuse: Boolean, whether to reuse the weights of a previous layer\n",
    "        by the same name.\n",
    "        \n",
    "    Returns\n",
    "      A 3d tensor with shape of (N, T_q, C)  \n",
    "    '''\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        # Set the fall back option for num_units\n",
    "        if num_units is None:\n",
    "            num_units = queries.get_shape().as_list[-1]\n",
    "        \n",
    "        # Linear projections\n",
    "        Q = tf.contrib.layers.fully_connected(queries, num_units,trainable=trainable) # (N, T_q, C)\n",
    "        K = tf.contrib.layers.fully_connected(keys, num_units,trainable=trainable ) # (N, T_k, C)\n",
    "        V = tf.contrib.layers.fully_connected(keys, num_units,trainable=trainable) # (N, T_k, C)\n",
    "        \n",
    "        Q1 = tf.reshape(Q,(1,n_frames,num_units))\n",
    "        K1 = tf.reshape(K,(1,n_frames,num_units))        \n",
    "        V1 = tf.reshape(V,(1,n_frames,num_units))\n",
    "        \n",
    "        \n",
    "        # Split and concat\n",
    "        Q_ = tf.concat(tf.split(Q1, num_heads, axis = 2),axis =0) # (h*N, T_q, C/h) \n",
    "        K_ = tf.concat(tf.split(K1, num_heads, axis = 2),axis =0) # (h*N, T_k, C/h) \n",
    "        V_ = tf.concat(tf.split(V1, num_heads, axis = 2),axis =0) # (h*N, T_k, C/h) \n",
    "\n",
    "        \n",
    "        # Multiplication\n",
    "        #outputs = tf.matmul(Q_, tf.transpose(K_, [0, 2, 1])) # (h*N, T_q, T_k)\n",
    "        output_1 = tf.matmul(Q_, wkeys) # (h*N, T_q, T_k)\n",
    "        outputs  = tf.matmul(output_1, tf.transpose(K_, [0, 2, 1])) # (h*N, T_q, T_k)\n",
    "\n",
    "        \n",
    "        # Scale\n",
    "        outputs = outputs / (K_.get_shape().as_list()[-1] ** 0.5)\n",
    "\n",
    "                \n",
    "        # Key Masking\n",
    "        key_masks = tf.sign(tf.abs(tf.reduce_sum(keys, axis=-1))) # (N, T_k)\n",
    "        key_masks = tf.tile(key_masks, [num_heads, 1]) # (h*N, T_k)\n",
    "        key_masks = tf.tile(tf.expand_dims(key_masks, 1), [1, tf.shape(queries)[1], 1]) # (h*N, T_q, T_k)\n",
    "       # \n",
    "        paddings = tf.ones_like(outputs)*(-2**32+1)\n",
    "        outputs = tf.where(tf.equal(key_masks, 0), paddings, outputs) # (h*N, T_q, T_k)\n",
    "          \n",
    "        # Causality = Future blinding\n",
    "        if causality:\n",
    "            diag_vals = tf.ones_like(outputs[0, :, :]) # (T_q, T_k)\n",
    "            tril = tf.linalg.LinearOperatorLowerTriangular(diag_vals).to_dense() # (T_q, T_k)\n",
    "            masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(outputs)[0], 1, 1]) # (h*N, T_q, T_k)\n",
    "  # \n",
    "            paddings = tf.ones_like(masks)*(-2**32+1)\n",
    "            outputs = tf.where(tf.equal(masks, 0), paddings, outputs) # (h*N, T_q, T_k)\n",
    "  \n",
    "        # Activation\n",
    "        outputs = tf.nn.softmax(outputs) # (h*N, T_q, T_k)\n",
    "        matt    = outputs\n",
    "        \n",
    "        \n",
    "        # Query Masking\n",
    "        query_masks = tf.sign(tf.abs(tf.reduce_sum(queries, axis=-1))) # (N, T_q)\n",
    "        query_masks = tf.tile(query_masks, [num_heads, 1]) # (h*N, T_q)\n",
    "        query_masks = tf.tile(tf.expand_dims(query_masks, -1), [1, 1, tf.shape(keys)[1]]) # (h*N, T_q, T_k)\n",
    "        outputs *= query_masks # broadcasting. (N, T_q, C)\n",
    "          \n",
    "        # Dropouts\n",
    "        outputs = tf.contrib.layers.dropout(outputs, keep_prob=dropout_rate, is_training=tf.convert_to_tensor(is_training))\n",
    "               \n",
    "        # Weighted sum\n",
    "        outputs = tf.matmul(outputs, V_) # ( h*N, T_q, C/h)\n",
    "        \n",
    "        # Restore shape\n",
    "        outputs = tf.concat(tf.split(outputs, num_heads,axis = 0),axis =2 ) # (N, T_q, C)\n",
    "              \n",
    "        # Residual connection\n",
    "        outputs += queries\n",
    "              \n",
    "        # Normalize\n",
    "        outputs = normalize(outputs) # (N, T_q, C)\n",
    " \n",
    "    return outputs, matt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "\n",
    "    # tf Graph input\n",
    "    x   = tf.placeholder(\"float\", [None, n_frames , n_input])\n",
    "    x2  = tf.placeholder(\"float\", [None, n_frames , n_input])\n",
    "    x3  = tf.placeholder(\"float\", [None, n_frames , n_input])\n",
    "    y  = tf.placeholder(\"float\", [None, n_classes])\n",
    "    # Define weights\n",
    "    weights = {\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden, n_classes], mean=0.0, stddev=0.01)),\n",
    "        'wa1': tf.Variable(tf.random_normal([2*n_hidden, 1], mean=0.0, stddev=0.01)),\n",
    "        'wa2': tf.Variable(tf.random_normal([2*n_hidden, 1], mean=0.0, stddev=0.01)),\n",
    "        'wa3': tf.Variable(tf.random_normal([2*n_hidden, 1], mean=0.0, stddev=0.01)),\n",
    "        'att_wk': tf.Variable(tf.random_normal([8,n_input/8, n_input/8], mean=0.0, stddev=0.02),trainable=True),\n",
    "        'att_wk2': tf.Variable(tf.random_normal([8,n_input/8, n_input/8], mean=0.0, stddev=0.02),trainable=True),\n",
    "        'att_wk3': tf.Variable(tf.random_normal([8,n_input/8, n_input/8], mean=0.0, stddev=0.02),trainable=True),\n",
    "        'att_wkb': tf.Variable(tf.random_normal([8,n_input/8, n_input/8], mean=0.0, stddev=0.02),trainable=True),\n",
    "        'att_wk2b': tf.Variable(tf.random_normal([8,n_input/8, n_input/8], mean=0.0, stddev=0.02),trainable=True),\n",
    "        'att_wk3b': tf.Variable(tf.random_normal([8,n_input/8, n_input/8], mean=0.0, stddev=0.02),trainable=True),\n",
    "        'att_wk4': tf.Variable(tf.random_normal([8,n_input/4, n_input/4], mean=0.0, stddev=0.02),trainable=True)\n",
    "    }\n",
    "    biases = {\n",
    "        'out': tf.Variable(tf.random_normal([n_classes], mean=0.0, stddev=0.01))\n",
    "\n",
    "\n",
    "    }\n",
    "#     m1 = tf.get_variable('m1',dtype=tf.float32, shape = [1])\n",
    "#     m2 = tf.get_variable('m2',dtype=tf.float32, shape = [1])\n",
    "#     m3 = tf.get_variable('m3',dtype=tf.float32, shape = [1])\n",
    "#     m1 = tf.sigmoid(m1)\n",
    "#     m2 = tf.sigmoid(m2)\n",
    "#     m3 = tf.sigmoid(m3)\n",
    "    \n",
    "    # init loss \n",
    "    loss = 0.0  \n",
    "    # Mask \n",
    "    regularizer = tf.contrib.layers.l1_regularizer(scale=0.0005)\n",
    "    reg_term    = tf.contrib.layers.apply_regularization(regularizer,[weights['att_wk'],weights['att_wk2']])\n",
    "    \n",
    "    # Start creat graph\n",
    "    X  = tf.reshape(x,(1,n_frames,n_input))\n",
    "    X2 = tf.reshape(x2,(1,n_frames,n_input))\n",
    "    X3 = tf.reshape(x3,(1,n_frames,n_input))\n",
    "            \n",
    "    # attention weighting  \n",
    "    wkeys1  = weights['att_wk']\n",
    "    wkeys2  = weights['att_wk2']    \n",
    "    wkeys3  = weights['att_wk3']\n",
    "    wkeys1b = weights['att_wkb']\n",
    "    wkeys2b = weights['att_wk2b']    \n",
    "    wkeys3b = weights['att_wk3b'] \n",
    "    wkeys4  = weights['att_wk4'] \n",
    "    # Forward   \n",
    "    multi_att, matt =  multihead_attention(queries=X, keys=X, wkeys = wkeys1, num_units=n_input, \n",
    "                                                        num_heads=8, \n",
    "                                                        dropout_rate=0.9,\n",
    "                                                        is_training=1,\n",
    "                                                        causality=True, \n",
    "                                                        scope=\"self_attention_1\") \n",
    "    \n",
    "    # Reverse \n",
    "    XR = X[:,::-1,:]\n",
    "    multi_attb, mattb =  multihead_attention(queries=XR, keys=XR, wkeys = wkeys1b, num_units=n_input, \n",
    "                                                        num_heads=8, \n",
    "                                                        dropout_rate=0.9,\n",
    "                                                        is_training=1,\n",
    "                                                        causality=True, \n",
    "                                                        scope=\"self_attention_1b\")\n",
    "    \n",
    "    multi_bi1=  tf.concat((multi_att,multi_attb[:,::-1,:]),axis = 2 )\n",
    "    \n",
    "    \n",
    "    # Forward   \n",
    "    multi_att2, matt2 =  multihead_attention(queries=X2, keys=X2, wkeys = wkeys2, num_units=n_input, \n",
    "                                                        num_heads=8, \n",
    "                                                        dropout_rate=0.9,\n",
    "                                                        is_training=1,\n",
    "                                                        causality=True, \n",
    "                                                        scope=\"self_attention_2\") \n",
    "    \n",
    "    # Reverse \n",
    "    XR2 = X2[:,::-1,:]\n",
    "    multi_att2b, _ =  multihead_attention(queries=XR2, keys=XR2, wkeys = wkeys2b, num_units=n_input, \n",
    "                                                        num_heads=8, \n",
    "                                                        dropout_rate=0.9,\n",
    "                                                        is_training=1,\n",
    "                                                        causality=True, \n",
    "                                                        scope=\"self_attention_2b\")\n",
    "    \n",
    "    multi_bi2=  tf.concat((multi_att2,multi_att2b[:,::-1,:]),axis = 2 )\n",
    "    \n",
    "     # Forward   \n",
    "    multi_att3, matt3 =  multihead_attention(queries=X3, keys=X3, wkeys = wkeys3, num_units=n_input, \n",
    "                                                        num_heads=8, \n",
    "                                                        dropout_rate=0.9,\n",
    "                                                        is_training=1,\n",
    "                                                        causality=True, \n",
    "                                                        scope=\"self_attention_3\") \n",
    "    \n",
    "    # Reverse \n",
    "    XR3 = X3[:,::-1,:]\n",
    "    multi_att3b, _ =  multihead_attention(queries=XR3, keys=XR3, wkeys = wkeys3b, num_units=n_input, \n",
    "                                                        num_heads=8, \n",
    "                                                        dropout_rate=0.9,\n",
    "                                                        is_training=1,\n",
    "                                                        causality=True, \n",
    "                                                        scope=\"self_attention_3b\")\n",
    "    \n",
    "    multi_bi3=  tf.concat((multi_att3,multi_att3b[:,::-1,:]),axis = 2 )\n",
    "    \n",
    "    \n",
    "#    multi_bid = tf.multiply(m1,multi_bi1) + tf.multiply(m2,multi_bi2) + tf.multiply(m3, multi_bi3)\n",
    "    multi_bid = multi_bi1 + multi_bi2 + multi_bi3 \n",
    "    \n",
    "#     multi_bid, mattbd =  multihead_attention(queries=multi_bi, keys=multi_bi, wkeys = wkeys4, num_units=2*n_input, \n",
    "#                                                         num_heads=8, \n",
    "#                                                         dropout_rate=0.9,\n",
    "#                                                         is_training=1,\n",
    "#                                                         causality=False, \n",
    "#                                                         scope=\"self_attention_4\")\n",
    "\n",
    "    w1 = tf.matmul(tf.squeeze(multi_bi1),weights['wa1'])\n",
    "    w2 = tf.matmul(tf.squeeze(multi_bi2),weights['wa2'])\n",
    "    w3 = tf.matmul(tf.squeeze(multi_bi3),weights['wa3'])\n",
    "    \n",
    "    with tf.variable_scope(\"fc_1\") as fc_1:\n",
    "            fc_1 = tf.contrib.layers.fully_connected(tf.reshape(multi_bid,(n_frames,2*n_hidden)), 3, activation_fn=tf.nn.relu)\n",
    "            fc_1_drop = tf.nn.dropout(fc_1, 0.8)\n",
    "            fc_1_variables =  [v for v in tf.global_variables() if v.name.startswith(fc_1.name)] \n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(n_frames):                  \n",
    "        with tf.variable_scope('model',reuse=tf.AUTO_REUSE):\n",
    "            mul1 =  tf.reshape(multi_bi1[0,i,:],(1,2*n_hidden))\n",
    "            mul2 =  tf.reshape(multi_bi2[0,i,:],(1,2*n_hidden))\n",
    "            mul3 =  tf.reshape(multi_bi3[0,i,:],(1,2*n_hidden))\n",
    "            m1   =  fc_1_drop[i,0] + w1[i]\n",
    "            m2   =  fc_1_drop[i,1] + w2[i]\n",
    "            m3   =  fc_1_drop[i,2] + w3[i]\n",
    "            m1   =  tf.sigmoid(m1)\n",
    "            m2   =  tf.sigmoid(m2)\n",
    "            m3   =  tf.sigmoid(m3)\n",
    "            multi_ahead = tf.multiply(m1,mul1) + tf.multiply(m2,mul2) + tf.multiply(m3, mul3)\n",
    "#             multi_ahead = tf.reshape(multi_bid[0,i,:],(1,2*n_hidden))\n",
    "            with tf.variable_scope(\"fc_m\") as fc_m:\n",
    "                fc_m = tf.contrib.layers.fully_connected(multi_ahead,n_hidden, activation_fn=tf.nn.relu)\n",
    "                fc_m_drop = tf.nn.dropout(fc_m, 0.8)\n",
    "                fc_m_variables =  [v for v in tf.global_variables() if v.name.startswith(fc_m.name)]   \n",
    "            \n",
    "            with tf.variable_scope(\"fc_m2\") as fc_m2:\n",
    "                fc_m2 = tf.contrib.layers.fully_connected(fc_m_drop,n_hidden, activation_fn=tf.nn.relu)\n",
    "                fc_m2_drop = tf.nn.dropout(fc_m2, 0.8)\n",
    "                fc_m_variables =  [v for v in tf.global_variables() if v.name.startswith(fc_m2.name)]\n",
    "\n",
    "            # FC to output classification\n",
    "            pred = tf.matmul(fc_m2_drop , weights['out']) + biases['out']\n",
    "\n",
    "            # save the predict of each time step\n",
    "            if i == 0:\n",
    "                soft_pred = tf.reshape((tf.nn.softmax(pred)),(1,n_classes))\n",
    " \n",
    "            else:\n",
    "                temp_soft_pred = tf.reshape((tf.nn.softmax(pred)),(1,n_classes))\n",
    "                soft_pred   = tf.concat([soft_pred,temp_soft_pred],axis =0)\n",
    "\n",
    "\n",
    "\n",
    "            # negative example\n",
    "            yc = tf.reshape(y[0,:],(1,n_classes))\n",
    "            neg_loss = tf.nn.softmax_cross_entropy_with_logits(labels = yc,logits = pred) # Softmax loss\n",
    "            temp_loss = tf.reduce_mean(neg_loss)\n",
    "            loss = tf.add(loss, temp_loss)\n",
    "            #loss = tf.add(loss,reg_term)\n",
    "\n",
    "      \n",
    "    # Define loss and optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss/n_frames) # Adam Optimizer\n",
    "\n",
    "    return x,x2,x3,y,optimizer,loss,soft_pred,fc_m_variables,multi_att,matt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-6026c1dd4629>:171: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method Tensor.get_shape of <tf.Tensor 'self_attention_1/Reshape_4:0' shape=(8, 32, 32) dtype=float32>>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "x,x2,x3,y,optimizer,loss,soft_pred,fc_m_variables,multi_att,matt= build_model()\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.9)\n",
    "sess = tf.InteractiveSession(config=tf.ConfigProto(allow_soft_placement=True,gpu_options=gpu_options))\n",
    "# mkdir folder for saving model\n",
    "if os.path.isdir(save_path) == False:\n",
    "        os.mkdir(save_path)\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "# Launch the graph\n",
    "sess.run(init)\n",
    "saver = tf.train.Saver()\n",
    "#saver = tf.train.import_meta_graph(save_path +'model-25.meta')\n",
    "#saver.restore(sess,save_path+'model-25')\n",
    "epoch = 1\n",
    "epoch_loss = np.zeros((train_num,1),dtype = float)\n",
    "n_batchs = np.arange(1,train_num+1)\n",
    "np.random.shuffle(n_batchs)\n",
    "tStart_epoch = time.time()\n",
    "matt.get_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch:', 1, ' done. Loss:', 33.091738519751786)\n",
      "('Epoch Time Cost:', 132.13, 's')\n",
      "('Epoch:', 2, ' done. Loss:', 8.870543316416235)\n",
      "('Epoch Time Cost:', 70.39, 's')\n",
      "('Epoch:', 3, ' done. Loss:', 5.075470248731963)\n",
      "('Epoch Time Cost:', 70.47, 's')\n",
      "('Epoch:', 4, ' done. Loss:', 3.636089676031739)\n",
      "('Epoch Time Cost:', 70.37, 's')\n",
      "('Epoch:', 5, ' done. Loss:', 2.810400542775569)\n",
      "('Epoch Time Cost:', 71.4, 's')\n",
      "('Epoch:', 6, ' done. Loss:', 2.3393373004830407)\n",
      "('Epoch Time Cost:', 71.54, 's')\n",
      "('Epoch:', 7, ' done. Loss:', 2.038633037654055)\n",
      "('Epoch Time Cost:', 71.17, 's')\n",
      "('Epoch:', 8, ' done. Loss:', 1.8843494593889545)\n",
      "('Epoch Time Cost:', 71.25, 's')\n",
      "('Epoch:', 9, ' done. Loss:', 1.7701509478355288)\n",
      "('Epoch Time Cost:', 71.32, 's')\n",
      "('Epoch:', 10, ' done. Loss:', 1.5811151706891804)\n",
      "('Epoch Time Cost:', 71.39, 's')\n",
      "('Epoch:', 11, ' done. Loss:', 1.4543466390654887)\n",
      "('Epoch Time Cost:', 71.81, 's')\n",
      "('Epoch:', 12, ' done. Loss:', 1.2435463588484845)\n",
      "('Epoch Time Cost:', 71.65, 's')\n",
      "('Epoch:', 13, ' done. Loss:', 1.186884082639877)\n",
      "('Epoch Time Cost:', 71.58, 's')\n",
      "('Epoch:', 14, ' done. Loss:', 1.2574755094713226)\n",
      "('Epoch Time Cost:', 71.7, 's')\n",
      "('Epoch:', 15, ' done. Loss:', 1.2376604807966913)\n",
      "('Epoch Time Cost:', 71.59, 's')\n",
      "('Epoch:', 16, ' done. Loss:', 1.0983868593123545)\n",
      "('Epoch Time Cost:', 71.77, 's')\n",
      "('Epoch:', 17, ' done. Loss:', 0.9604847720033592)\n",
      "('Epoch Time Cost:', 71.83, 's')\n",
      "('Epoch:', 18, ' done. Loss:', 1.1198568617690914)\n",
      "('Epoch Time Cost:', 71.62, 's')\n",
      "('Epoch:', 19, ' done. Loss:', 0.8751179190807997)\n",
      "('Epoch Time Cost:', 71.59, 's')\n",
      "('Epoch:', 20, ' done. Loss:', 0.8811629826200975)\n",
      "('Epoch Time Cost:', 71.55, 's')\n",
      "('Epoch:', 21, ' done. Loss:', 0.8195647947182203)\n",
      "('Epoch Time Cost:', 71.65, 's')\n",
      "('Epoch:', 22, ' done. Loss:', 1.066005489292051)\n",
      "('Epoch Time Cost:', 71.46, 's')\n",
      "('Epoch:', 23, ' done. Loss:', 0.5376360987387853)\n",
      "('Epoch Time Cost:', 71.84, 's')\n",
      "('Epoch:', 24, ' done. Loss:', 0.7704974023023464)\n",
      "('Epoch Time Cost:', 71.47, 's')\n",
      "('Epoch:', 25, ' done. Loss:', 0.8832245185911604)\n",
      "('Epoch Time Cost:', 71.49, 's')\n",
      "('Epoch:', 26, ' done. Loss:', 0.6187869021332185)\n",
      "('Epoch Time Cost:', 71.35, 's')\n",
      "('Epoch:', 27, ' done. Loss:', 0.5350972233826838)\n",
      "('Epoch Time Cost:', 71.5, 's')\n",
      "('Epoch:', 28, ' done. Loss:', 0.6264905497483305)\n",
      "('Epoch Time Cost:', 71.65, 's')\n",
      "('Epoch:', 29, ' done. Loss:', 0.5249141228365173)\n",
      "('Epoch Time Cost:', 71.5, 's')\n",
      "('Epoch:', 30, ' done. Loss:', 0.6836818939294328)\n",
      "('Epoch Time Cost:', 71.67, 's')\n",
      "('Epoch:', 31, ' done. Loss:', 0.8112620187466525)\n",
      "('Epoch Time Cost:', 71.56, 's')\n",
      "('Epoch:', 32, ' done. Loss:', 0.4066040786419834)\n",
      "('Epoch Time Cost:', 71.83, 's')\n",
      "('Epoch:', 33, ' done. Loss:', 0.4974292441749902)\n",
      "('Epoch Time Cost:', 71.36, 's')\n",
      "('Epoch:', 34, ' done. Loss:', 0.6074615145479962)\n",
      "('Epoch Time Cost:', 71.62, 's')\n",
      "('Epoch:', 35, ' done. Loss:', 0.5516054229133526)\n",
      "('Epoch Time Cost:', 71.37, 's')\n",
      "('Epoch:', 36, ' done. Loss:', 0.43629669839070107)\n",
      "('Epoch Time Cost:', 72.25, 's')\n",
      "('Epoch:', 37, ' done. Loss:', 0.7312271071951616)\n",
      "('Epoch Time Cost:', 71.52, 's')\n",
      "('Epoch:', 38, ' done. Loss:', 0.3689503183846262)\n",
      "('Epoch Time Cost:', 71.79, 's')\n",
      "('Epoch:', 39, ' done. Loss:', 0.6094550804493529)\n",
      "('Epoch Time Cost:', 71.92, 's')\n",
      "('Epoch:', 40, ' done. Loss:', 0.49999988367153236)\n",
      "('Epoch Time Cost:', 71.92, 's')\n",
      "('Epoch:', 41, ' done. Loss:', 0.26617972972123943)\n",
      "('Epoch Time Cost:', 71.65, 's')\n",
      "('Epoch:', 42, ' done. Loss:', 0.19530176919758746)\n",
      "('Epoch Time Cost:', 71.48, 's')\n",
      "('Epoch:', 43, ' done. Loss:', 0.7552874109513016)\n",
      "('Epoch Time Cost:', 71.68, 's')\n",
      "('Epoch:', 44, ' done. Loss:', 0.42393764717298305)\n",
      "('Epoch Time Cost:', 72.03, 's')\n",
      "('Epoch:', 45, ' done. Loss:', 0.29977047581578015)\n",
      "('Epoch Time Cost:', 71.73, 's')\n",
      "('Epoch:', 46, ' done. Loss:', 0.18346619783221219)\n",
      "('Epoch Time Cost:', 71.76, 's')\n",
      "('Epoch:', 47, ' done. Loss:', 0.7261256890304794)\n",
      "('Epoch Time Cost:', 71.39, 's')\n",
      "('Epoch:', 48, ' done. Loss:', 0.3225411486375235)\n",
      "('Epoch Time Cost:', 72.18, 's')\n",
      "('Epoch:', 49, ' done. Loss:', 0.3013217722836097)\n",
      "('Epoch Time Cost:', 71.73, 's')\n",
      "('Epoch:', 50, ' done. Loss:', 0.40480681975707)\n",
      "('Epoch Time Cost:', 71.53, 's')\n",
      "('Epoch:', 51, ' done. Loss:', 0.2800543287844576)\n",
      "('Epoch Time Cost:', 71.72, 's')\n",
      "('Epoch:', 52, ' done. Loss:', 0.6572073547206347)\n",
      "('Epoch Time Cost:', 71.83, 's')\n",
      "('Epoch:', 53, ' done. Loss:', 0.3083409995415113)\n",
      "('Epoch Time Cost:', 71.89, 's')\n",
      "('Epoch:', 54, ' done. Loss:', 0.31540715568550404)\n",
      "('Epoch Time Cost:', 72.0, 's')\n",
      "('Epoch:', 55, ' done. Loss:', 0.19785400974361303)\n",
      "('Epoch Time Cost:', 71.59, 's')\n",
      "('Epoch:', 56, ' done. Loss:', 0.34409504954295705)\n",
      "('Epoch Time Cost:', 71.82, 's')\n",
      "('Epoch:', 57, ' done. Loss:', 0.2718774495643224)\n",
      "('Epoch Time Cost:', 71.56, 's')\n",
      "('Epoch:', 58, ' done. Loss:', 0.16617806504659471)\n",
      "('Epoch Time Cost:', 71.81, 's')\n",
      "('Epoch:', 59, ' done. Loss:', 0.32976174921293794)\n",
      "('Epoch Time Cost:', 71.72, 's')\n",
      "('Epoch:', 60, ' done. Loss:', 0.22672102118066376)\n",
      "('Epoch Time Cost:', 71.66, 's')\n",
      "('Epoch:', 61, ' done. Loss:', 0.14681977794653603)\n",
      "('Epoch Time Cost:', 71.52, 's')\n",
      "('Epoch:', 62, ' done. Loss:', 0.38172130325437753)\n",
      "('Epoch Time Cost:', 71.75, 's')\n",
      "('Epoch:', 63, ' done. Loss:', 0.5286192765107391)\n",
      "('Epoch Time Cost:', 71.58, 's')\n",
      "('Epoch:', 64, ' done. Loss:', 0.17291782284650795)\n",
      "('Epoch Time Cost:', 71.95, 's')\n",
      "('Epoch:', 65, ' done. Loss:', 0.10282386254432421)\n",
      "('Epoch Time Cost:', 71.44, 's')\n",
      "('Epoch:', 66, ' done. Loss:', 0.3438562542336375)\n",
      "('Epoch Time Cost:', 71.6, 's')\n",
      "('Epoch:', 67, ' done. Loss:', 0.35761682191980293)\n",
      "('Epoch Time Cost:', 71.61, 's')\n",
      "('Epoch:', 68, ' done. Loss:', 0.18405656588333763)\n",
      "('Epoch Time Cost:', 72.05, 's')\n",
      "('Epoch:', 69, ' done. Loss:', 0.2370899695586786)\n",
      "('Epoch Time Cost:', 72.16, 's')\n",
      "('Epoch:', 70, ' done. Loss:', 0.23206709598692607)\n",
      "('Epoch Time Cost:', 71.66, 's')\n",
      "('Epoch:', 71, ' done. Loss:', 0.40812895665787063)\n",
      "('Epoch Time Cost:', 71.54, 's')\n",
      "('Epoch:', 72, ' done. Loss:', 0.14218336933992623)\n",
      "('Epoch Time Cost:', 71.6, 's')\n",
      "('Epoch:', 73, ' done. Loss:', 0.12771534582365757)\n",
      "('Epoch Time Cost:', 71.79, 's')\n",
      "('Epoch:', 74, ' done. Loss:', 0.25095549780717025)\n",
      "('Epoch Time Cost:', 71.79, 's')\n",
      "('Epoch:', 75, ' done. Loss:', 0.1850574316727769)\n",
      "('Epoch Time Cost:', 71.84, 's')\n",
      "('Epoch:', 76, ' done. Loss:', 0.06704555116270372)\n",
      "('Epoch Time Cost:', 71.72, 's')\n",
      "('Epoch:', 77, ' done. Loss:', 0.2079514421199543)\n",
      "('Epoch Time Cost:', 71.7, 's')\n",
      "('Epoch:', 78, ' done. Loss:', 0.212544617692032)\n",
      "('Epoch Time Cost:', 71.69, 's')\n",
      "('Epoch:', 79, ' done. Loss:', 0.20372827294726645)\n",
      "('Epoch Time Cost:', 71.54, 's')\n",
      "('Epoch:', 80, ' done. Loss:', 0.07161392670506339)\n",
      "('Epoch Time Cost:', 71.79, 's')\n",
      "('Epoch:', 81, ' done. Loss:', 0.28392190003103607)\n",
      "('Epoch Time Cost:', 71.72, 's')\n",
      "('Epoch:', 82, ' done. Loss:', 0.26028818635406353)\n",
      "('Epoch Time Cost:', 71.67, 's')\n",
      "('Epoch:', 83, ' done. Loss:', 0.19262766439090462)\n",
      "('Epoch Time Cost:', 71.77, 's')\n",
      "('Epoch:', 84, ' done. Loss:', 0.06749664995376196)\n",
      "('Epoch Time Cost:', 71.96, 's')\n",
      "('Epoch:', 85, ' done. Loss:', 0.281709758993084)\n",
      "('Epoch Time Cost:', 71.66, 's')\n",
      "('Epoch:', 86, ' done. Loss:', 0.11278363982029181)\n",
      "('Epoch Time Cost:', 71.47, 's')\n",
      "('Epoch:', 87, ' done. Loss:', 0.14828527469697575)\n",
      "('Epoch Time Cost:', 71.86, 's')\n",
      "('Epoch:', 88, ' done. Loss:', 0.3746248372976856)\n",
      "('Epoch Time Cost:', 71.97, 's')\n",
      "('Epoch:', 89, ' done. Loss:', 0.08674391426955189)\n",
      "('Epoch Time Cost:', 71.83, 's')\n",
      "('Epoch:', 90, ' done. Loss:', 0.04812726965451783)\n",
      "('Epoch Time Cost:', 71.63, 's')\n",
      "('Epoch:', 91, ' done. Loss:', 0.5329398292159251)\n",
      "('Epoch Time Cost:', 71.89, 's')\n",
      "('Epoch:', 92, ' done. Loss:', 0.09774998134785899)\n",
      "('Epoch Time Cost:', 72.21, 's')\n",
      "('Epoch:', 93, ' done. Loss:', 0.047290839203835436)\n",
      "('Epoch Time Cost:', 71.93, 's')\n",
      "('Epoch:', 94, ' done. Loss:', 0.2992039362999807)\n",
      "('Epoch Time Cost:', 71.8, 's')\n",
      "('Epoch:', 95, ' done. Loss:', 0.16275384869546705)\n",
      "('Epoch Time Cost:', 71.39, 's')\n",
      "('Epoch:', 96, ' done. Loss:', 0.04759529084790629)\n",
      "('Epoch Time Cost:', 70.36, 's')\n",
      "('Epoch:', 97, ' done. Loss:', 0.4241506657990078)\n",
      "('Epoch Time Cost:', 70.22, 's')\n",
      "('Epoch:', 98, ' done. Loss:', 0.06212166736387965)\n",
      "('Epoch Time Cost:', 70.06, 's')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch:', 99, ' done. Loss:', 0.16950925085368254)\n",
      "('Epoch Time Cost:', 70.1, 's')\n",
      "('Epoch:', 100, ' done. Loss:', 0.2215232433530038)\n",
      "('Epoch Time Cost:', 70.27, 's')\n",
      "Optimization Finished!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./multi_model/ixmas_fusion_bi_scratch/final_model'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keep training until reach max iterations\n",
    "# start training\n",
    "for epoch in range(n_epochs):\n",
    "#  chose batch randomly\n",
    "    epoch_loss = np.zeros((train_num,1),dtype = float)\n",
    "    n_batchs = np.arange(1,train_num+1)\n",
    "    np.random.shuffle(n_batchs)\n",
    "    tStart_epoch = time.time()\n",
    "    k = 0\n",
    "    for batch in n_batchs:\n",
    "        \n",
    "        # load features and labels\n",
    "        file_name    = video_list['video'][0][batch][0][0]\n",
    "        batch_data   = np.load(train_path   + file_name +'.npy').item()\n",
    "        batch_data2  = np.load(train_path_2 + file_name +'.npy').item()\n",
    "        batch_data3  = np.load(train_path_3 + file_name +'.npy').item()\n",
    "        batch_xs     = batch_data['feat']\n",
    "        batch_xs2    = batch_data2['feat']\n",
    "        batch_xs3    = batch_data3['feat']\n",
    "        \n",
    "        batch_ys     = batch_data['label']\n",
    "        totalframe   = batch_xs.shape\n",
    "        totalframe   = totalframe[0]\n",
    "        \n",
    "        # reshape input batch \n",
    "        batch_xc  = np.zeros((1,n_frames,batch_xs.shape[1]))\n",
    "        batch_xc2 = np.zeros((1,n_frames,batch_xs.shape[1]))\n",
    "        batch_xc3 = np.zeros((1,n_frames,batch_xs.shape[1]))\n",
    "        batch_yc  = np.zeros((1,n_frames,n_classes))                   \n",
    "        batch_xc[0,:,:]  = batch_xs\n",
    "        batch_xc2[0,:,:] = batch_xs2\n",
    "        batch_xc3[0,:,:] = batch_xs3\n",
    "        \n",
    "        # feed-forward\n",
    "        _,batch_loss = sess.run([optimizer,loss], feed_dict={x: batch_xc, x2: batch_xc2, x3: batch_xc3, y: batch_ys})\n",
    "\n",
    "        epoch_loss[batch-1] = batch_loss\n",
    "        # Debug per batch\n",
    "        k = k + 1\n",
    "\n",
    "    # print per epoch\n",
    "    print (\"Epoch:\", epoch+1, \" done. Loss:\", np.mean(epoch_loss))\n",
    "    tStop_epoch = time.time()\n",
    "    print (\"Epoch Time Cost:\", round(tStop_epoch - tStart_epoch,2), \"s\")\n",
    "    sys.stdout.flush()\n",
    "    if (epoch+1) %10 == 0:\n",
    "        saver.save(sess,save_path+\"model\", global_step = epoch+1)\n",
    "print (\"Optimization Finished!\")\n",
    "saver.save(sess, save_path+\"final_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py:1645: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./multi_model/ixmas_fusion_bi_scratch/final_model\n",
      "INFO:tensorflow:Restoring parameters from ./multi_model/ixmas_fusion_bi_scratch/final_model\n",
      "clare2_01_check-watch_cam0_frames_0030_0103\n",
      "('video number:', 0, ' of ', 179)\n",
      "('current mAP = ', 1.0)\n",
      "amel2_01_check-watch_cam1_frames_0076_0123\n",
      "('video number:', 1, ' of ', 179)\n",
      "('current mAP = ', 1.0)\n",
      "hedlena2_01_check-watch_cam2_frames_0052_0122\n",
      "('video number:', 2, ' of ', 179)\n",
      "('current mAP = ', 1.0)\n",
      "nicolas1_01_check-watch_cam2_frames_0050_0119\n",
      "('video number:', 3, ' of ', 179)\n",
      "('current mAP = ', 1.0)\n",
      "andreas2_01_check-watch_cam1_frames_0043_0107\n",
      "('video number:', 4, ' of ', 179)\n",
      "('current mAP = ', 1.0)\n",
      "florian3_01_check-watch_cam4_frames_0040_0090\n",
      "('video number:', 5, ' of ', 179)\n",
      "('current mAP = ', 0.8333333333333334)\n",
      "clare1_01_check-watch_cam0_frames_0112_0182\n",
      "('video number:', 6, ' of ', 179)\n",
      "('current mAP = ', 0.7142857142857143)\n",
      "chiara2_01_check-watch_cam3_frames_0033_0087\n",
      "('video number:', 7, ' of ', 179)\n",
      "('current mAP = ', 0.75)\n",
      "hedlena1_01_check-watch_cam4_frames_0058_0138\n",
      "('video number:', 8, ' of ', 179)\n",
      "('current mAP = ', 0.7777777777777778)\n",
      "florian3_01_check-watch_cam3_frames_0040_0090\n",
      "('video number:', 9, ' of ', 179)\n",
      "('current mAP = ', 0.8)\n",
      "daniel2_01_check-watch_cam1_frames_0015_0080\n",
      "('video number:', 10, ' of ', 179)\n",
      "('current mAP = ', 0.8181818181818182)\n",
      "nicolas1_01_check-watch_cam4_frames_0050_0119\n",
      "('video number:', 11, ' of ', 179)\n",
      "('current mAP = ', 0.8333333333333334)\n",
      "amel3_01_check-watch_cam4_frames_0083_0138\n",
      "('video number:', 12, ' of ', 179)\n",
      "('current mAP = ', 0.8461538461538461)\n",
      "andreas2_01_check-watch_cam2_frames_0043_0107\n",
      "('video number:', 13, ' of ', 179)\n",
      "('current mAP = ', 0.8571428571428571)\n",
      "andreas3_01_check-watch_cam2_frames_0029_0097\n",
      "('video number:', 14, ' of ', 179)\n",
      "('current mAP = ', 0.8666666666666667)\n",
      "nicolas3_02_cross-arms_cam4_frames_0128_0192\n",
      "('video number:', 15, ' of ', 179)\n",
      "('current mAP = ', 0.875)\n",
      "clare2_02_cross-arms_cam0_frames_0124_0200\n",
      "('video number:', 16, ' of ', 179)\n",
      "('current mAP = ', 0.8823529411764706)\n",
      "julien2_02_cross-arms_cam4_frames_0136_0179\n",
      "('video number:', 17, ' of ', 179)\n",
      "('current mAP = ', 0.8888888888888888)\n",
      "chiara3_02_cross-arms_cam3_frames_0101_0154\n",
      "('video number:', 18, ' of ', 179)\n",
      "('current mAP = ', 0.8947368421052632)\n",
      "daniel3_02_cross-arms_cam4_frames_0098_0150\n",
      "('video number:', 19, ' of ', 179)\n",
      "('current mAP = ', 0.9)\n",
      "daniel1_02_cross-arms_cam3_frames_0111_0183\n",
      "('video number:', 20, ' of ', 179)\n",
      "('current mAP = ', 0.9047619047619048)\n",
      "andreas2_02_cross-arms_cam1_frames_0108_0178\n",
      "('video number:', 21, ' of ', 179)\n",
      "('current mAP = ', 0.9090909090909091)\n",
      "chiara1_02_cross-arms_cam2_frames_0108_0163\n",
      "('video number:', 22, ' of ', 179)\n",
      "('current mAP = ', 0.9130434782608695)\n",
      "daniel2_02_cross-arms_cam3_frames_0101_0161\n",
      "('video number:', 23, ' of ', 179)\n",
      "('current mAP = ', 0.9166666666666666)\n",
      "clare3_02_cross-arms_cam3_frames_0096_0158\n",
      "('video number:', 24, ' of ', 179)\n",
      "('current mAP = ', 0.92)\n",
      "florian1_02_cross-arms_cam3_frames_0126_0186\n",
      "('video number:', 25, ' of ', 179)\n",
      "('current mAP = ', 0.9230769230769231)\n",
      "clare1_02_cross-arms_cam3_frames_0294_0347\n",
      "('video number:', 26, ' of ', 179)\n",
      "('current mAP = ', 0.9259259259259259)\n",
      "florian3_02_cross-arms_cam3_frames_0101_0168\n",
      "('video number:', 27, ' of ', 179)\n",
      "('current mAP = ', 0.9285714285714286)\n",
      "florian1_02_cross-arms_cam1_frames_0126_0186\n",
      "('video number:', 28, ' of ', 179)\n",
      "('current mAP = ', 0.9310344827586207)\n",
      "alba1_02_cross-arms_cam2_frames_0115_0191\n",
      "('video number:', 29, ' of ', 179)\n",
      "('current mAP = ', 0.9)\n",
      "daniel3_03_scratch-head_cam0_frames_0151_0237\n",
      "('video number:', 30, ' of ', 179)\n",
      "('current mAP = ', 0.8709677419354839)\n",
      "chiara2_03_scratch-head_cam1_frames_0145_0202\n",
      "('video number:', 31, ' of ', 179)\n",
      "('current mAP = ', 0.84375)\n",
      "nicolas3_03_scratch-head_cam3_frames_0208_0270\n",
      "('video number:', 32, ' of ', 179)\n",
      "('current mAP = ', 0.8484848484848485)\n",
      "daniel2_03_scratch-head_cam0_frames_0162_0218\n",
      "('video number:', 33, ' of ', 179)\n",
      "('current mAP = ', 0.8529411764705882)\n",
      "chiara2_03_scratch-head_cam2_frames_0145_0202\n",
      "('video number:', 34, ' of ', 179)\n",
      "('current mAP = ', 0.8571428571428571)\n",
      "florian1_03_scratch-head_cam2_frames_0187_0264\n",
      "('video number:', 35, ' of ', 179)\n",
      "('current mAP = ', 0.8611111111111112)\n",
      "andreas2_03_scratch-head_cam2_frames_0179_0238\n",
      "('video number:', 36, ' of ', 179)\n",
      "('current mAP = ', 0.8648648648648649)\n",
      "hedlena3_03_scratch-head_cam2_frames_0220_0296\n",
      "('video number:', 37, ' of ', 179)\n",
      "('current mAP = ', 0.868421052631579)\n",
      "andreas3_03_scratch-head_cam1_frames_0150_0226\n",
      "('video number:', 38, ' of ', 179)\n",
      "('current mAP = ', 0.8717948717948718)\n",
      "daniel3_03_scratch-head_cam2_frames_0151_0237\n",
      "('video number:', 39, ' of ', 179)\n",
      "('current mAP = ', 0.875)\n",
      "andreas2_03_scratch-head_cam0_frames_0179_0238\n",
      "('video number:', 40, ' of ', 179)\n",
      "('current mAP = ', 0.8780487804878049)\n",
      "clare3_03_scratch-head_cam4_frames_0159_0238\n",
      "('video number:', 41, ' of ', 179)\n",
      "('current mAP = ', 0.8809523809523809)\n",
      "nicolas1_03_scratch-head_cam3_frames_0206_0300\n",
      "('video number:', 42, ' of ', 179)\n",
      "('current mAP = ', 0.8604651162790697)\n",
      "amel3_03_scratch-head_cam2_frames_0318_0372\n",
      "('video number:', 43, ' of ', 179)\n",
      "('current mAP = ', 0.8636363636363636)\n",
      "daniel3_03_scratch-head_cam1_frames_0151_0237\n",
      "('video number:', 44, ' of ', 179)\n",
      "('current mAP = ', 0.8666666666666667)\n",
      "clare1_04_sit-down_cam3_frames_0450_0520\n",
      "('video number:', 45, ' of ', 179)\n",
      "('current mAP = ', 0.8695652173913043)\n",
      "florian2_04_sit-down_cam4_frames_0254_0335\n",
      "('video number:', 46, ' of ', 179)\n",
      "('current mAP = ', 0.8723404255319149)\n",
      "julien1_04_sit-down_cam4_frames_0247_0315\n",
      "('video number:', 47, ' of ', 179)\n",
      "('current mAP = ', 0.875)\n",
      "florian3_04_sit-down_cam4_frames_0239_0290\n",
      "('video number:', 48, ' of ', 179)\n",
      "('current mAP = ', 0.8775510204081632)\n",
      "daniel2_04_sit-down_cam1_frames_0233_0306\n",
      "('video number:', 49, ' of ', 179)\n",
      "('current mAP = ', 0.88)\n",
      "andreas2_04_sit-down_cam0_frames_0254_0327\n",
      "('video number:', 50, ' of ', 179)\n",
      "('current mAP = ', 0.8823529411764706)\n",
      "alba2_04_sit-down_cam4_frames_0299_0354\n",
      "('video number:', 51, ' of ', 179)\n",
      "('current mAP = ', 0.8846153846153846)\n",
      "florian1_04_sit-down_cam4_frames_0265_0352\n",
      "('video number:', 52, ' of ', 179)\n",
      "('current mAP = ', 0.8867924528301887)\n",
      "chiara2_04_sit-down_cam3_frames_0203_0261\n",
      "('video number:', 53, ' of ', 179)\n",
      "('current mAP = ', 0.8888888888888888)\n",
      "clare2_04_sit-down_cam2_frames_0283_0348\n",
      "('video number:', 54, ' of ', 179)\n",
      "('current mAP = ', 0.8909090909090909)\n",
      "chiara1_04_sit-down_cam3_frames_0247_0339\n",
      "('video number:', 55, ' of ', 179)\n",
      "('current mAP = ', 0.8928571428571429)\n",
      "florian2_04_sit-down_cam3_frames_0254_0335\n",
      "('video number:', 56, ' of ', 179)\n",
      "('current mAP = ', 0.8947368421052632)\n",
      "chiara1_04_sit-down_cam0_frames_0247_0339\n",
      "('video number:', 57, ' of ', 179)\n",
      "('current mAP = ', 0.896551724137931)\n",
      "amel3_04_sit-down_cam4_frames_0373_0460\n",
      "('video number:', 58, ' of ', 179)\n",
      "('current mAP = ', 0.8983050847457628)\n",
      "alba3_04_sit-down_cam4_frames_0262_0338\n",
      "('video number:', 59, ' of ', 179)\n",
      "('current mAP = ', 0.9)\n",
      "clare2_05_get-up_cam3_frames_0371_0446\n",
      "('video number:', 60, ' of ', 179)\n",
      "('current mAP = ', 0.9016393442622951)\n",
      "nicolas3_05_get-up_cam3_frames_0378_0443\n",
      "('video number:', 61, ' of ', 179)\n",
      "('current mAP = ', 0.9032258064516129)\n",
      "florian1_05_get-up_cam1_frames_0353_0412\n",
      "('video number:', 62, ' of ', 179)\n",
      "('current mAP = ', 0.9047619047619048)\n",
      "nicolas3_05_get-up_cam0_frames_0378_0443\n",
      "('video number:', 63, ' of ', 179)\n",
      "('current mAP = ', 0.90625)\n",
      "nicolas3_05_get-up_cam4_frames_0378_0443\n",
      "('video number:', 64, ' of ', 179)\n",
      "('current mAP = ', 0.8923076923076924)\n",
      "julien1_05_get-up_cam0_frames_0316_0403\n",
      "('video number:', 65, ' of ', 179)\n",
      "('current mAP = ', 0.8939393939393939)\n",
      "hedlena2_05_get-up_cam2_frames_0375_0438\n",
      "('video number:', 66, ' of ', 179)\n",
      "('current mAP = ', 0.8955223880597015)\n",
      "hedlena3_05_get-up_cam2_frames_0372_0460\n",
      "('video number:', 67, ' of ', 179)\n",
      "('current mAP = ', 0.8970588235294118)\n",
      "amel1_05_get-up_cam4_frames_0441_0523\n",
      "('video number:', 68, ' of ', 179)\n",
      "('current mAP = ', 0.8985507246376812)\n",
      "andreas3_05_get-up_cam2_frames_0316_0400\n",
      "('video number:', 69, ' of ', 179)\n",
      "('current mAP = ', 0.9)\n",
      "florian2_05_get-up_cam0_frames_0336_0393\n",
      "('video number:', 70, ' of ', 179)\n",
      "('current mAP = ', 0.9014084507042254)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alba1_05_get-up_cam1_frames_0350_0424\n",
      "('video number:', 71, ' of ', 179)\n",
      "('current mAP = ', 0.9027777777777778)\n",
      "alba3_05_get-up_cam4_frames_0348_0400\n",
      "('video number:', 72, ' of ', 179)\n",
      "('current mAP = ', 0.9041095890410958)\n",
      "chiara3_05_get-up_cam4_frames_0293_0386\n",
      "('video number:', 73, ' of ', 179)\n",
      "('current mAP = ', 0.9054054054054054)\n",
      "julien1_05_get-up_cam3_frames_0316_0403\n",
      "('video number:', 74, ' of ', 179)\n",
      "('current mAP = ', 0.9066666666666666)\n",
      "hedlena1_06_turn-around_cam3_frames_0501_0564\n",
      "('video number:', 75, ' of ', 179)\n",
      "('current mAP = ', 0.9078947368421053)\n",
      "chiara1_06_turn-around_cam0_frames_0439_0519\n",
      "('video number:', 76, ' of ', 179)\n",
      "('current mAP = ', 0.9090909090909091)\n",
      "chiara2_06_turn-around_cam1_frames_0384_0453\n",
      "('video number:', 77, ' of ', 179)\n",
      "('current mAP = ', 0.9102564102564102)\n",
      "daniel2_06_turn-around_cam1_frames_0396_0472\n",
      "('video number:', 78, ' of ', 179)\n",
      "('current mAP = ', 0.9113924050632911)\n",
      "amel2_06_turn-around_cam1_frames_0474_0549\n",
      "('video number:', 79, ' of ', 179)\n",
      "('current mAP = ', 0.9125)\n",
      "amel2_06_turn-around_cam3_frames_0474_0549\n",
      "('video number:', 80, ' of ', 179)\n",
      "('current mAP = ', 0.9135802469135802)\n",
      "julien2_06_turn-around_cam1_frames_0412_0469\n",
      "('video number:', 81, ' of ', 179)\n",
      "('current mAP = ', 0.9146341463414634)\n",
      "amel1_06_turn-around_cam1_frames_0536_0617\n",
      "('video number:', 82, ' of ', 179)\n",
      "('current mAP = ', 0.9156626506024096)\n",
      "hedlena1_06_turn-around_cam4_frames_0501_0564\n",
      "('video number:', 83, ' of ', 179)\n",
      "('current mAP = ', 0.9166666666666666)\n",
      "florian1_06_turn-around_cam3_frames_0429_0497\n",
      "('video number:', 84, ' of ', 179)\n",
      "('current mAP = ', 0.9176470588235294)\n",
      "clare1_06_turn-around_cam0_frames_0738_0819\n",
      "('video number:', 85, ' of ', 179)\n",
      "('current mAP = ', 0.9186046511627907)\n",
      "andreas2_06_turn-around_cam2_frames_0449_0537\n",
      "('video number:', 86, ' of ', 179)\n",
      "('current mAP = ', 0.9195402298850575)\n",
      "julien3_06_turn-around_cam0_frames_0410_0473\n",
      "('video number:', 87, ' of ', 179)\n",
      "('current mAP = ', 0.9204545454545454)\n",
      "florian3_06_turn-around_cam2_frames_0390_0479\n",
      "('video number:', 88, ' of ', 179)\n",
      "('current mAP = ', 0.9213483146067416)\n",
      "hedlena2_06_turn-around_cam2_frames_0459_0533\n",
      "('video number:', 89, ' of ', 179)\n",
      "('current mAP = ', 0.9222222222222223)\n",
      "alba2_07_walk_cam3_frames_0591_0760\n",
      "('video number:', 90, ' of ', 179)\n",
      "('current mAP = ', 0.9230769230769231)\n",
      "hedlena1_07_walk_cam0_frames_0571_0717\n",
      "('video number:', 91, ' of ', 179)\n",
      "('current mAP = ', 0.9239130434782609)\n",
      "clare3_07_walk_cam2_frames_0486_0694\n",
      "('video number:', 92, ' of ', 179)\n",
      "('current mAP = ', 0.9247311827956989)\n",
      "nicolas3_07_walk_cam2_frames_0561_0738\n",
      "('video number:', 93, ' of ', 179)\n",
      "('current mAP = ', 0.925531914893617)\n",
      "clare1_07_walk_cam4_frames_0880_1160\n",
      "('video number:', 94, ' of ', 179)\n",
      "('current mAP = ', 0.9263157894736842)\n",
      "daniel1_07_walk_cam3_frames_0491_0671\n",
      "('video number:', 95, ' of ', 179)\n",
      "('current mAP = ', 0.9270833333333334)\n",
      "julien2_07_walk_cam4_frames_0470_0587\n",
      "('video number:', 96, ' of ', 179)\n",
      "('current mAP = ', 0.9278350515463918)\n",
      "chiara2_07_walk_cam2_frames_0454_0599\n",
      "('video number:', 97, ' of ', 179)\n",
      "('current mAP = ', 0.9285714285714286)\n",
      "amel3_07_walk_cam0_frames_0642_0780\n",
      "('video number:', 98, ' of ', 179)\n",
      "('current mAP = ', 0.9292929292929293)\n",
      "julien2_07_walk_cam2_frames_0470_0587\n",
      "('video number:', 99, ' of ', 179)\n",
      "('current mAP = ', 0.93)\n",
      "hedlena2_07_walk_cam2_frames_0534_0682\n",
      "('video number:', 100, ' of ', 179)\n",
      "('current mAP = ', 0.9306930693069307)\n",
      "nicolas1_07_walk_cam4_frames_0645_0789\n",
      "('video number:', 101, ' of ', 179)\n",
      "('current mAP = ', 0.9313725490196079)\n",
      "amel1_07_walk_cam3_frames_0618_0781\n",
      "('video number:', 102, ' of ', 179)\n",
      "('current mAP = ', 0.9320388349514563)\n",
      "nicolas1_07_walk_cam2_frames_0645_0789\n",
      "('video number:', 103, ' of ', 179)\n",
      "('current mAP = ', 0.9326923076923077)\n",
      "clare2_07_walk_cam2_frames_0545_0714\n",
      "('video number:', 104, ' of ', 179)\n",
      "('current mAP = ', 0.9333333333333333)\n",
      "amel2_08_wave_cam1_frames_0684_0773\n",
      "('video number:', 105, ' of ', 179)\n",
      "('current mAP = ', 0.9339622641509434)\n",
      "chiara1_08_wave_cam3_frames_0669_0704\n",
      "('video number:', 106, ' of ', 179)\n",
      "('current mAP = ', 0.9345794392523364)\n",
      "florian3_08_wave_cam0_frames_0633_0696\n",
      "('video number:', 107, ' of ', 179)\n",
      "('current mAP = ', 0.9351851851851852)\n",
      "andreas1_08_wave_cam0_frames_0773_0843\n",
      "('video number:', 108, ' of ', 179)\n",
      "('current mAP = ', 0.9357798165137615)\n",
      "hedlena1_08_wave_cam3_frames_0756_0816\n",
      "('video number:', 109, ' of ', 179)\n",
      "('current mAP = ', 0.9363636363636364)\n",
      "alba2_08_wave_cam2_frames_0761_0825\n",
      "('video number:', 110, ' of ', 179)\n",
      "('current mAP = ', 0.9369369369369369)\n",
      "daniel1_08_wave_cam0_frames_0672_0727\n",
      "('video number:', 111, ' of ', 179)\n",
      "('current mAP = ', 0.9375)\n",
      "chiara2_08_wave_cam4_frames_0600_0635\n",
      "('video number:', 112, ' of ', 179)\n",
      "('current mAP = ', 0.9380530973451328)\n",
      "alba3_08_wave_cam0_frames_0661_0721\n",
      "('video number:', 113, ' of ', 179)\n",
      "('current mAP = ', 0.9385964912280702)\n",
      "florian1_08_wave_cam4_frames_0667_0726\n",
      "('video number:', 114, ' of ', 179)\n",
      "('current mAP = ', 0.9391304347826087)\n",
      "daniel3_08_wave_cam3_frames_0602_0663\n",
      "('video number:', 115, ' of ', 179)\n",
      "('current mAP = ', 0.9396551724137931)\n",
      "andreas2_08_wave_cam4_frames_0729_0788\n",
      "('video number:', 116, ' of ', 179)\n",
      "('current mAP = ', 0.9401709401709402)\n",
      "julien3_08_wave_cam2_frames_0642_0724\n",
      "('video number:', 117, ' of ', 179)\n",
      "('current mAP = ', 0.940677966101695)\n",
      "amel2_08_wave_cam3_frames_0684_0773\n",
      "('video number:', 118, ' of ', 179)\n",
      "('current mAP = ', 0.9411764705882353)\n",
      "andreas3_08_wave_cam1_frames_0690_0758\n",
      "('video number:', 119, ' of ', 179)\n",
      "('current mAP = ', 0.9416666666666667)\n",
      "julien1_09_punch_cam1_frames_0649_0698\n",
      "('video number:', 120, ' of ', 179)\n",
      "('current mAP = ', 0.9421487603305785)\n",
      "nicolas3_09_punch_cam0_frames_0826_0869\n",
      "('video number:', 121, ' of ', 179)\n",
      "('current mAP = ', 0.9426229508196722)\n",
      "chiara2_09_punch_cam2_frames_0661_0710\n",
      "('video number:', 122, ' of ', 179)\n",
      "('current mAP = ', 0.943089430894309)\n",
      "andreas3_09_punch_cam0_frames_0759_0819\n",
      "('video number:', 123, ' of ', 179)\n",
      "('current mAP = ', 0.9435483870967742)\n",
      "andreas3_09_punch_cam1_frames_0759_0819\n",
      "('video number:', 124, ' of ', 179)\n",
      "('current mAP = ', 0.944)\n",
      "florian2_09_punch_cam0_frames_0708_0759\n",
      "('video number:', 125, ' of ', 179)\n",
      "('current mAP = ', 0.9444444444444444)\n",
      "clare3_09_punch_cam0_frames_0769_0818\n",
      "('video number:', 126, ' of ', 179)\n",
      "('current mAP = ', 0.9448818897637795)\n",
      "florian3_09_punch_cam1_frames_0697_0754\n",
      "('video number:', 127, ' of ', 179)\n",
      "('current mAP = ', 0.9453125)\n",
      "daniel1_09_punch_cam4_frames_0734_0789\n",
      "('video number:', 128, ' of ', 179)\n",
      "('current mAP = ', 0.9457364341085271)\n",
      "florian1_09_punch_cam2_frames_0727_0791\n",
      "('video number:', 129, ' of ', 179)\n",
      "('current mAP = ', 0.9461538461538461)\n",
      "clare2_09_punch_cam0_frames_0868_0919\n",
      "('video number:', 130, ' of ', 179)\n",
      "('current mAP = ', 0.9465648854961832)\n",
      "andreas1_09_punch_cam0_frames_0844_0908\n",
      "('video number:', 131, ' of ', 179)\n",
      "('current mAP = ', 0.946969696969697)\n",
      "amel3_09_punch_cam4_frames_0870_0892\n",
      "('video number:', 132, ' of ', 179)\n",
      "('current mAP = ', 0.9473684210526315)\n",
      "hedlena3_09_punch_cam3_frames_0809_0868\n",
      "('video number:', 133, ' of ', 179)\n",
      "('current mAP = ', 0.9477611940298507)\n",
      "florian2_09_punch_cam2_frames_0708_0759\n",
      "('video number:', 134, ' of ', 179)\n",
      "('current mAP = ', 0.9481481481481482)\n",
      "daniel1_10_kick_cam3_frames_0790_0866\n",
      "('video number:', 135, ' of ', 179)\n",
      "('current mAP = ', 0.9485294117647058)\n",
      "clare1_10_kick_cam4_frames_1356_1411\n",
      "('video number:', 136, ' of ', 179)\n",
      "('current mAP = ', 0.948905109489051)\n",
      "amel1_10_kick_cam0_frames_0918_1008\n",
      "('video number:', 137, ' of ', 179)\n",
      "('current mAP = ', 0.9492753623188406)\n",
      "amel2_10_kick_cam0_frames_0825_0892\n",
      "('video number:', 138, ' of ', 179)\n",
      "('current mAP = ', 0.9496402877697842)\n",
      "nicolas1_10_kick_cam3_frames_0914_0999\n",
      "('video number:', 139, ' of ', 179)\n",
      "('current mAP = ', 0.95)\n",
      "amel2_10_kick_cam1_frames_0825_0892\n",
      "('video number:', 140, ' of ', 179)\n",
      "('current mAP = ', 0.950354609929078)\n",
      "amel1_10_kick_cam1_frames_0918_1008\n",
      "('video number:', 141, ' of ', 179)\n",
      "('current mAP = ', 0.9507042253521126)\n",
      "clare1_10_kick_cam1_frames_1356_1411\n",
      "('video number:', 142, ' of ', 179)\n",
      "('current mAP = ', 0.951048951048951)\n",
      "florian3_10_kick_cam2_frames_0755_0799\n",
      "('video number:', 143, ' of ', 179)\n",
      "('current mAP = ', 0.9513888888888888)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "florian1_10_kick_cam2_frames_0792_0850\n",
      "('video number:', 144, ' of ', 179)\n",
      "('current mAP = ', 0.9517241379310345)\n",
      "chiara3_10_kick_cam4_frames_0689_0757\n",
      "('video number:', 145, ' of ', 179)\n",
      "('current mAP = ', 0.952054794520548)\n",
      "andreas2_10_kick_cam3_frames_0822_0899\n",
      "('video number:', 146, ' of ', 179)\n",
      "('current mAP = ', 0.9523809523809523)\n",
      "alba1_10_kick_cam1_frames_0794_0871\n",
      "('video number:', 147, ' of ', 179)\n",
      "('current mAP = ', 0.9459459459459459)\n",
      "hedlena3_10_kick_cam0_frames_0869_0953\n",
      "('video number:', 148, ' of ', 179)\n",
      "('current mAP = ', 0.9463087248322147)\n",
      "andreas3_10_kick_cam0_frames_0820_0861\n",
      "('video number:', 149, ' of ', 179)\n",
      "('current mAP = ', 0.9466666666666667)\n",
      "clare2_11_point_cam0_frames_1089_1146\n",
      "('video number:', 150, ' of ', 179)\n",
      "('current mAP = ', 0.9470198675496688)\n",
      "clare2_11_point_cam3_frames_1089_1146\n",
      "('video number:', 151, ' of ', 179)\n",
      "('current mAP = ', 0.9473684210526315)\n",
      "alba1_11_point_cam4_frames_0872_0967\n",
      "('video number:', 152, ' of ', 179)\n",
      "('current mAP = ', 0.9477124183006536)\n",
      "florian2_11_point_cam3_frames_0839_0888\n",
      "('video number:', 153, ' of ', 179)\n",
      "('current mAP = ', 0.948051948051948)\n",
      "nicolas1_11_point_cam4_frames_1009_1056\n",
      "('video number:', 154, ' of ', 179)\n",
      "('current mAP = ', 0.9483870967741935)\n",
      "hedlena1_11_point_cam4_frames_1007_1069\n",
      "('video number:', 155, ' of ', 179)\n",
      "('current mAP = ', 0.9487179487179487)\n",
      "daniel3_11_point_cam0_frames_0795_0836\n",
      "('video number:', 156, ' of ', 179)\n",
      "('current mAP = ', 0.9490445859872612)\n",
      "andreas1_11_point_cam4_frames_1009_1067\n",
      "('video number:', 157, ' of ', 179)\n",
      "('current mAP = ', 0.9430379746835443)\n",
      "julien3_11_point_cam4_frames_0909_0964\n",
      "('video number:', 158, ' of ', 179)\n",
      "('current mAP = ', 0.9433962264150944)\n",
      "clare3_11_point_cam0_frames_0907_0971\n",
      "('video number:', 159, ' of ', 179)\n",
      "('current mAP = ', 0.94375)\n",
      "chiara3_11_point_cam4_frames_0758_0820\n",
      "('video number:', 160, ' of ', 179)\n",
      "('current mAP = ', 0.937888198757764)\n",
      "florian1_11_point_cam0_frames_0930_0996\n",
      "('video number:', 161, ' of ', 179)\n",
      "('current mAP = ', 0.9382716049382716)\n",
      "hedlena2_11_point_cam2_frames_0929_1001\n",
      "('video number:', 162, ' of ', 179)\n",
      "('current mAP = ', 0.9386503067484663)\n",
      "amel1_11_point_cam3_frames_1070_1152\n",
      "('video number:', 163, ' of ', 179)\n",
      "('current mAP = ', 0.9329268292682927)\n",
      "andreas3_11_point_cam0_frames_0884_0940\n",
      "('video number:', 164, ' of ', 179)\n",
      "('current mAP = ', 0.9333333333333333)\n",
      "andreas1_12_pick-up_cam2_frames_1107_1169\n",
      "('video number:', 165, ' of ', 179)\n",
      "('current mAP = ', 0.9337349397590361)\n",
      "florian2_12_pick-up_cam4_frames_0909_0972\n",
      "('video number:', 166, ' of ', 179)\n",
      "('current mAP = ', 0.9341317365269461)\n",
      "nicolas3_12_pick-up_cam3_frames_1034_1098\n",
      "('video number:', 167, ' of ', 179)\n",
      "('current mAP = ', 0.9345238095238095)\n",
      "andreas3_12_pick-up_cam2_frames_0959_1014\n",
      "('video number:', 168, ' of ', 179)\n",
      "('current mAP = ', 0.9349112426035503)\n",
      "julien2_12_pick-up_cam3_frames_0832_0918\n",
      "('video number:', 169, ' of ', 179)\n",
      "('current mAP = ', 0.9352941176470588)\n",
      "daniel3_12_pick-up_cam4_frames_0884_0967\n",
      "('video number:', 170, ' of ', 179)\n",
      "('current mAP = ', 0.935672514619883)\n",
      "julien3_12_pick-up_cam3_frames_0965_1016\n",
      "('video number:', 171, ' of ', 179)\n",
      "('current mAP = ', 0.936046511627907)\n",
      "amel2_12_pick-up_cam4_frames_0983_1012\n",
      "('video number:', 172, ' of ', 179)\n",
      "('current mAP = ', 0.930635838150289)\n",
      "daniel1_12_pick-up_cam0_frames_0971_1030\n",
      "('video number:', 173, ' of ', 179)\n",
      "('current mAP = ', 0.9310344827586207)\n",
      "alba2_12_pick-up_cam3_frames_1137_1198\n",
      "('video number:', 174, ' of ', 179)\n",
      "('current mAP = ', 0.9314285714285714)\n",
      "amel3_12_pick-up_cam4_frames_1032_1112\n",
      "('video number:', 175, ' of ', 179)\n",
      "('current mAP = ', 0.9318181818181818)\n",
      "daniel3_12_pick-up_cam1_frames_0884_0967\n",
      "('video number:', 176, ' of ', 179)\n",
      "('current mAP = ', 0.9322033898305084)\n",
      "alba2_12_pick-up_cam1_frames_1137_1198\n",
      "('video number:', 177, ' of ', 179)\n",
      "('current mAP = ', 0.9325842696629213)\n",
      "daniel2_12_pick-up_cam2_frames_0897_0996\n",
      "('video number:', 178, ' of ', 179)\n",
      "('current mAP = ', 0.9329608938547486)\n",
      "chiara3_12_pick-up_cam1_frames_0821_0882\n",
      "('video number:', 179, ' of ', 179)\n",
      "('current mAP = ', 0.9333333333333333)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "att_res = {}\n",
    "x,x2,x3,y,optimizer,loss,soft_pred,fc_m_variables,multi_att,matt= build_model()\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.5)\n",
    "sess = tf.InteractiveSession(config=tf.ConfigProto(allow_soft_placement=True,gpu_options=gpu_options))\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# restore model\n",
    "print(default_model_path)\n",
    "saver.restore(sess, default_model_path)\n",
    "top1_list = []\n",
    "for num_batch in range(0,test_num+1): # 0, test_num = 0 : test_num-1\n",
    "    file_name = test_list['video'][0][num_batch][0][0]\n",
    "    test_all_data = np.load(test_path + file_name+'.npy').item()\n",
    "    test_all_data2 = np.load(test_path_2 + file_name+'.npy').item()\n",
    "    test_all_data3 = np.load(test_path_3 + file_name+'.npy').item()\n",
    "    print(file_name)\n",
    "    test_data = test_all_data['feat']\n",
    "    test_data2 = test_all_data2['feat']\n",
    "    test_data3 = test_all_data3['feat']\n",
    "   \n",
    "    #test_data_final = np.concatenate ((test_data,test_data_2),axis = 2)\n",
    "    test_labels = test_all_data['label']\n",
    "        \n",
    "    totalframe = test_data.shape\n",
    "    totalframe = totalframe[0]\n",
    "    att_res['name'] = file_name\n",
    "    #print(totalframe)\n",
    "    pred_list = np.zeros((n_frames,n_classes))\n",
    "    batch_xc  = np.zeros((1,n_frames,test_data.shape[1]))\n",
    "    batch_xc2 = np.zeros((1,n_frames,test_data.shape[1]))\n",
    "    batch_xc3 = np.zeros((1,n_frames,test_data.shape[1]))\n",
    "    batch_xc[0,:,:] = test_data\n",
    "    batch_xc2[0,:,:] = test_data2\n",
    "    batch_xc3[0,:,:] = test_data3\n",
    "                 \n",
    "    [pred,matte] = sess.run([soft_pred,matt], feed_dict={x: batch_xc, x2: batch_xc2, x3: batch_xc3, y:test_labels})\n",
    "   \n",
    "    pred_list    = pred\n",
    "    matt_list    = matte\n",
    "    \n",
    "\n",
    "    label_index  = np.where(test_labels==1)[1][0]\n",
    "    \n",
    "    avg_pre = np.mean(pred_list, axis=0).tolist()\n",
    "    top1 = (avg_pre.index(max(avg_pre))==label_index)\n",
    "    top1_list.append(top1)\n",
    "    att_res['pred_list'] = avg_pre\n",
    "    att_res['label'] = test_labels \n",
    "    att_res['matt'] = matt_list\n",
    "  \n",
    "    print('video number:' ,num_batch, ' of ', test_num)\n",
    "    print('current mAP = ',np.mean(top1_list))\n",
    "    \n",
    "    sio.savemat(output_path+file_name+'.mat',att_res, do_compression=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(test_labels==1)[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "168"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(top1_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
