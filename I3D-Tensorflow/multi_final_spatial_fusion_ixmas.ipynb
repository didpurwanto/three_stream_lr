{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/requests/__init__.py:83: RequestsDependencyWarning: Old version of cryptography ([1, 2, 3]) may cause slowdown.\n",
      "  warnings.warn(warning, RequestsDependencyWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import tensorflow as tf\n",
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import pdb\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "print(tf.__version__)\n",
    "import scipy.io as sio\n",
    "import math\n",
    "\n",
    "############### Global Parameters ###############\n",
    "# path\n",
    "video_list  = sio.loadmat('trainlist_ix2.mat')\n",
    "test_list = sio.loadmat('testlist_ix2.mat')\n",
    "\n",
    "train_path = './experiments/ixmas/rgb_data_split2/train/'\n",
    "\n",
    "train_path_2 = './experiments/ixmas/flow_data_split2/train/'\n",
    "\n",
    "test_path = './experiments/ixmas/rgb_data_split2/test/'\n",
    "\n",
    "test_path_2 = './experiments/ixmas/flow_data_split2/test/'\n",
    "\n",
    "train_path_3 = './experiments/ixmas/flowt_data_split2/train/'\n",
    "\n",
    "test_path_3 = './experiments/ixmas/flowt_data_split2/test/'\n",
    "\n",
    "# Model Path\n",
    "default_model_path = './multi_model/ixmas_fusion/final_model'\n",
    "save_path = './multi_model/ixmas_fusion/'\n",
    "\n",
    "# Output Path\n",
    "output_path = './ixmas_fusion_res/'\n",
    "\n",
    "# Train Test Split\n",
    "train_num = video_list['video'][0].shape[0]-1\n",
    "test_num = test_list['video'][0].shape[0]-1\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([u'chiara3_01_check-watch_cam2_frames_0029_0093'], dtype='<U44')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_list ['video'][0][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Train Parameters #################\n",
    "# Parameters\n",
    "learning_rate = 0.00002\n",
    "n_epochs = 100\n",
    "batch_size = 1\n",
    "# Network Parameters\n",
    "n_input = 1024 # input dimension\n",
    "n_hidden = 1024 # hidden layer num of multi_head\n",
    "n_classes = 12 # has 51 classes\n",
    "n_frames = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_dist(A, B):\n",
    "    assert A.shape.as_list() == B.shape.as_list()\n",
    "\n",
    "    row_norms_A = tf.reduce_sum(tf.square(A), axis=1)\n",
    "    row_norms_A = tf.reshape(row_norms_A, [-1, 1])  # Column vector.\n",
    "\n",
    "    row_norms_B = tf.reduce_sum(tf.square(B), axis=1)\n",
    "    row_norms_B = tf.reshape(row_norms_B, [1, -1])  # Row vector.\n",
    "\n",
    "    return row_norms_A - 2 * tf.matmul(A, tf.transpose(B)) + row_norms_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(inputs, \n",
    "              epsilon = 1e-8,\n",
    "              scope=\"ln\",\n",
    "              reuse=None):\n",
    "    '''Applies layer normalization.\n",
    "    \n",
    "    Args:\n",
    "      inputs: A tensor with 2 or more dimensions, where the first dimension has\n",
    "        `batch_size`.\n",
    "      epsilon: A floating number. A very small number for preventing ZeroDivision Error.\n",
    "      scope: Optional scope for `variable_scope`.\n",
    "      reuse: Boolean, whether to reuse the weights of a previous layer\n",
    "        by the same name.\n",
    "      \n",
    "    Returns:\n",
    "      A tensor with the same shape and data dtype as `inputs`.\n",
    "    '''\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        inputs_shape = inputs.get_shape()\n",
    "        params_shape = inputs_shape[-1:]\n",
    "    \n",
    "        mean, variance = tf.nn.moments(inputs, [-1], keep_dims=True)\n",
    "        beta= tf.Variable(tf.zeros(params_shape))\n",
    "        gamma = tf.Variable(tf.ones(params_shape))\n",
    "        normalized = (inputs - mean) / ( (variance + epsilon) ** (.5) )\n",
    "        outputs = gamma * normalized + beta\n",
    "        \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multihead_attention(queries, \n",
    "                        keys,\n",
    "                        wkeys, \n",
    "                        num_units=None, \n",
    "                        num_heads=8, \n",
    "                        dropout_rate=0,\n",
    "                        is_training=True,\n",
    "                        causality=False,\n",
    "                        trainable=True,\n",
    "                        scope=\"multihead_attention\", \n",
    "                        reuse=None):\n",
    "    '''Applies multihead attention.\n",
    "    \n",
    "    Args:\n",
    "      queries: A 3d tensor with shape of [N, T_q, C_q].\n",
    "      keys: A 3d tensor with shape of [N, T_k, C_k].\n",
    "      num_units: A scalar. Attention size.\n",
    "      dropout_rate: A floating point number.\n",
    "      is_training: Boolean. Controller of mechanism for dropout.\n",
    "      causality: Boolean. If true, units that reference the future are masked. \n",
    "      num_heads: An int. Number of heads.\n",
    "      scope: Optional scope for `variable_scope`.\n",
    "      reuse: Boolean, whether to reuse the weights of a previous layer\n",
    "        by the same name.\n",
    "        \n",
    "    Returns\n",
    "      A 3d tensor with shape of (N, T_q, C)  \n",
    "    '''\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        # Set the fall back option for num_units\n",
    "        if num_units is None:\n",
    "            num_units = queries.get_shape().as_list[-1]\n",
    "        \n",
    "        # Linear projections\n",
    "        Q = tf.contrib.layers.fully_connected(queries, num_units,trainable=trainable) # (N, T_q, C)\n",
    "        K = tf.contrib.layers.fully_connected(keys, num_units,trainable=trainable ) # (N, T_k, C)\n",
    "        V = tf.contrib.layers.fully_connected(keys, num_units,trainable=trainable) # (N, T_k, C)\n",
    "        \n",
    "        Q1 = tf.reshape(Q,(1,n_frames,num_units))\n",
    "        K1 = tf.reshape(K,(1,n_frames,num_units))        \n",
    "        V1 = tf.reshape(V,(1,n_frames,num_units))\n",
    "        \n",
    "        \n",
    "        # Split and concat\n",
    "        Q_ = tf.concat(tf.split(Q1, num_heads, axis = 2),axis =0) # (h*N, T_q, C/h) \n",
    "        K_ = tf.concat(tf.split(K1, num_heads, axis = 2),axis =0) # (h*N, T_k, C/h) \n",
    "        V_ = tf.concat(tf.split(V1, num_heads, axis = 2),axis =0) # (h*N, T_k, C/h) \n",
    "\n",
    "        \n",
    "        # Multiplication\n",
    "        #outputs = tf.matmul(Q_, tf.transpose(K_, [0, 2, 1])) # (h*N, T_q, T_k)\n",
    "        output_1 = tf.matmul(Q_, wkeys) # (h*N, T_q, T_k)\n",
    "        outputs  = tf.matmul(output_1, tf.transpose(K_, [0, 2, 1])) # (h*N, T_q, T_k)\n",
    "\n",
    "        \n",
    "        # Scale\n",
    "        outputs = outputs / (K_.get_shape().as_list()[-1] ** 0.5)\n",
    "\n",
    "                \n",
    "        # Key Masking\n",
    "        key_masks = tf.sign(tf.abs(tf.reduce_sum(keys, axis=-1))) # (N, T_k)\n",
    "        key_masks = tf.tile(key_masks, [num_heads, 1]) # (h*N, T_k)\n",
    "        key_masks = tf.tile(tf.expand_dims(key_masks, 1), [1, tf.shape(queries)[1], 1]) # (h*N, T_q, T_k)\n",
    "       # \n",
    "        paddings = tf.ones_like(outputs)*(-2**32+1)\n",
    "        outputs = tf.where(tf.equal(key_masks, 0), paddings, outputs) # (h*N, T_q, T_k)\n",
    "          \n",
    "        # Causality = Future blinding\n",
    "        if causality:\n",
    "            diag_vals = tf.ones_like(outputs[0, :, :]) # (T_q, T_k)\n",
    "            tril = tf.contrib.linalg.LinearOperatorTriL(diag_vals).to_dense() # (T_q, T_k)\n",
    "            masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(outputs)[0], 1, 1]) # (h*N, T_q, T_k)\n",
    "  # \n",
    "            paddings = tf.ones_like(masks)*(-2**32+1)\n",
    "            outputs = tf.where(tf.equal(masks, 0), paddings, outputs) # (h*N, T_q, T_k)\n",
    "  \n",
    "        # Activation\n",
    "        outputs = tf.nn.softmax(outputs) # (h*N, T_q, T_k)\n",
    "        matt    = outputs\n",
    "        \n",
    "        \n",
    "        # Query Masking\n",
    "        query_masks = tf.sign(tf.abs(tf.reduce_sum(queries, axis=-1))) # (N, T_q)\n",
    "        query_masks = tf.tile(query_masks, [num_heads, 1]) # (h*N, T_q)\n",
    "        query_masks = tf.tile(tf.expand_dims(query_masks, -1), [1, 1, tf.shape(keys)[1]]) # (h*N, T_q, T_k)\n",
    "        outputs *= query_masks # broadcasting. (N, T_q, C)\n",
    "          \n",
    "        # Dropouts\n",
    "        outputs = tf.contrib.layers.dropout(outputs, keep_prob=dropout_rate, is_training=tf.convert_to_tensor(is_training))\n",
    "               \n",
    "        # Weighted sum\n",
    "        outputs = tf.matmul(outputs, V_) # ( h*N, T_q, C/h)\n",
    "        \n",
    "        # Restore shape\n",
    "        outputs = tf.concat(tf.split(outputs, num_heads,axis = 0),axis =2 ) # (N, T_q, C)\n",
    "              \n",
    "        # Residual connection\n",
    "        outputs += queries\n",
    "              \n",
    "        # Normalize\n",
    "        outputs = normalize(outputs) # (N, T_q, C)\n",
    " \n",
    "    return outputs, matt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "\n",
    "    # tf Graph input\n",
    "    x   = tf.placeholder(\"float\", [None, n_frames , n_input])\n",
    "    x2  = tf.placeholder(\"float\", [None, n_frames , n_input])\n",
    "    x3  = tf.placeholder(\"float\", [None, n_frames , n_input])\n",
    "    y   = tf.placeholder(\"float\", [None, n_classes])\n",
    "    # Define weights\n",
    "    weights = {\n",
    "        'out'    : tf.Variable(tf.random_normal([n_hidden, n_classes], mean=0.0, stddev=0.01)),\n",
    "        'att_wk' : tf.Variable(tf.random_normal([8,n_input/8, n_input/8], mean=0.0, stddev=0.02),trainable=True),\n",
    "        'att_wk2': tf.Variable(tf.random_normal([8,n_input/8, n_input/8], mean=0.0, stddev=0.02),trainable=True),\n",
    "        'att_wk3': tf.Variable(tf.random_normal([8,n_input/8, n_input/8], mean=0.0, stddev=0.02),trainable=True)\n",
    "\n",
    "    }\n",
    "    biases = {\n",
    "        'out': tf.Variable(tf.random_normal([n_classes], mean=0.0, stddev=0.01))\n",
    "\n",
    "\n",
    "    }\n",
    "    # Attention Weighting\n",
    "#     m1 = tf.get_variable('m1',dtype=tf.float32, shape = [1])\n",
    "#     m2 = tf.get_variable('m2',dtype=tf.float32, shape = [1])\n",
    "#     m3 = tf.get_variable('m3',dtype=tf.float32, shape = [1])\n",
    "#     m1 = tf.sigmoid(m1)\n",
    "#     m2 = tf.sigmoid(m2)\n",
    "#     m3 = tf.sigmoid(m3)\n",
    "    # init loss \n",
    "    loss = 0.0  \n",
    "    # Mask \n",
    "    regularizer = tf.contrib.layers.l1_regularizer(scale=0.0005)\n",
    "    reg_term    = tf.contrib.layers.apply_regularization(regularizer,[weights['att_wk']])\n",
    "    # Start creat graph\n",
    "    X  = tf.reshape(x,(1,n_frames,n_input))\n",
    "    X2 = tf.reshape(x2,(1,n_frames,n_input))\n",
    "    X3 = tf.reshape(x3,(1,n_frames,n_input))\n",
    "            \n",
    "    # 1 rank approximation of attention weighting  \n",
    "    wkeys1 = weights['att_wk']\n",
    "    wkeys2 = weights['att_wk2']\n",
    "    wkeys3 = weights['att_wk3']\n",
    "        \n",
    "        \n",
    "    # Multi attention   \n",
    "    multi_att, matt =  multihead_attention(queries=X, keys=X, wkeys = wkeys1, num_units=n_input, \n",
    "                                                        num_heads=8, \n",
    "                                                        dropout_rate=0.9,\n",
    "                                                        is_training=1,\n",
    "                                                        causality=False, \n",
    "                                                        scope=\"self_attention_1\")\n",
    "    multi_att2, _ =  multihead_attention(queries=X2, keys=X2, wkeys = wkeys2, num_units=n_input, \n",
    "                                                        num_heads=8, \n",
    "                                                        dropout_rate=0.9,\n",
    "                                                        is_training=1,\n",
    "                                                        causality=False, \n",
    "                                                        scope=\"self_attention_2\")\n",
    "    multi_att3, _ =  multihead_attention(queries=X3, keys=X3, wkeys = wkeys3, num_units=n_input, \n",
    "                                                        num_heads=8, \n",
    "                                                        dropout_rate=0.9,\n",
    "                                                        is_training=1,\n",
    "                                                        causality=False, \n",
    "                                                        scope=\"self_attention_3\")\n",
    "    \n",
    "    multi_res = multi_att + multi_att2 + multi_att3 \n",
    "    \n",
    "    with tf.variable_scope(\"fc_1\") as fc_1:\n",
    "            fc_1 = tf.contrib.layers.fully_connected(tf.reshape(multi_res,(n_frames,1*n_hidden)), 3, activation_fn=tf.nn.relu)\n",
    "            fc_1_drop = tf.nn.dropout(fc_1, 0.8)\n",
    "            fc_1_variables =  [v for v in tf.global_variables() if v.name.startswith(fc_1.name)]        \n",
    "    \n",
    "    for i in range(n_frames):                  \n",
    "        with tf.variable_scope('model',reuse=tf.AUTO_REUSE):\n",
    "            \n",
    "            mul1 =  tf.reshape(multi_att[0,i,:],(1,1*n_hidden))\n",
    "            mul2 =  tf.reshape(multi_att2[0,i,:],(1,1*n_hidden))\n",
    "            mul3 =  tf.reshape(multi_att3[0,i,:],(1,1*n_hidden))\n",
    "            m1   =  fc_1_drop[i,0]\n",
    "            m2   =  fc_1_drop[i,1]\n",
    "            m3   =  fc_1_drop[i,2]\n",
    "            m1   =  tf.sigmoid(m1)\n",
    "            m2   =  tf.sigmoid(m2)\n",
    "            m3   =  tf.sigmoid(m3)\n",
    "            multi_ahead = tf.multiply(m1,mul1) + tf.multiply(m2,mul2) + tf.multiply(m3, mul3)\n",
    "            \n",
    "#             multi_ahead = tf.reshape(multi_res[0,i,:],(1,n_hidden))\n",
    "            with tf.variable_scope(\"fc_m\") as fc_m:\n",
    "                fc_m = tf.contrib.layers.fully_connected(multi_ahead,n_hidden, activation_fn=tf.nn.relu)\n",
    "                fc_m_drop = tf.nn.dropout(fc_m, 0.8)\n",
    "                fc_m_variables =  [v for v in tf.global_variables() if v.name.startswith(fc_m.name)]   \n",
    "            \n",
    "            with tf.variable_scope(\"fc_m2\") as fc_m2:\n",
    "                fc_m2 = tf.contrib.layers.fully_connected(fc_m_drop,n_hidden, activation_fn=tf.nn.relu)\n",
    "                fc_m2_drop = tf.nn.dropout(fc_m2, 0.8)\n",
    "                fc_m_variables =  [v for v in tf.global_variables() if v.name.startswith(fc_m.name)]\n",
    "\n",
    "            # FC to output classification\n",
    "            pred = tf.matmul(fc_m2_drop , weights['out']) + biases['out']\n",
    "\n",
    "            # save the predict of each time step\n",
    "            if i == 0:\n",
    "                soft_pred = tf.reshape((tf.nn.softmax(pred)),(1,n_classes))\n",
    " \n",
    "            else:\n",
    "                temp_soft_pred = tf.reshape((tf.nn.softmax(pred)),(1,n_classes))\n",
    "                soft_pred   = tf.concat([soft_pred,temp_soft_pred],axis =0)\n",
    "\n",
    "\n",
    "\n",
    "            # negative example\n",
    "            yc = tf.reshape(y[0,:],(1,n_classes))\n",
    "            neg_loss = tf.nn.softmax_cross_entropy_with_logits(labels = yc,logits = pred) # Softmax loss\n",
    "            temp_loss = tf.reduce_mean(neg_loss)\n",
    "            loss = tf.add(loss, temp_loss)\n",
    "            #loss = tf.add(loss,reg_term)\n",
    "\n",
    "      \n",
    "    # Define loss and optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss/n_frames) # Adam Optimizer\n",
    "\n",
    "    return x,x2,x3,y,optimizer,loss,soft_pred,fc_m_variables,multi_att,matt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-4d13dc370eda>:111: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method Tensor.get_shape of <tf.Tensor 'self_attention_1/Reshape_4:0' shape=(8, 32, 32) dtype=float32>>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "x,x2,x3,y,optimizer,loss,soft_pred,fc_m_variables,multi_att,matt= build_model()\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.8)\n",
    "sess = tf.InteractiveSession(config=tf.ConfigProto(allow_soft_placement=True,gpu_options=gpu_options))\n",
    "# mkdir folder for saving model\n",
    "if os.path.isdir(save_path) == False:\n",
    "        os.mkdir(save_path)\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "# Launch the graph\n",
    "sess.run(init)\n",
    "saver = tf.train.Saver()\n",
    "#saver = tf.train.import_meta_graph(save_path +'model-25.meta')\n",
    "#saver.restore(sess,save_path+'model-25')\n",
    "epoch = 1\n",
    "epoch_loss = np.zeros((train_num,1),dtype = float)\n",
    "n_batchs = np.arange(1,train_num+1)\n",
    "np.random.shuffle(n_batchs)\n",
    "tStart_epoch = time.time()\n",
    "matt.get_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch:', 1, ' done. Loss:', 20.96615699623896)\n",
      "('Epoch Time Cost:', 56.34, 's')\n",
      "('Epoch:', 2, ' done. Loss:', 2.829602699536126)\n",
      "('Epoch Time Cost:', 55.33, 's')\n",
      "('Epoch:', 3, ' done. Loss:', 1.1719480908727917)\n",
      "('Epoch Time Cost:', 55.63, 's')\n",
      "('Epoch:', 4, ' done. Loss:', 1.435274453438173)\n",
      "('Epoch Time Cost:', 55.32, 's')\n",
      "('Epoch:', 5, ' done. Loss:', 0.618251042745361)\n",
      "('Epoch Time Cost:', 55.52, 's')\n",
      "('Epoch:', 6, ' done. Loss:', 0.9216691430704859)\n",
      "('Epoch Time Cost:', 55.49, 's')\n",
      "('Epoch:', 7, ' done. Loss:', 0.6663288922501013)\n",
      "('Epoch Time Cost:', 55.54, 's')\n",
      "('Epoch:', 8, ' done. Loss:', 0.6265547199128119)\n",
      "('Epoch Time Cost:', 55.77, 's')\n",
      "('Epoch:', 9, ' done. Loss:', 0.04991936112461406)\n",
      "('Epoch Time Cost:', 55.13, 's')\n",
      "('Epoch:', 10, ' done. Loss:', 0.826758643872159)\n",
      "('Epoch Time Cost:', 55.66, 's')\n",
      "('Epoch:', 11, ' done. Loss:', 0.7833805597700765)\n",
      "('Epoch Time Cost:', 55.7, 's')\n",
      "('Epoch:', 12, ' done. Loss:', 0.4710083683045031)\n",
      "('Epoch Time Cost:', 56.04, 's')\n",
      "('Epoch:', 13, ' done. Loss:', 0.02519331406523602)\n",
      "('Epoch Time Cost:', 55.69, 's')\n",
      "('Epoch:', 14, ' done. Loss:', 0.6074118989843912)\n",
      "('Epoch Time Cost:', 55.35, 's')\n",
      "('Epoch:', 15, ' done. Loss:', 0.03683404547315956)\n",
      "('Epoch Time Cost:', 55.6, 's')\n",
      "('Epoch:', 16, ' done. Loss:', 1.0147949356274357)\n",
      "('Epoch Time Cost:', 55.78, 's')\n",
      "('Epoch:', 17, ' done. Loss:', 0.08542223918706415)\n",
      "('Epoch Time Cost:', 55.77, 's')\n",
      "('Epoch:', 18, ' done. Loss:', 0.012193046264294743)\n",
      "('Epoch Time Cost:', 55.72, 's')\n",
      "('Epoch:', 19, ' done. Loss:', 0.005712567891611528)\n",
      "('Epoch Time Cost:', 55.6, 's')\n",
      "('Epoch:', 20, ' done. Loss:', 0.4722214564963128)\n",
      "('Epoch Time Cost:', 55.84, 's')\n",
      "('Epoch:', 21, ' done. Loss:', 0.009836084175043154)\n",
      "('Epoch Time Cost:', 56.07, 's')\n",
      "('Epoch:', 22, ' done. Loss:', 0.0054097234188354194)\n",
      "('Epoch Time Cost:', 55.62, 's')\n",
      "('Epoch:', 23, ' done. Loss:', 0.0035828289969856075)\n",
      "('Epoch Time Cost:', 55.86, 's')\n",
      "('Epoch:', 24, ' done. Loss:', 0.3450154731471134)\n",
      "('Epoch Time Cost:', 55.9, 's')\n",
      "('Epoch:', 25, ' done. Loss:', 0.0037061670268004913)\n",
      "('Epoch Time Cost:', 55.89, 's')\n",
      "('Epoch:', 26, ' done. Loss:', 0.5602183617750796)\n",
      "('Epoch Time Cost:', 55.6, 's')\n",
      "('Epoch:', 27, ' done. Loss:', 0.010162437930953212)\n",
      "('Epoch Time Cost:', 55.52, 's')\n",
      "('Epoch:', 28, ' done. Loss:', 0.004937481011338975)\n",
      "('Epoch Time Cost:', 55.7, 's')\n",
      "('Epoch:', 29, ' done. Loss:', 0.0034045456897498485)\n",
      "('Epoch Time Cost:', 55.78, 's')\n",
      "('Epoch:', 30, ' done. Loss:', 0.18012299182073258)\n",
      "('Epoch Time Cost:', 55.82, 's')\n",
      "('Epoch:', 31, ' done. Loss:', 0.602900835648223)\n",
      "('Epoch Time Cost:', 55.28, 's')\n",
      "('Epoch:', 32, ' done. Loss:', 0.011413584103412085)\n",
      "('Epoch Time Cost:', 55.91, 's')\n",
      "('Epoch:', 33, ' done. Loss:', 0.0044288808526182005)\n",
      "('Epoch Time Cost:', 55.81, 's')\n",
      "('Epoch:', 34, ' done. Loss:', 0.0017694570152163359)\n",
      "('Epoch Time Cost:', 55.7, 's')\n",
      "('Epoch:', 35, ' done. Loss:', 0.38068282143567284)\n",
      "('Epoch Time Cost:', 55.91, 's')\n",
      "('Epoch:', 36, ' done. Loss:', 0.252292471775509)\n",
      "('Epoch Time Cost:', 55.74, 's')\n",
      "('Epoch:', 37, ' done. Loss:', 0.00672485684692839)\n",
      "('Epoch Time Cost:', 55.91, 's')\n",
      "('Epoch:', 38, ' done. Loss:', 0.0037875386635397652)\n",
      "('Epoch Time Cost:', 55.81, 's')\n",
      "('Epoch:', 39, ' done. Loss:', 0.5156911588607985)\n",
      "('Epoch Time Cost:', 55.59, 's')\n",
      "('Epoch:', 40, ' done. Loss:', 0.005069302349094159)\n",
      "('Epoch Time Cost:', 55.75, 's')\n",
      "('Epoch:', 41, ' done. Loss:', 0.0018264240557978877)\n",
      "('Epoch Time Cost:', 55.99, 's')\n",
      "('Epoch:', 42, ' done. Loss:', 0.0007731311362445851)\n",
      "('Epoch Time Cost:', 55.97, 's')\n",
      "('Epoch:', 43, ' done. Loss:', 0.7908783740648925)\n",
      "('Epoch Time Cost:', 55.84, 's')\n",
      "('Epoch:', 44, ' done. Loss:', 0.004603450810754915)\n",
      "('Epoch Time Cost:', 55.91, 's')\n",
      "('Epoch:', 45, ' done. Loss:', 0.0023379682650372696)\n",
      "('Epoch Time Cost:', 55.6, 's')\n",
      "('Epoch:', 46, ' done. Loss:', 0.3536848001793259)\n",
      "('Epoch Time Cost:', 55.96, 's')\n",
      "('Epoch:', 47, ' done. Loss:', 0.005906234067718121)\n",
      "('Epoch Time Cost:', 55.54, 's')\n",
      "('Epoch:', 48, ' done. Loss:', 0.001568391070260318)\n",
      "('Epoch Time Cost:', 56.0, 's')\n",
      "('Epoch:', 49, ' done. Loss:', 0.0015517039998132849)\n",
      "('Epoch Time Cost:', 55.99, 's')\n",
      "('Epoch:', 50, ' done. Loss:', 0.6804820469924211)\n",
      "('Epoch Time Cost:', 55.7, 's')\n",
      "('Epoch:', 51, ' done. Loss:', 0.005310787070565504)\n",
      "('Epoch Time Cost:', 55.69, 's')\n",
      "('Epoch:', 52, ' done. Loss:', 0.0017832417299234356)\n",
      "('Epoch Time Cost:', 55.8, 's')\n",
      "('Epoch:', 53, ' done. Loss:', 0.0011047421091936445)\n",
      "('Epoch Time Cost:', 55.92, 's')\n",
      "('Epoch:', 54, ' done. Loss:', 0.0005119207588697117)\n",
      "('Epoch Time Cost:', 55.79, 's')\n",
      "('Epoch:', 55, ' done. Loss:', 0.4379073593828797)\n",
      "('Epoch Time Cost:', 55.77, 's')\n",
      "('Epoch:', 56, ' done. Loss:', 0.0017465565054251115)\n",
      "('Epoch Time Cost:', 56.04, 's')\n",
      "('Epoch:', 57, ' done. Loss:', 0.000768237816521864)\n",
      "('Epoch Time Cost:', 55.75, 's')\n",
      "('Epoch:', 58, ' done. Loss:', 0.0004485226229043836)\n",
      "('Epoch Time Cost:', 55.84, 's')\n",
      "('Epoch:', 59, ' done. Loss:', 0.0003912977857275177)\n",
      "('Epoch Time Cost:', 55.48, 's')\n",
      "('Epoch:', 60, ' done. Loss:', 0.39815723346478316)\n",
      "('Epoch Time Cost:', 55.81, 's')\n",
      "('Epoch:', 61, ' done. Loss:', 0.0012835605097288115)\n",
      "('Epoch Time Cost:', 55.76, 's')\n",
      "('Epoch:', 62, ' done. Loss:', 0.0004271709314126459)\n",
      "('Epoch Time Cost:', 56.07, 's')\n",
      "('Epoch:', 63, ' done. Loss:', 0.0002733257309074053)\n",
      "('Epoch Time Cost:', 55.57, 's')\n",
      "('Epoch:', 64, ' done. Loss:', 0.00014537103999709942)\n",
      "('Epoch Time Cost:', 55.89, 's')\n",
      "('Epoch:', 65, ' done. Loss:', 0.5787102332637968)\n",
      "('Epoch Time Cost:', 55.54, 's')\n",
      "('Epoch:', 66, ' done. Loss:', 0.002610783515867165)\n",
      "('Epoch Time Cost:', 55.84, 's')\n",
      "('Epoch:', 67, ' done. Loss:', 0.0007412783093826112)\n",
      "('Epoch Time Cost:', 56.11, 's')\n",
      "('Epoch:', 68, ' done. Loss:', 0.0008918440555032833)\n",
      "('Epoch Time Cost:', 55.72, 's')\n",
      "('Epoch:', 69, ' done. Loss:', 0.7415747381832662)\n",
      "('Epoch Time Cost:', 55.78, 's')\n",
      "('Epoch:', 70, ' done. Loss:', 0.0029311421173961122)\n",
      "('Epoch Time Cost:', 56.01, 's')\n",
      "('Epoch:', 71, ' done. Loss:', 0.00109331077328619)\n",
      "('Epoch Time Cost:', 55.5, 's')\n",
      "('Epoch:', 72, ' done. Loss:', 0.0006766630758040751)\n",
      "('Epoch Time Cost:', 55.65, 's')\n",
      "('Epoch:', 73, ' done. Loss:', 0.2737588432236331)\n",
      "('Epoch Time Cost:', 55.83, 's')\n",
      "('Epoch:', 74, ' done. Loss:', 0.002954919292374813)\n",
      "('Epoch Time Cost:', 55.6, 's')\n",
      "('Epoch:', 75, ' done. Loss:', 0.000939765141505914)\n",
      "('Epoch Time Cost:', 55.71, 's')\n",
      "('Epoch:', 76, ' done. Loss:', 0.6190313226253957)\n",
      "('Epoch Time Cost:', 55.75, 's')\n",
      "('Epoch:', 77, ' done. Loss:', 0.00216037490826662)\n",
      "('Epoch Time Cost:', 55.48, 's')\n",
      "('Epoch:', 78, ' done. Loss:', 0.001393588541987726)\n",
      "('Epoch Time Cost:', 55.79, 's')\n",
      "('Epoch:', 79, ' done. Loss:', 0.0004511180699142239)\n",
      "('Epoch Time Cost:', 55.77, 's')\n",
      "('Epoch:', 80, ' done. Loss:', 0.0002393444846416058)\n",
      "('Epoch Time Cost:', 55.72, 's')\n",
      "('Epoch:', 81, ' done. Loss:', 0.9570612760309276)\n",
      "('Epoch Time Cost:', 55.28, 's')\n",
      "('Epoch:', 82, ' done. Loss:', 0.0023131327253642715)\n",
      "('Epoch Time Cost:', 55.83, 's')\n",
      "('Epoch:', 83, ' done. Loss:', 0.0010874908449626567)\n",
      "('Epoch Time Cost:', 55.64, 's')\n",
      "('Epoch:', 84, ' done. Loss:', 0.0005603996916031519)\n",
      "('Epoch Time Cost:', 55.67, 's')\n",
      "('Epoch:', 85, ' done. Loss:', 0.6206631650529321)\n",
      "('Epoch Time Cost:', 55.53, 's')\n",
      "('Epoch:', 86, ' done. Loss:', 0.001280877771231095)\n",
      "('Epoch Time Cost:', 55.82, 's')\n",
      "('Epoch:', 87, ' done. Loss:', 0.0009563951537452334)\n",
      "('Epoch Time Cost:', 55.61, 's')\n",
      "('Epoch:', 88, ' done. Loss:', 0.001281400259408692)\n",
      "('Epoch Time Cost:', 55.87, 's')\n",
      "('Epoch:', 89, ' done. Loss:', 0.5771342472061818)\n",
      "('Epoch Time Cost:', 55.7, 's')\n",
      "('Epoch:', 90, ' done. Loss:', 0.025569655527460544)\n",
      "('Epoch Time Cost:', 55.59, 's')\n",
      "('Epoch:', 91, ' done. Loss:', 0.0016403851090138852)\n",
      "('Epoch Time Cost:', 56.03, 's')\n",
      "('Epoch:', 92, ' done. Loss:', 0.0007000259786738737)\n",
      "('Epoch Time Cost:', 55.8, 's')\n",
      "('Epoch:', 93, ' done. Loss:', 0.49430745160508505)\n",
      "('Epoch Time Cost:', 55.72, 's')\n",
      "('Epoch:', 94, ' done. Loss:', 0.0019960121355931215)\n",
      "('Epoch Time Cost:', 55.78, 's')\n",
      "('Epoch:', 95, ' done. Loss:', 0.000793152924278172)\n",
      "('Epoch Time Cost:', 55.58, 's')\n",
      "('Epoch:', 96, ' done. Loss:', 0.0004890189441873496)\n",
      "('Epoch Time Cost:', 55.55, 's')\n",
      "('Epoch:', 97, ' done. Loss:', 0.00022868294711117488)\n",
      "('Epoch Time Cost:', 55.75, 's')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch:', 98, ' done. Loss:', 0.35051129609369575)\n",
      "('Epoch Time Cost:', 55.75, 's')\n",
      "('Epoch:', 99, ' done. Loss:', 0.0007176268839978079)\n",
      "('Epoch Time Cost:', 55.76, 's')\n",
      "('Epoch:', 100, ' done. Loss:', 0.00024458927506541974)\n",
      "('Epoch Time Cost:', 55.68, 's')\n",
      "Optimization Finished!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./multi_model/ixmas_fusion/final_model'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keep training until reach max iterations\n",
    "# start training\n",
    "for epoch in range(n_epochs):\n",
    "#  chose batch randomly\n",
    "    epoch_loss = np.zeros((train_num,1),dtype = float)\n",
    "    n_batchs = np.arange(1,train_num+1)\n",
    "    np.random.shuffle(n_batchs)\n",
    "    tStart_epoch = time.time()\n",
    "    k = 0\n",
    "    for batch in n_batchs:\n",
    "        \n",
    "        # load features and labels\n",
    "        file_name    = video_list['video'][0][batch][0][0]\n",
    "        batch_data   = np.load(train_path   + file_name +'.npy').item()\n",
    "        batch_data2  = np.load(train_path_2 + file_name +'.npy').item()\n",
    "        batch_data3  = np.load(train_path_3 + file_name +'.npy').item()\n",
    "        batch_xs     = batch_data['feat']\n",
    "        batch_xs2    = batch_data2['feat']\n",
    "        batch_xs3    = batch_data3['feat']\n",
    "        \n",
    "        batch_ys     = batch_data['label']\n",
    "        totalframe   = batch_xs.shape\n",
    "        totalframe   = totalframe[0]\n",
    "        \n",
    "        # reshape input batch \n",
    "        batch_xc  = np.zeros((1,n_frames,batch_xs.shape[1]))\n",
    "        batch_xc2 = np.zeros((1,n_frames,batch_xs.shape[1]))\n",
    "        batch_xc3 = np.zeros((1,n_frames,batch_xs.shape[1]))\n",
    "        batch_yc  = np.zeros((1,n_frames,n_classes))                   \n",
    "        batch_xc[0,:,:]  = batch_xs\n",
    "        batch_xc2[0,:,:] = batch_xs2\n",
    "        batch_xc3[0,:,:] = batch_xs3\n",
    "        \n",
    "        # feed-forward\n",
    "        _,batch_loss = sess.run([optimizer,loss], feed_dict={x: batch_xc, x2: batch_xc2, x3: batch_xc3, y: batch_ys})\n",
    "\n",
    "        epoch_loss[batch-1] = batch_loss\n",
    "        # Debug per batch\n",
    "        k = k + 1\n",
    "\n",
    "    # print per epoch\n",
    "    print (\"Epoch:\", epoch+1, \" done. Loss:\", np.mean(epoch_loss))\n",
    "    tStop_epoch = time.time()\n",
    "    print (\"Epoch Time Cost:\", round(tStop_epoch - tStart_epoch,2), \"s\")\n",
    "    sys.stdout.flush()\n",
    "    if (epoch+1) %10 == 0:\n",
    "        saver.save(sess,save_path+\"model\", global_step = epoch+1)\n",
    "print (\"Optimization Finished!\")\n",
    "saver.save(sess, save_path+\"final_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py:1645: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./multi_model/ixmas_fusion/final_model\n",
      "INFO:tensorflow:Restoring parameters from ./multi_model/ixmas_fusion/final_model\n",
      "clare2_01_check-watch_cam0_frames_0030_0103\n",
      "('video number:', 0, ' of ', 179)\n",
      "('current mAP = ', 1.0)\n",
      "amel2_01_check-watch_cam1_frames_0076_0123\n",
      "('video number:', 1, ' of ', 179)\n",
      "('current mAP = ', 1.0)\n",
      "hedlena2_01_check-watch_cam2_frames_0052_0122\n",
      "('video number:', 2, ' of ', 179)\n",
      "('current mAP = ', 1.0)\n",
      "nicolas1_01_check-watch_cam2_frames_0050_0119\n",
      "('video number:', 3, ' of ', 179)\n",
      "('current mAP = ', 1.0)\n",
      "andreas2_01_check-watch_cam1_frames_0043_0107\n",
      "('video number:', 4, ' of ', 179)\n",
      "('current mAP = ', 1.0)\n",
      "florian3_01_check-watch_cam4_frames_0040_0090\n",
      "('video number:', 5, ' of ', 179)\n",
      "('current mAP = ', 1.0)\n",
      "clare1_01_check-watch_cam0_frames_0112_0182\n",
      "('video number:', 6, ' of ', 179)\n",
      "('current mAP = ', 0.8571428571428571)\n",
      "chiara2_01_check-watch_cam3_frames_0033_0087\n",
      "('video number:', 7, ' of ', 179)\n",
      "('current mAP = ', 0.875)\n",
      "hedlena1_01_check-watch_cam4_frames_0058_0138\n",
      "('video number:', 8, ' of ', 179)\n",
      "('current mAP = ', 0.8888888888888888)\n",
      "florian3_01_check-watch_cam3_frames_0040_0090\n",
      "('video number:', 9, ' of ', 179)\n",
      "('current mAP = ', 0.9)\n",
      "daniel2_01_check-watch_cam1_frames_0015_0080\n",
      "('video number:', 10, ' of ', 179)\n",
      "('current mAP = ', 0.9090909090909091)\n",
      "nicolas1_01_check-watch_cam4_frames_0050_0119\n",
      "('video number:', 11, ' of ', 179)\n",
      "('current mAP = ', 0.8333333333333334)\n",
      "amel3_01_check-watch_cam4_frames_0083_0138\n",
      "('video number:', 12, ' of ', 179)\n",
      "('current mAP = ', 0.8461538461538461)\n",
      "andreas2_01_check-watch_cam2_frames_0043_0107\n",
      "('video number:', 13, ' of ', 179)\n",
      "('current mAP = ', 0.8571428571428571)\n",
      "andreas3_01_check-watch_cam2_frames_0029_0097\n",
      "('video number:', 14, ' of ', 179)\n",
      "('current mAP = ', 0.8666666666666667)\n",
      "nicolas3_02_cross-arms_cam4_frames_0128_0192\n",
      "('video number:', 15, ' of ', 179)\n",
      "('current mAP = ', 0.875)\n",
      "clare2_02_cross-arms_cam0_frames_0124_0200\n",
      "('video number:', 16, ' of ', 179)\n",
      "('current mAP = ', 0.8823529411764706)\n",
      "julien2_02_cross-arms_cam4_frames_0136_0179\n",
      "('video number:', 17, ' of ', 179)\n",
      "('current mAP = ', 0.8888888888888888)\n",
      "chiara3_02_cross-arms_cam3_frames_0101_0154\n",
      "('video number:', 18, ' of ', 179)\n",
      "('current mAP = ', 0.8947368421052632)\n",
      "daniel3_02_cross-arms_cam4_frames_0098_0150\n",
      "('video number:', 19, ' of ', 179)\n",
      "('current mAP = ', 0.9)\n",
      "daniel1_02_cross-arms_cam3_frames_0111_0183\n",
      "('video number:', 20, ' of ', 179)\n",
      "('current mAP = ', 0.9047619047619048)\n",
      "andreas2_02_cross-arms_cam1_frames_0108_0178\n",
      "('video number:', 21, ' of ', 179)\n",
      "('current mAP = ', 0.9090909090909091)\n",
      "chiara1_02_cross-arms_cam2_frames_0108_0163\n",
      "('video number:', 22, ' of ', 179)\n",
      "('current mAP = ', 0.9130434782608695)\n",
      "daniel2_02_cross-arms_cam3_frames_0101_0161\n",
      "('video number:', 23, ' of ', 179)\n",
      "('current mAP = ', 0.9166666666666666)\n",
      "clare3_02_cross-arms_cam3_frames_0096_0158\n",
      "('video number:', 24, ' of ', 179)\n",
      "('current mAP = ', 0.92)\n",
      "florian1_02_cross-arms_cam3_frames_0126_0186\n",
      "('video number:', 25, ' of ', 179)\n",
      "('current mAP = ', 0.9230769230769231)\n",
      "clare1_02_cross-arms_cam3_frames_0294_0347\n",
      "('video number:', 26, ' of ', 179)\n",
      "('current mAP = ', 0.9259259259259259)\n",
      "florian3_02_cross-arms_cam3_frames_0101_0168\n",
      "('video number:', 27, ' of ', 179)\n",
      "('current mAP = ', 0.9285714285714286)\n",
      "florian1_02_cross-arms_cam1_frames_0126_0186\n",
      "('video number:', 28, ' of ', 179)\n",
      "('current mAP = ', 0.9310344827586207)\n",
      "alba1_02_cross-arms_cam2_frames_0115_0191\n",
      "('video number:', 29, ' of ', 179)\n",
      "('current mAP = ', 0.9333333333333333)\n",
      "daniel3_03_scratch-head_cam0_frames_0151_0237\n",
      "('video number:', 30, ' of ', 179)\n",
      "('current mAP = ', 0.9354838709677419)\n",
      "chiara2_03_scratch-head_cam1_frames_0145_0202\n",
      "('video number:', 31, ' of ', 179)\n",
      "('current mAP = ', 0.9375)\n",
      "nicolas3_03_scratch-head_cam3_frames_0208_0270\n",
      "('video number:', 32, ' of ', 179)\n",
      "('current mAP = ', 0.9393939393939394)\n",
      "daniel2_03_scratch-head_cam0_frames_0162_0218\n",
      "('video number:', 33, ' of ', 179)\n",
      "('current mAP = ', 0.9411764705882353)\n",
      "chiara2_03_scratch-head_cam2_frames_0145_0202\n",
      "('video number:', 34, ' of ', 179)\n",
      "('current mAP = ', 0.9428571428571428)\n",
      "florian1_03_scratch-head_cam2_frames_0187_0264\n",
      "('video number:', 35, ' of ', 179)\n",
      "('current mAP = ', 0.9444444444444444)\n",
      "andreas2_03_scratch-head_cam2_frames_0179_0238\n",
      "('video number:', 36, ' of ', 179)\n",
      "('current mAP = ', 0.9459459459459459)\n",
      "hedlena3_03_scratch-head_cam2_frames_0220_0296\n",
      "('video number:', 37, ' of ', 179)\n",
      "('current mAP = ', 0.9473684210526315)\n",
      "andreas3_03_scratch-head_cam1_frames_0150_0226\n",
      "('video number:', 38, ' of ', 179)\n",
      "('current mAP = ', 0.9487179487179487)\n",
      "daniel3_03_scratch-head_cam2_frames_0151_0237\n",
      "('video number:', 39, ' of ', 179)\n",
      "('current mAP = ', 0.95)\n",
      "andreas2_03_scratch-head_cam0_frames_0179_0238\n",
      "('video number:', 40, ' of ', 179)\n",
      "('current mAP = ', 0.9512195121951219)\n",
      "clare3_03_scratch-head_cam4_frames_0159_0238\n",
      "('video number:', 41, ' of ', 179)\n",
      "('current mAP = ', 0.9523809523809523)\n",
      "nicolas1_03_scratch-head_cam3_frames_0206_0300\n",
      "('video number:', 42, ' of ', 179)\n",
      "('current mAP = ', 0.9534883720930233)\n",
      "amel3_03_scratch-head_cam2_frames_0318_0372\n",
      "('video number:', 43, ' of ', 179)\n",
      "('current mAP = ', 0.9545454545454546)\n",
      "daniel3_03_scratch-head_cam1_frames_0151_0237\n",
      "('video number:', 44, ' of ', 179)\n",
      "('current mAP = ', 0.9555555555555556)\n",
      "clare1_04_sit-down_cam3_frames_0450_0520\n",
      "('video number:', 45, ' of ', 179)\n",
      "('current mAP = ', 0.9565217391304348)\n",
      "florian2_04_sit-down_cam4_frames_0254_0335\n",
      "('video number:', 46, ' of ', 179)\n",
      "('current mAP = ', 0.9574468085106383)\n",
      "julien1_04_sit-down_cam4_frames_0247_0315\n",
      "('video number:', 47, ' of ', 179)\n",
      "('current mAP = ', 0.9583333333333334)\n",
      "florian3_04_sit-down_cam4_frames_0239_0290\n",
      "('video number:', 48, ' of ', 179)\n",
      "('current mAP = ', 0.9591836734693877)\n",
      "daniel2_04_sit-down_cam1_frames_0233_0306\n",
      "('video number:', 49, ' of ', 179)\n",
      "('current mAP = ', 0.96)\n",
      "andreas2_04_sit-down_cam0_frames_0254_0327\n",
      "('video number:', 50, ' of ', 179)\n",
      "('current mAP = ', 0.9607843137254902)\n",
      "alba2_04_sit-down_cam4_frames_0299_0354\n",
      "('video number:', 51, ' of ', 179)\n",
      "('current mAP = ', 0.9615384615384616)\n",
      "florian1_04_sit-down_cam4_frames_0265_0352\n",
      "('video number:', 52, ' of ', 179)\n",
      "('current mAP = ', 0.9622641509433962)\n",
      "chiara2_04_sit-down_cam3_frames_0203_0261\n",
      "('video number:', 53, ' of ', 179)\n",
      "('current mAP = ', 0.9629629629629629)\n",
      "clare2_04_sit-down_cam2_frames_0283_0348\n",
      "('video number:', 54, ' of ', 179)\n",
      "('current mAP = ', 0.9636363636363636)\n",
      "chiara1_04_sit-down_cam3_frames_0247_0339\n",
      "('video number:', 55, ' of ', 179)\n",
      "('current mAP = ', 0.9642857142857143)\n",
      "florian2_04_sit-down_cam3_frames_0254_0335\n",
      "('video number:', 56, ' of ', 179)\n",
      "('current mAP = ', 0.9649122807017544)\n",
      "chiara1_04_sit-down_cam0_frames_0247_0339\n",
      "('video number:', 57, ' of ', 179)\n",
      "('current mAP = ', 0.9655172413793104)\n",
      "amel3_04_sit-down_cam4_frames_0373_0460\n",
      "('video number:', 58, ' of ', 179)\n",
      "('current mAP = ', 0.9661016949152542)\n",
      "alba3_04_sit-down_cam4_frames_0262_0338\n",
      "('video number:', 59, ' of ', 179)\n",
      "('current mAP = ', 0.9666666666666667)\n",
      "clare2_05_get-up_cam3_frames_0371_0446\n",
      "('video number:', 60, ' of ', 179)\n",
      "('current mAP = ', 0.9672131147540983)\n",
      "nicolas3_05_get-up_cam3_frames_0378_0443\n",
      "('video number:', 61, ' of ', 179)\n",
      "('current mAP = ', 0.967741935483871)\n",
      "florian1_05_get-up_cam1_frames_0353_0412\n",
      "('video number:', 62, ' of ', 179)\n",
      "('current mAP = ', 0.9682539682539683)\n",
      "nicolas3_05_get-up_cam0_frames_0378_0443\n",
      "('video number:', 63, ' of ', 179)\n",
      "('current mAP = ', 0.96875)\n",
      "nicolas3_05_get-up_cam4_frames_0378_0443\n",
      "('video number:', 64, ' of ', 179)\n",
      "('current mAP = ', 0.9692307692307692)\n",
      "julien1_05_get-up_cam0_frames_0316_0403\n",
      "('video number:', 65, ' of ', 179)\n",
      "('current mAP = ', 0.9696969696969697)\n",
      "hedlena2_05_get-up_cam2_frames_0375_0438\n",
      "('video number:', 66, ' of ', 179)\n",
      "('current mAP = ', 0.9701492537313433)\n",
      "hedlena3_05_get-up_cam2_frames_0372_0460\n",
      "('video number:', 67, ' of ', 179)\n",
      "('current mAP = ', 0.9705882352941176)\n",
      "amel1_05_get-up_cam4_frames_0441_0523\n",
      "('video number:', 68, ' of ', 179)\n",
      "('current mAP = ', 0.9710144927536232)\n",
      "andreas3_05_get-up_cam2_frames_0316_0400\n",
      "('video number:', 69, ' of ', 179)\n",
      "('current mAP = ', 0.9714285714285714)\n",
      "florian2_05_get-up_cam0_frames_0336_0393\n",
      "('video number:', 70, ' of ', 179)\n",
      "('current mAP = ', 0.971830985915493)\n",
      "alba1_05_get-up_cam1_frames_0350_0424\n",
      "('video number:', 71, ' of ', 179)\n",
      "('current mAP = ', 0.9722222222222222)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alba3_05_get-up_cam4_frames_0348_0400\n",
      "('video number:', 72, ' of ', 179)\n",
      "('current mAP = ', 0.9726027397260274)\n",
      "chiara3_05_get-up_cam4_frames_0293_0386\n",
      "('video number:', 73, ' of ', 179)\n",
      "('current mAP = ', 0.972972972972973)\n",
      "julien1_05_get-up_cam3_frames_0316_0403\n",
      "('video number:', 74, ' of ', 179)\n",
      "('current mAP = ', 0.9733333333333334)\n",
      "hedlena1_06_turn-around_cam3_frames_0501_0564\n",
      "('video number:', 75, ' of ', 179)\n",
      "('current mAP = ', 0.9736842105263158)\n",
      "chiara1_06_turn-around_cam0_frames_0439_0519\n",
      "('video number:', 76, ' of ', 179)\n",
      "('current mAP = ', 0.974025974025974)\n",
      "chiara2_06_turn-around_cam1_frames_0384_0453\n",
      "('video number:', 77, ' of ', 179)\n",
      "('current mAP = ', 0.9743589743589743)\n",
      "daniel2_06_turn-around_cam1_frames_0396_0472\n",
      "('video number:', 78, ' of ', 179)\n",
      "('current mAP = ', 0.9746835443037974)\n",
      "amel2_06_turn-around_cam1_frames_0474_0549\n",
      "('video number:', 79, ' of ', 179)\n",
      "('current mAP = ', 0.975)\n",
      "amel2_06_turn-around_cam3_frames_0474_0549\n",
      "('video number:', 80, ' of ', 179)\n",
      "('current mAP = ', 0.9753086419753086)\n",
      "julien2_06_turn-around_cam1_frames_0412_0469\n",
      "('video number:', 81, ' of ', 179)\n",
      "('current mAP = ', 0.975609756097561)\n",
      "amel1_06_turn-around_cam1_frames_0536_0617\n",
      "('video number:', 82, ' of ', 179)\n",
      "('current mAP = ', 0.9759036144578314)\n",
      "hedlena1_06_turn-around_cam4_frames_0501_0564\n",
      "('video number:', 83, ' of ', 179)\n",
      "('current mAP = ', 0.9761904761904762)\n",
      "florian1_06_turn-around_cam3_frames_0429_0497\n",
      "('video number:', 84, ' of ', 179)\n",
      "('current mAP = ', 0.9764705882352941)\n",
      "clare1_06_turn-around_cam0_frames_0738_0819\n",
      "('video number:', 85, ' of ', 179)\n",
      "('current mAP = ', 0.9767441860465116)\n",
      "andreas2_06_turn-around_cam2_frames_0449_0537\n",
      "('video number:', 86, ' of ', 179)\n",
      "('current mAP = ', 0.9770114942528736)\n",
      "julien3_06_turn-around_cam0_frames_0410_0473\n",
      "('video number:', 87, ' of ', 179)\n",
      "('current mAP = ', 0.9772727272727273)\n",
      "florian3_06_turn-around_cam2_frames_0390_0479\n",
      "('video number:', 88, ' of ', 179)\n",
      "('current mAP = ', 0.9775280898876404)\n",
      "hedlena2_06_turn-around_cam2_frames_0459_0533\n",
      "('video number:', 89, ' of ', 179)\n",
      "('current mAP = ', 0.9777777777777777)\n",
      "alba2_07_walk_cam3_frames_0591_0760\n",
      "('video number:', 90, ' of ', 179)\n",
      "('current mAP = ', 0.978021978021978)\n",
      "hedlena1_07_walk_cam0_frames_0571_0717\n",
      "('video number:', 91, ' of ', 179)\n",
      "('current mAP = ', 0.9782608695652174)\n",
      "clare3_07_walk_cam2_frames_0486_0694\n",
      "('video number:', 92, ' of ', 179)\n",
      "('current mAP = ', 0.978494623655914)\n",
      "nicolas3_07_walk_cam2_frames_0561_0738\n",
      "('video number:', 93, ' of ', 179)\n",
      "('current mAP = ', 0.9787234042553191)\n",
      "clare1_07_walk_cam4_frames_0880_1160\n",
      "('video number:', 94, ' of ', 179)\n",
      "('current mAP = ', 0.9789473684210527)\n",
      "daniel1_07_walk_cam3_frames_0491_0671\n",
      "('video number:', 95, ' of ', 179)\n",
      "('current mAP = ', 0.9791666666666666)\n",
      "julien2_07_walk_cam4_frames_0470_0587\n",
      "('video number:', 96, ' of ', 179)\n",
      "('current mAP = ', 0.979381443298969)\n",
      "chiara2_07_walk_cam2_frames_0454_0599\n",
      "('video number:', 97, ' of ', 179)\n",
      "('current mAP = ', 0.9795918367346939)\n",
      "amel3_07_walk_cam0_frames_0642_0780\n",
      "('video number:', 98, ' of ', 179)\n",
      "('current mAP = ', 0.9797979797979798)\n",
      "julien2_07_walk_cam2_frames_0470_0587\n",
      "('video number:', 99, ' of ', 179)\n",
      "('current mAP = ', 0.98)\n",
      "hedlena2_07_walk_cam2_frames_0534_0682\n",
      "('video number:', 100, ' of ', 179)\n",
      "('current mAP = ', 0.9801980198019802)\n",
      "nicolas1_07_walk_cam4_frames_0645_0789\n",
      "('video number:', 101, ' of ', 179)\n",
      "('current mAP = ', 0.9803921568627451)\n",
      "amel1_07_walk_cam3_frames_0618_0781\n",
      "('video number:', 102, ' of ', 179)\n",
      "('current mAP = ', 0.9805825242718447)\n",
      "nicolas1_07_walk_cam2_frames_0645_0789\n",
      "('video number:', 103, ' of ', 179)\n",
      "('current mAP = ', 0.9807692307692307)\n",
      "clare2_07_walk_cam2_frames_0545_0714\n",
      "('video number:', 104, ' of ', 179)\n",
      "('current mAP = ', 0.9809523809523809)\n",
      "amel2_08_wave_cam1_frames_0684_0773\n",
      "('video number:', 105, ' of ', 179)\n",
      "('current mAP = ', 0.9811320754716981)\n",
      "chiara1_08_wave_cam3_frames_0669_0704\n",
      "('video number:', 106, ' of ', 179)\n",
      "('current mAP = ', 0.9813084112149533)\n",
      "florian3_08_wave_cam0_frames_0633_0696\n",
      "('video number:', 107, ' of ', 179)\n",
      "('current mAP = ', 0.9814814814814815)\n",
      "andreas1_08_wave_cam0_frames_0773_0843\n",
      "('video number:', 108, ' of ', 179)\n",
      "('current mAP = ', 0.981651376146789)\n",
      "hedlena1_08_wave_cam3_frames_0756_0816\n",
      "('video number:', 109, ' of ', 179)\n",
      "('current mAP = ', 0.9818181818181818)\n",
      "alba2_08_wave_cam2_frames_0761_0825\n",
      "('video number:', 110, ' of ', 179)\n",
      "('current mAP = ', 0.9819819819819819)\n",
      "daniel1_08_wave_cam0_frames_0672_0727\n",
      "('video number:', 111, ' of ', 179)\n",
      "('current mAP = ', 0.9821428571428571)\n",
      "chiara2_08_wave_cam4_frames_0600_0635\n",
      "('video number:', 112, ' of ', 179)\n",
      "('current mAP = ', 0.9823008849557522)\n",
      "alba3_08_wave_cam0_frames_0661_0721\n",
      "('video number:', 113, ' of ', 179)\n",
      "('current mAP = ', 0.9824561403508771)\n",
      "florian1_08_wave_cam4_frames_0667_0726\n",
      "('video number:', 114, ' of ', 179)\n",
      "('current mAP = ', 0.9826086956521739)\n",
      "daniel3_08_wave_cam3_frames_0602_0663\n",
      "('video number:', 115, ' of ', 179)\n",
      "('current mAP = ', 0.9827586206896551)\n",
      "andreas2_08_wave_cam4_frames_0729_0788\n",
      "('video number:', 116, ' of ', 179)\n",
      "('current mAP = ', 0.9829059829059829)\n",
      "julien3_08_wave_cam2_frames_0642_0724\n",
      "('video number:', 117, ' of ', 179)\n",
      "('current mAP = ', 0.9830508474576272)\n",
      "amel2_08_wave_cam3_frames_0684_0773\n",
      "('video number:', 118, ' of ', 179)\n",
      "('current mAP = ', 0.9831932773109243)\n",
      "andreas3_08_wave_cam1_frames_0690_0758\n",
      "('video number:', 119, ' of ', 179)\n",
      "('current mAP = ', 0.9833333333333333)\n",
      "julien1_09_punch_cam1_frames_0649_0698\n",
      "('video number:', 120, ' of ', 179)\n",
      "('current mAP = ', 0.9834710743801653)\n",
      "nicolas3_09_punch_cam0_frames_0826_0869\n",
      "('video number:', 121, ' of ', 179)\n",
      "('current mAP = ', 0.9836065573770492)\n",
      "chiara2_09_punch_cam2_frames_0661_0710\n",
      "('video number:', 122, ' of ', 179)\n",
      "('current mAP = ', 0.975609756097561)\n",
      "andreas3_09_punch_cam0_frames_0759_0819\n",
      "('video number:', 123, ' of ', 179)\n",
      "('current mAP = ', 0.9758064516129032)\n",
      "andreas3_09_punch_cam1_frames_0759_0819\n",
      "('video number:', 124, ' of ', 179)\n",
      "('current mAP = ', 0.976)\n",
      "florian2_09_punch_cam0_frames_0708_0759\n",
      "('video number:', 125, ' of ', 179)\n",
      "('current mAP = ', 0.9761904761904762)\n",
      "clare3_09_punch_cam0_frames_0769_0818\n",
      "('video number:', 126, ' of ', 179)\n",
      "('current mAP = ', 0.9763779527559056)\n",
      "florian3_09_punch_cam1_frames_0697_0754\n",
      "('video number:', 127, ' of ', 179)\n",
      "('current mAP = ', 0.9765625)\n",
      "daniel1_09_punch_cam4_frames_0734_0789\n",
      "('video number:', 128, ' of ', 179)\n",
      "('current mAP = ', 0.9767441860465116)\n",
      "florian1_09_punch_cam2_frames_0727_0791\n",
      "('video number:', 129, ' of ', 179)\n",
      "('current mAP = ', 0.9769230769230769)\n",
      "clare2_09_punch_cam0_frames_0868_0919\n",
      "('video number:', 130, ' of ', 179)\n",
      "('current mAP = ', 0.9770992366412213)\n",
      "andreas1_09_punch_cam0_frames_0844_0908\n",
      "('video number:', 131, ' of ', 179)\n",
      "('current mAP = ', 0.9772727272727273)\n",
      "amel3_09_punch_cam4_frames_0870_0892\n",
      "('video number:', 132, ' of ', 179)\n",
      "('current mAP = ', 0.9774436090225563)\n",
      "hedlena3_09_punch_cam3_frames_0809_0868\n",
      "('video number:', 133, ' of ', 179)\n",
      "('current mAP = ', 0.9776119402985075)\n",
      "florian2_09_punch_cam2_frames_0708_0759\n",
      "('video number:', 134, ' of ', 179)\n",
      "('current mAP = ', 0.9777777777777777)\n",
      "daniel1_10_kick_cam3_frames_0790_0866\n",
      "('video number:', 135, ' of ', 179)\n",
      "('current mAP = ', 0.9779411764705882)\n",
      "clare1_10_kick_cam4_frames_1356_1411\n",
      "('video number:', 136, ' of ', 179)\n",
      "('current mAP = ', 0.9781021897810219)\n",
      "amel1_10_kick_cam0_frames_0918_1008\n",
      "('video number:', 137, ' of ', 179)\n",
      "('current mAP = ', 0.9782608695652174)\n",
      "amel2_10_kick_cam0_frames_0825_0892\n",
      "('video number:', 138, ' of ', 179)\n",
      "('current mAP = ', 0.9784172661870504)\n",
      "nicolas1_10_kick_cam3_frames_0914_0999\n",
      "('video number:', 139, ' of ', 179)\n",
      "('current mAP = ', 0.9785714285714285)\n",
      "amel2_10_kick_cam1_frames_0825_0892\n",
      "('video number:', 140, ' of ', 179)\n",
      "('current mAP = ', 0.9787234042553191)\n",
      "amel1_10_kick_cam1_frames_0918_1008\n",
      "('video number:', 141, ' of ', 179)\n",
      "('current mAP = ', 0.9788732394366197)\n",
      "clare1_10_kick_cam1_frames_1356_1411\n",
      "('video number:', 142, ' of ', 179)\n",
      "('current mAP = ', 0.9790209790209791)\n",
      "florian3_10_kick_cam2_frames_0755_0799\n",
      "('video number:', 143, ' of ', 179)\n",
      "('current mAP = ', 0.9791666666666666)\n",
      "florian1_10_kick_cam2_frames_0792_0850\n",
      "('video number:', 144, ' of ', 179)\n",
      "('current mAP = ', 0.9793103448275862)\n",
      "chiara3_10_kick_cam4_frames_0689_0757\n",
      "('video number:', 145, ' of ', 179)\n",
      "('current mAP = ', 0.9794520547945206)\n",
      "andreas2_10_kick_cam3_frames_0822_0899\n",
      "('video number:', 146, ' of ', 179)\n",
      "('current mAP = ', 0.9795918367346939)\n",
      "alba1_10_kick_cam1_frames_0794_0871\n",
      "('video number:', 147, ' of ', 179)\n",
      "('current mAP = ', 0.9797297297297297)\n",
      "hedlena3_10_kick_cam0_frames_0869_0953\n",
      "('video number:', 148, ' of ', 179)\n",
      "('current mAP = ', 0.9798657718120806)\n",
      "andreas3_10_kick_cam0_frames_0820_0861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('video number:', 149, ' of ', 179)\n",
      "('current mAP = ', 0.98)\n",
      "clare2_11_point_cam0_frames_1089_1146\n",
      "('video number:', 150, ' of ', 179)\n",
      "('current mAP = ', 0.9801324503311258)\n",
      "clare2_11_point_cam3_frames_1089_1146\n",
      "('video number:', 151, ' of ', 179)\n",
      "('current mAP = ', 0.9802631578947368)\n",
      "alba1_11_point_cam4_frames_0872_0967\n",
      "('video number:', 152, ' of ', 179)\n",
      "('current mAP = ', 0.9803921568627451)\n",
      "florian2_11_point_cam3_frames_0839_0888\n",
      "('video number:', 153, ' of ', 179)\n",
      "('current mAP = ', 0.9805194805194806)\n",
      "nicolas1_11_point_cam4_frames_1009_1056\n",
      "('video number:', 154, ' of ', 179)\n",
      "('current mAP = ', 0.9741935483870968)\n",
      "hedlena1_11_point_cam4_frames_1007_1069\n",
      "('video number:', 155, ' of ', 179)\n",
      "('current mAP = ', 0.9743589743589743)\n",
      "daniel3_11_point_cam0_frames_0795_0836\n",
      "('video number:', 156, ' of ', 179)\n",
      "('current mAP = ', 0.9745222929936306)\n",
      "andreas1_11_point_cam4_frames_1009_1067\n",
      "('video number:', 157, ' of ', 179)\n",
      "('current mAP = ', 0.9746835443037974)\n",
      "julien3_11_point_cam4_frames_0909_0964\n",
      "('video number:', 158, ' of ', 179)\n",
      "('current mAP = ', 0.9748427672955975)\n",
      "clare3_11_point_cam0_frames_0907_0971\n",
      "('video number:', 159, ' of ', 179)\n",
      "('current mAP = ', 0.975)\n",
      "chiara3_11_point_cam4_frames_0758_0820\n",
      "('video number:', 160, ' of ', 179)\n",
      "('current mAP = ', 0.9751552795031055)\n",
      "florian1_11_point_cam0_frames_0930_0996\n",
      "('video number:', 161, ' of ', 179)\n",
      "('current mAP = ', 0.9753086419753086)\n",
      "hedlena2_11_point_cam2_frames_0929_1001\n",
      "('video number:', 162, ' of ', 179)\n",
      "('current mAP = ', 0.9754601226993865)\n",
      "amel1_11_point_cam3_frames_1070_1152\n",
      "('video number:', 163, ' of ', 179)\n",
      "('current mAP = ', 0.975609756097561)\n",
      "andreas3_11_point_cam0_frames_0884_0940\n",
      "('video number:', 164, ' of ', 179)\n",
      "('current mAP = ', 0.9757575757575757)\n",
      "andreas1_12_pick-up_cam2_frames_1107_1169\n",
      "('video number:', 165, ' of ', 179)\n",
      "('current mAP = ', 0.9759036144578314)\n",
      "florian2_12_pick-up_cam4_frames_0909_0972\n",
      "('video number:', 166, ' of ', 179)\n",
      "('current mAP = ', 0.9760479041916168)\n",
      "nicolas3_12_pick-up_cam3_frames_1034_1098\n",
      "('video number:', 167, ' of ', 179)\n",
      "('current mAP = ', 0.9761904761904762)\n",
      "andreas3_12_pick-up_cam2_frames_0959_1014\n",
      "('video number:', 168, ' of ', 179)\n",
      "('current mAP = ', 0.9763313609467456)\n",
      "julien2_12_pick-up_cam3_frames_0832_0918\n",
      "('video number:', 169, ' of ', 179)\n",
      "('current mAP = ', 0.9764705882352941)\n",
      "daniel3_12_pick-up_cam4_frames_0884_0967\n",
      "('video number:', 170, ' of ', 179)\n",
      "('current mAP = ', 0.9766081871345029)\n",
      "julien3_12_pick-up_cam3_frames_0965_1016\n",
      "('video number:', 171, ' of ', 179)\n",
      "('current mAP = ', 0.9767441860465116)\n",
      "amel2_12_pick-up_cam4_frames_0983_1012\n",
      "('video number:', 172, ' of ', 179)\n",
      "('current mAP = ', 0.976878612716763)\n",
      "daniel1_12_pick-up_cam0_frames_0971_1030\n",
      "('video number:', 173, ' of ', 179)\n",
      "('current mAP = ', 0.9770114942528736)\n",
      "alba2_12_pick-up_cam3_frames_1137_1198\n",
      "('video number:', 174, ' of ', 179)\n",
      "('current mAP = ', 0.9771428571428571)\n",
      "amel3_12_pick-up_cam4_frames_1032_1112\n",
      "('video number:', 175, ' of ', 179)\n",
      "('current mAP = ', 0.9772727272727273)\n",
      "daniel3_12_pick-up_cam1_frames_0884_0967\n",
      "('video number:', 176, ' of ', 179)\n",
      "('current mAP = ', 0.9774011299435028)\n",
      "alba2_12_pick-up_cam1_frames_1137_1198\n",
      "('video number:', 177, ' of ', 179)\n",
      "('current mAP = ', 0.9775280898876404)\n",
      "daniel2_12_pick-up_cam2_frames_0897_0996\n",
      "('video number:', 178, ' of ', 179)\n",
      "('current mAP = ', 0.9776536312849162)\n",
      "chiara3_12_pick-up_cam1_frames_0821_0882\n",
      "('video number:', 179, ' of ', 179)\n",
      "('current mAP = ', 0.9777777777777777)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "att_res = {}\n",
    "x,x2,x3,y,optimizer,loss,soft_pred,fc_m_variables,multi_att,matt= build_model()\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.5)\n",
    "sess = tf.InteractiveSession(config=tf.ConfigProto(allow_soft_placement=True,gpu_options=gpu_options))\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# restore model\n",
    "print(default_model_path)\n",
    "saver.restore(sess, default_model_path)\n",
    "top1_list = []\n",
    "for num_batch in range(0,test_num+1): # 0, test_num = 0 : test_num-1\n",
    "    file_name = test_list['video'][0][num_batch][0][0]\n",
    "    test_all_data = np.load(test_path + file_name+'.npy').item()\n",
    "    test_all_data2 = np.load(test_path_2 + file_name+'.npy').item()\n",
    "    test_all_data3 = np.load(test_path_3 + file_name+'.npy').item()\n",
    "    print(file_name)\n",
    "    test_data = test_all_data['feat']\n",
    "    test_data2 = test_all_data2['feat']\n",
    "    test_data3 = test_all_data3['feat']\n",
    "   \n",
    "    #test_data_final = np.concatenate ((test_data,test_data_2),axis = 2)\n",
    "    test_labels = test_all_data['label']\n",
    "        \n",
    "    totalframe = test_data.shape\n",
    "    totalframe = totalframe[0]\n",
    "    att_res['name'] = file_name\n",
    "    #print(totalframe)\n",
    "    pred_list = np.zeros((n_frames,n_classes))\n",
    "    batch_xc  = np.zeros((1,n_frames,test_data.shape[1]))\n",
    "    batch_xc2 = np.zeros((1,n_frames,test_data.shape[1]))\n",
    "    batch_xc3 = np.zeros((1,n_frames,test_data.shape[1]))\n",
    "    batch_xc[0,:,:] = test_data\n",
    "    batch_xc2[0,:,:] = test_data2\n",
    "    batch_xc3[0,:,:] = test_data3\n",
    "                 \n",
    "    [pred,matte] = sess.run([soft_pred,matt], feed_dict={x: batch_xc, x2: batch_xc2, x3: batch_xc3, y:test_labels})\n",
    "   \n",
    "    pred_list    = pred\n",
    "    matt_list    = matte\n",
    "    \n",
    "\n",
    "    label_index  = np.where(test_labels==1)[1][0]\n",
    "    \n",
    "    avg_pre = np.mean(pred_list, axis=0).tolist()\n",
    "    top1 = (avg_pre.index(max(avg_pre))==label_index)\n",
    "    top1_list.append(top1)\n",
    "    att_res['pred_list'] = avg_pre\n",
    "    att_res['label'] = test_labels \n",
    "    att_res['matt'] = matt_list\n",
    "  \n",
    "    print('video number:' ,num_batch, ' of ', test_num)\n",
    "    print('current mAP = ',np.mean(top1_list))\n",
    "    \n",
    "    sio.savemat(output_path+file_name+'.mat',att_res, do_compression=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(test_labels==1)[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(top1_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
