{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/requests/__init__.py:83: RequestsDependencyWarning: Old version of cryptography ([1, 2, 3]) may cause slowdown.\n",
      "  warnings.warn(warning, RequestsDependencyWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import tensorflow as tf\n",
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import pdb\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "print(tf.__version__)\n",
    "import scipy.io as sio\n",
    "import math\n",
    "\n",
    "############### Global Parameters ###############\n",
    "# path\n",
    "video_list  = sio.loadmat('trainlist_ix2.mat')\n",
    "test_list = sio.loadmat('testlist_ix2.mat')\n",
    "\n",
    "train_path = './experiments/ixmas/rgb_data_split2/train/'\n",
    "\n",
    "train_path_2 = './experiments/ixmas/flow_data_split2/train/'\n",
    "\n",
    "test_path = './experiments/ixmas/rgb_data_split2/test/'\n",
    "\n",
    "test_path_2 = './experiments/ixmas/flow_data_split2/test/'\n",
    "\n",
    "# Model Path\n",
    "default_model_path = './multi_model/ixmas_fusionft/final_model'\n",
    "save_path = './multi_model/ixmas_fusionft/'\n",
    "\n",
    "# Output Path\n",
    "output_path = './ixmas_fusionft_res/'\n",
    "\n",
    "# Train Test Split\n",
    "train_num = video_list['video'][0].shape[0]-1\n",
    "test_num = test_list['video'][0].shape[0]-1\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([u'chiara3_01_check-watch_cam2_frames_0029_0093'], dtype='<U44')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_list ['video'][0][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Train Parameters #################\n",
    "# Parameters\n",
    "learning_rate = 0.00005\n",
    "n_epochs = 100\n",
    "batch_size = 1\n",
    "# Network Parameters\n",
    "n_input = 1024 # input dimension\n",
    "n_hidden = 1024 # hidden layer num of multi_head\n",
    "n_classes = 12 # has 51 classes\n",
    "n_frames = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_dist(A, B):\n",
    "    assert A.shape.as_list() == B.shape.as_list()\n",
    "\n",
    "    row_norms_A = tf.reduce_sum(tf.square(A), axis=1)\n",
    "    row_norms_A = tf.reshape(row_norms_A, [-1, 1])  # Column vector.\n",
    "\n",
    "    row_norms_B = tf.reduce_sum(tf.square(B), axis=1)\n",
    "    row_norms_B = tf.reshape(row_norms_B, [1, -1])  # Row vector.\n",
    "\n",
    "    return row_norms_A - 2 * tf.matmul(A, tf.transpose(B)) + row_norms_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(inputs, \n",
    "              epsilon = 1e-8,\n",
    "              scope=\"ln\",\n",
    "              reuse=None):\n",
    "    '''Applies layer normalization.\n",
    "    \n",
    "    Args:\n",
    "      inputs: A tensor with 2 or more dimensions, where the first dimension has\n",
    "        `batch_size`.\n",
    "      epsilon: A floating number. A very small number for preventing ZeroDivision Error.\n",
    "      scope: Optional scope for `variable_scope`.\n",
    "      reuse: Boolean, whether to reuse the weights of a previous layer\n",
    "        by the same name.\n",
    "      \n",
    "    Returns:\n",
    "      A tensor with the same shape and data dtype as `inputs`.\n",
    "    '''\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        inputs_shape = inputs.get_shape()\n",
    "        params_shape = inputs_shape[-1:]\n",
    "    \n",
    "        mean, variance = tf.nn.moments(inputs, [-1], keep_dims=True)\n",
    "        beta= tf.Variable(tf.zeros(params_shape))\n",
    "        gamma = tf.Variable(tf.ones(params_shape))\n",
    "        normalized = (inputs - mean) / ( (variance + epsilon) ** (.5) )\n",
    "        outputs = gamma * normalized + beta\n",
    "        \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multihead_attention(queries, \n",
    "                        keys,\n",
    "                        qf,\n",
    "                        wkeys, \n",
    "                        num_units=None, \n",
    "                        num_heads=8, \n",
    "                        dropout_rate=0,\n",
    "                        is_training=True,\n",
    "                        causality=False,\n",
    "                        trainable=True,\n",
    "                        scope=\"multihead_attention\", \n",
    "                        reuse=None):\n",
    "    '''Applies multihead attention.\n",
    "    \n",
    "    Args:\n",
    "      queries: A 3d tensor with shape of [N, T_q, C_q].\n",
    "      keys: A 3d tensor with shape of [N, T_k, C_k].\n",
    "      num_units: A scalar. Attention size.\n",
    "      dropout_rate: A floating point number.\n",
    "      is_training: Boolean. Controller of mechanism for dropout.\n",
    "      causality: Boolean. If true, units that reference the future are masked. \n",
    "      num_heads: An int. Number of heads.\n",
    "      scope: Optional scope for `variable_scope`.\n",
    "      reuse: Boolean, whether to reuse the weights of a previous layer\n",
    "        by the same name.\n",
    "        \n",
    "    Returns\n",
    "      A 3d tensor with shape of (N, T_q, C)  \n",
    "    '''\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        # Set the fall back option for num_units\n",
    "        if num_units is None:\n",
    "            num_units = queries.get_shape().as_list[-1]\n",
    "        \n",
    "        # Linear projections\n",
    "        Q = tf.contrib.layers.fully_connected(queries, num_units,trainable=trainable) # (N, T_q, C)\n",
    "        K = tf.contrib.layers.fully_connected(keys, num_units,trainable=trainable ) # (N, T_k, C)\n",
    "        V = tf.contrib.layers.fully_connected(keys, num_units,trainable=trainable) # (N, T_k, C)\n",
    "        \n",
    "        QF = tf.contrib.layers.fully_connected(qf, num_units,trainable=trainable) # (N, T_q, C)\n",
    "        KF = tf.contrib.layers.fully_connected(qf, num_units,trainable=trainable)\n",
    "        \n",
    "        Q1 = tf.reshape(Q,(1,n_frames,num_units))\n",
    "        K1 = tf.reshape(K,(1,n_frames,num_units))        \n",
    "        V1 = tf.reshape(V,(1,n_frames,num_units))\n",
    "        \n",
    "        \n",
    "        # Split and concat\n",
    "        Q_ = tf.concat(tf.split(Q1, num_heads, axis = 2),axis =0) # (h*N, T_q, C/h) \n",
    "        K_ = tf.concat(tf.split(K1, num_heads, axis = 2),axis =0) # (h*N, T_k, C/h) \n",
    "        V_ = tf.concat(tf.split(V1, num_heads, axis = 2),axis =0) # (h*N, T_k, C/h) \n",
    "        \n",
    "        \n",
    "        QF_ = tf.concat(tf.split(QF, num_heads, axis = 2),axis =0) # (h*N, T_q, C/h) \n",
    "        KF_ = tf.concat(tf.split(KF, num_heads, axis = 2),axis =0) # (h*N, T_k, C/h) \n",
    "        \n",
    "        # Multiplication\n",
    "        #outputs = tf.matmul(Q_, tf.transpose(K_, [0, 2, 1])) # (h*N, T_q, T_k)\n",
    "       # output_1 = tf.matmul(Q_, wkeys) # (h*N, T_q, T_k)\n",
    "        #outputs  = tf.matmul(output_1, tf.transpose(K_, [0, 2, 1])) # (h*N, T_q, T_k)\n",
    "        #output_2 = tf.matmul(QF, tf.transpose(KF, [0, 2, 1]))\n",
    "        output_1  = output_1 = tf.matmul(QF_, wkeys) # (h*N, T_q, T_k)\n",
    "        outputs  = tf.multiply(tf.matmul(Q_, tf.transpose(K_, [0, 2, 1])),tf.matmul(output_1, tf.transpose(KF_, [0, 2, 1])))\n",
    "        #outputs  = output_2 + outputs\n",
    "        \n",
    "        # Scale\n",
    "        outputs = outputs / (K_.get_shape().as_list()[-1] ** 0.5)\n",
    "\n",
    "                \n",
    "        # Key Masking\n",
    "        key_masks = tf.sign(tf.abs(tf.reduce_sum(keys, axis=-1))) # (N, T_k)\n",
    "        key_masks = tf.tile(key_masks, [num_heads, 1]) # (h*N, T_k)\n",
    "        key_masks = tf.tile(tf.expand_dims(key_masks, 1), [1, tf.shape(queries)[1], 1]) # (h*N, T_q, T_k)\n",
    "       # \n",
    "        paddings = tf.ones_like(outputs)*(-2**32+1)\n",
    "        outputs = tf.where(tf.equal(key_masks, 0), paddings, outputs) # (h*N, T_q, T_k)\n",
    "          \n",
    "        # Causality = Future blinding\n",
    "        if causality:\n",
    "            diag_vals = tf.ones_like(outputs[0, :, :]) # (T_q, T_k)\n",
    "            tril = tf.contrib.linalg.LinearOperatorTriL(diag_vals).to_dense() # (T_q, T_k)\n",
    "            masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(outputs)[0], 1, 1]) # (h*N, T_q, T_k)\n",
    "  # \n",
    "            paddings = tf.ones_like(masks)*(-2**32+1)\n",
    "            outputs = tf.where(tf.equal(masks, 0), paddings, outputs) # (h*N, T_q, T_k)\n",
    "  \n",
    "        # Activation\n",
    "        outputs = tf.nn.softmax(outputs) # (h*N, T_q, T_k)\n",
    "        matt    = outputs\n",
    "        \n",
    "        \n",
    "        # Query Masking\n",
    "        query_masks = tf.sign(tf.abs(tf.reduce_sum(queries, axis=-1))) # (N, T_q)\n",
    "        query_masks = tf.tile(query_masks, [num_heads, 1]) # (h*N, T_q)\n",
    "        query_masks = tf.tile(tf.expand_dims(query_masks, -1), [1, 1, tf.shape(keys)[1]]) # (h*N, T_q, T_k)\n",
    "        outputs *= query_masks # broadcasting. (N, T_q, C)\n",
    "          \n",
    "        # Dropouts\n",
    "        outputs = tf.contrib.layers.dropout(outputs, keep_prob=dropout_rate, is_training=tf.convert_to_tensor(is_training))\n",
    "               \n",
    "        # Weighted sum\n",
    "        outputs = tf.matmul(outputs, V_) # ( h*N, T_q, C/h)\n",
    "        \n",
    "        # Restore shape\n",
    "        outputs = tf.concat(tf.split(outputs, num_heads,axis = 0),axis =2 ) # (N, T_q, C)\n",
    "              \n",
    "        # Residual connection\n",
    "        outputs += queries\n",
    "              \n",
    "        # Normalize\n",
    "        outputs = normalize(outputs) # (N, T_q, C)\n",
    " \n",
    "    return outputs, matt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "\n",
    "    # tf Graph input\n",
    "    x  = tf.placeholder(\"float\", [None, n_frames , n_input])\n",
    "    y  = tf.placeholder(\"float\", [None, n_classes])\n",
    "    # Define weights\n",
    "    weights = {\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden, n_classes], mean=0.0, stddev=0.01)),\n",
    "        'att_wk': tf.Variable(tf.random_normal([8,n_input/8, n_input/8], mean=0.0, stddev=0.02),trainable=True),\n",
    "\n",
    "    }\n",
    "    biases = {\n",
    "        'out': tf.Variable(tf.random_normal([n_classes], mean=0.0, stddev=0.01))\n",
    "\n",
    "\n",
    "    }\n",
    "    # init loss \n",
    "    loss = 0.0  \n",
    "    # Mask \n",
    "    regularizer = tf.contrib.layers.l1_regularizer(scale=0.0005)\n",
    "    reg_term    = tf.contrib.layers.apply_regularization(regularizer,[weights['att_wk']])\n",
    "    # Start creat graph\n",
    "    X = tf.reshape(x,(1,n_frames,n_input))\n",
    "    for i in range(0,n_frames):\n",
    "        xs = X[0,i,:]\n",
    "        xf = tf.cast(tf.spectral.fft(tf.cast(xs,dtype=tf.complex64)),tf.float32)\n",
    "        if i == 0:\n",
    "                soft_xf      = tf.reshape(xf,(1,1,n_input)) \n",
    "        else:\n",
    "                temp_soft_xf = tf.reshape(xf,(1,1,n_input))\n",
    "                soft_xf      = tf.concat([soft_xf,temp_soft_xf],axis = 1)\n",
    "\n",
    "    # 1 rank approximation of attention weighting  \n",
    "    wkeys1 = weights['att_wk']\n",
    "        \n",
    "        \n",
    "    # Multi attention   \n",
    "    multi_att, matt =  multihead_attention(queries=X, keys=X, wkeys = wkeys1, qf = soft_xf, num_units=n_input, \n",
    "                                                        num_heads=8, \n",
    "                                                        dropout_rate=0.9,\n",
    "                                                        is_training=1,\n",
    "                                                        causality=False, \n",
    "                                                        scope=\"self_attention_1\")\n",
    "\n",
    "    for i in range(n_frames):                  \n",
    "        with tf.variable_scope('model',reuse=tf.AUTO_REUSE):\n",
    "            multi_ahead = tf.reshape(multi_att[0,i,:],(1,n_hidden))\n",
    "            with tf.variable_scope(\"fc_m\") as fc_m:\n",
    "                fc_m = tf.contrib.layers.fully_connected(multi_ahead,n_hidden, activation_fn=tf.nn.relu)\n",
    "                fc_m_drop = tf.nn.dropout(fc_m, 0.8)\n",
    "                fc_m_variables =  [v for v in tf.global_variables() if v.name.startswith(fc_m.name)]   \n",
    "            \n",
    "            with tf.variable_scope(\"fc_m2\") as fc_m2:\n",
    "                fc_m2 = tf.contrib.layers.fully_connected(fc_m_drop,n_hidden, activation_fn=tf.nn.relu)\n",
    "                fc_m2_drop = tf.nn.dropout(fc_m2, 0.8)\n",
    "                fc_m_variables =  [v for v in tf.global_variables() if v.name.startswith(fc_m.name)]\n",
    "\n",
    "            # FC to output classification\n",
    "            pred = tf.matmul(fc_m2_drop , weights['out']) + biases['out']\n",
    "\n",
    "            # save the predict of each time step\n",
    "            if i == 0:\n",
    "                soft_pred = tf.reshape((tf.nn.softmax(pred)),(1,n_classes))\n",
    " \n",
    "            else:\n",
    "                temp_soft_pred = tf.reshape((tf.nn.softmax(pred)),(1,n_classes))\n",
    "                soft_pred   = tf.concat([soft_pred,temp_soft_pred],axis =0)\n",
    "\n",
    "\n",
    "\n",
    "            # negative example\n",
    "            yc = tf.reshape(y[0,:],(1,n_classes))\n",
    "            neg_loss = tf.nn.softmax_cross_entropy_with_logits(labels = yc,logits = pred) # Softmax loss\n",
    "            temp_loss = tf.reduce_mean(neg_loss)\n",
    "            loss = tf.add(loss, temp_loss)\n",
    "            loss = tf.add(loss,reg_term)\n",
    "\n",
    "      \n",
    "    # Define loss and optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss/n_frames) # Adam Optimizer\n",
    "\n",
    "    return x,y,optimizer,loss,soft_pred,fc_m_variables,multi_att,matt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-117aaabcba61>:73: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method Tensor.get_shape of <tf.Tensor 'self_attention_1/Reshape_4:0' shape=(8, 32, 32) dtype=float32>>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "x,y,optimizer,loss,soft_pred,fc_m_variables,multi_att,matt= build_model()\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.8)\n",
    "sess = tf.InteractiveSession(config=tf.ConfigProto(allow_soft_placement=True,gpu_options=gpu_options))\n",
    "# mkdir folder for saving model\n",
    "if os.path.isdir(save_path) == False:\n",
    "        os.mkdir(save_path)\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "# Launch the graph\n",
    "sess.run(init)\n",
    "saver = tf.train.Saver()\n",
    "#saver = tf.train.import_meta_graph(save_path +'model-40.meta')\n",
    "#saver.restore(sess,save_path+'model-40')\n",
    "epoch = 1\n",
    "epoch_loss = np.zeros((train_num,1),dtype = float)\n",
    "n_batchs = np.arange(1,train_num+1)\n",
    "np.random.shuffle(n_batchs)\n",
    "tStart_epoch = time.time()\n",
    "matt.get_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch:', 1, ' done. Loss:', 23.395364987109964)\n",
      "('Epoch Time Cost:', 79.55, 's')\n",
      "('Epoch:', 2, ' done. Loss:', 4.331438839212957)\n",
      "('Epoch Time Cost:', 76.43, 's')\n",
      "('Epoch:', 3, ' done. Loss:', 2.9013527253115754)\n",
      "('Epoch Time Cost:', 76.56, 's')\n",
      "('Epoch:', 4, ' done. Loss:', 2.5325477969297165)\n",
      "('Epoch Time Cost:', 76.66, 's')\n",
      "('Epoch:', 5, ' done. Loss:', 1.956132526928818)\n",
      "('Epoch Time Cost:', 76.71, 's')\n",
      "('Epoch:', 6, ' done. Loss:', 1.2942178037530703)\n",
      "('Epoch Time Cost:', 76.38, 's')\n",
      "('Epoch:', 7, ' done. Loss:', 1.4827574499864926)\n",
      "('Epoch Time Cost:', 76.74, 's')\n",
      "('Epoch:', 8, ' done. Loss:', 0.8401899750875407)\n",
      "('Epoch Time Cost:', 76.7, 's')\n",
      "('Epoch:', 9, ' done. Loss:', 1.653526130544253)\n",
      "('Epoch Time Cost:', 76.69, 's')\n",
      "('Epoch:', 10, ' done. Loss:', 0.5733870658523059)\n",
      "('Epoch Time Cost:', 77.2, 's')\n",
      "('Epoch:', 11, ' done. Loss:', 1.2320036923908586)\n",
      "('Epoch Time Cost:', 78.6, 's')\n",
      "('Epoch:', 12, ' done. Loss:', 0.7829595203801916)\n",
      "('Epoch Time Cost:', 81.19, 's')\n",
      "('Epoch:', 13, ' done. Loss:', 1.3777297509867534)\n",
      "('Epoch Time Cost:', 81.03, 's')\n",
      "('Epoch:', 14, ' done. Loss:', 1.2007044549494144)\n",
      "('Epoch Time Cost:', 81.9, 's')\n",
      "('Epoch:', 15, ' done. Loss:', 0.5468612154922722)\n",
      "('Epoch Time Cost:', 82.51, 's')\n",
      "('Epoch:', 16, ' done. Loss:', 0.9323087171573765)\n",
      "('Epoch Time Cost:', 79.57, 's')\n",
      "('Epoch:', 17, ' done. Loss:', 0.20608770991707195)\n",
      "('Epoch Time Cost:', 79.9, 's')\n",
      "('Epoch:', 18, ' done. Loss:', 0.9032830563221423)\n",
      "('Epoch Time Cost:', 79.2, 's')\n",
      "('Epoch:', 19, ' done. Loss:', 0.6643108520582042)\n",
      "('Epoch Time Cost:', 79.68, 's')\n",
      "('Epoch:', 20, ' done. Loss:', 0.9345566113792945)\n",
      "('Epoch Time Cost:', 79.9, 's')\n",
      "('Epoch:', 21, ' done. Loss:', 0.6890865204150549)\n",
      "('Epoch Time Cost:', 80.15, 's')\n",
      "('Epoch:', 22, ' done. Loss:', 0.348947592362085)\n",
      "('Epoch Time Cost:', 80.05, 's')\n",
      "('Epoch:', 23, ' done. Loss:', 1.2610425811774058)\n",
      "('Epoch Time Cost:', 80.01, 's')\n",
      "('Epoch:', 24, ' done. Loss:', 0.2557439200736587)\n",
      "('Epoch Time Cost:', 79.82, 's')\n",
      "('Epoch:', 25, ' done. Loss:', 0.1653670375669689)\n",
      "('Epoch Time Cost:', 79.29, 's')\n",
      "('Epoch:', 26, ' done. Loss:', 0.8636399256980128)\n",
      "('Epoch Time Cost:', 78.75, 's')\n",
      "('Epoch:', 27, ' done. Loss:', 0.0392642690356616)\n",
      "('Epoch Time Cost:', 78.82, 's')\n",
      "('Epoch:', 28, ' done. Loss:', 1.579262259015336)\n",
      "('Epoch Time Cost:', 78.04, 's')\n",
      "('Epoch:', 29, ' done. Loss:', 0.185834066283678)\n",
      "('Epoch Time Cost:', 77.84, 's')\n",
      "('Epoch:', 30, ' done. Loss:', 0.7592161286028632)\n",
      "('Epoch Time Cost:', 77.85, 's')\n",
      "('Epoch:', 31, ' done. Loss:', 0.4258823230416227)\n",
      "('Epoch Time Cost:', 78.0, 's')\n",
      "('Epoch:', 32, ' done. Loss:', 0.16715140468679676)\n",
      "('Epoch Time Cost:', 79.68, 's')\n",
      "('Epoch:', 33, ' done. Loss:', 0.832413273234751)\n",
      "('Epoch Time Cost:', 79.71, 's')\n",
      "('Epoch:', 34, ' done. Loss:', 0.10795695935286825)\n",
      "('Epoch Time Cost:', 79.96, 's')\n",
      "('Epoch:', 35, ' done. Loss:', 0.9099887056877909)\n",
      "('Epoch Time Cost:', 79.79, 's')\n",
      "('Epoch:', 36, ' done. Loss:', 0.04788039488415335)\n",
      "('Epoch Time Cost:', 79.63, 's')\n",
      "('Epoch:', 37, ' done. Loss:', 1.4519459371235846)\n",
      "('Epoch Time Cost:', 79.48, 's')\n",
      "('Epoch:', 38, ' done. Loss:', 0.03867265921479541)\n",
      "('Epoch Time Cost:', 79.57, 's')\n",
      "('Epoch:', 39, ' done. Loss:', 0.023151486086957686)\n",
      "('Epoch Time Cost:', 79.8, 's')\n",
      "('Epoch:', 40, ' done. Loss:', 1.393754684109151)\n",
      "('Epoch Time Cost:', 80.09, 's')\n",
      "('Epoch:', 41, ' done. Loss:', 0.17514652140332712)\n",
      "('Epoch Time Cost:', 79.38, 's')\n",
      "('Epoch:', 42, ' done. Loss:', 0.40149785090787793)\n",
      "('Epoch Time Cost:', 79.11, 's')\n",
      "('Epoch:', 43, ' done. Loss:', 0.48226553635928293)\n",
      "('Epoch Time Cost:', 79.72, 's')\n",
      "('Epoch:', 44, ' done. Loss:', 0.949582688437937)\n",
      "('Epoch Time Cost:', 79.16, 's')\n",
      "('Epoch:', 45, ' done. Loss:', 0.04217905844455335)\n",
      "('Epoch Time Cost:', 79.2, 's')\n",
      "('Epoch:', 46, ' done. Loss:', 0.9771021346344743)\n",
      "('Epoch Time Cost:', 78.62, 's')\n",
      "('Epoch:', 47, ' done. Loss:', 0.14414632438962163)\n",
      "('Epoch Time Cost:', 79.75, 's')\n",
      "('Epoch:', 48, ' done. Loss:', 0.5986872399709038)\n",
      "('Epoch Time Cost:', 79.87, 's')\n",
      "('Epoch:', 49, ' done. Loss:', 0.02926623062854788)\n",
      "('Epoch Time Cost:', 79.55, 's')\n",
      "('Epoch:', 50, ' done. Loss:', 1.0697095010175361)\n",
      "('Epoch Time Cost:', 79.95, 's')\n",
      "('Epoch:', 51, ' done. Loss:', 0.5436430432511018)\n",
      "('Epoch Time Cost:', 79.79, 's')\n",
      "('Epoch:', 52, ' done. Loss:', 0.050886632939610786)\n",
      "('Epoch Time Cost:', 79.8, 's')\n",
      "('Epoch:', 53, ' done. Loss:', 0.9741387134791801)\n",
      "('Epoch Time Cost:', 80.06, 's')\n",
      "('Epoch:', 54, ' done. Loss:', 1.0498499045927374)\n",
      "('Epoch Time Cost:', 79.51, 's')\n",
      "('Epoch:', 55, ' done. Loss:', 0.05078402446862483)\n",
      "('Epoch Time Cost:', 80.22, 's')\n",
      "('Epoch:', 56, ' done. Loss:', 0.023955138648127868)\n",
      "('Epoch Time Cost:', 79.81, 's')\n",
      "('Epoch:', 57, ' done. Loss:', 1.3398589245242154)\n",
      "('Epoch Time Cost:', 79.96, 's')\n",
      "('Epoch:', 58, ' done. Loss:', 0.7686499148737259)\n",
      "('Epoch Time Cost:', 79.65, 's')\n",
      "('Epoch:', 59, ' done. Loss:', 0.4196581341898031)\n",
      "('Epoch Time Cost:', 79.81, 's')\n",
      "('Epoch:', 60, ' done. Loss:', 0.02244135839122545)\n",
      "('Epoch Time Cost:', 79.87, 's')\n",
      "('Epoch:', 61, ' done. Loss:', 0.6574121190044038)\n",
      "('Epoch Time Cost:', 79.84, 's')\n",
      "('Epoch:', 62, ' done. Loss:', 0.19200059596087882)\n",
      "('Epoch Time Cost:', 79.23, 's')\n",
      "('Epoch:', 63, ' done. Loss:', 0.3802661869572527)\n",
      "('Epoch Time Cost:', 79.92, 's')\n",
      "('Epoch:', 64, ' done. Loss:', 0.5593070351036059)\n",
      "('Epoch Time Cost:', 79.66, 's')\n",
      "('Epoch:', 65, ' done. Loss:', 1.215597799573027)\n",
      "('Epoch Time Cost:', 79.25, 's')\n",
      "('Epoch:', 66, ' done. Loss:', 0.05783654540751896)\n",
      "('Epoch Time Cost:', 79.71, 's')\n",
      "('Epoch:', 67, ' done. Loss:', 0.01728783426041254)\n",
      "('Epoch Time Cost:', 80.24, 's')\n",
      "('Epoch:', 68, ' done. Loss:', 0.016714937199157943)\n",
      "('Epoch Time Cost:', 79.56, 's')\n",
      "('Epoch:', 69, ' done. Loss:', 0.014493435823748036)\n",
      "('Epoch Time Cost:', 79.65, 's')\n",
      "('Epoch:', 70, ' done. Loss:', 1.8838318290322364)\n",
      "('Epoch Time Cost:', 80.16, 's')\n",
      "('Epoch:', 71, ' done. Loss:', 0.04997606399118753)\n",
      "('Epoch Time Cost:', 81.63, 's')\n",
      "('Epoch:', 72, ' done. Loss:', 0.02029911458890349)\n",
      "('Epoch Time Cost:', 80.44, 's')\n",
      "('Epoch:', 73, ' done. Loss:', 0.015253778987111428)\n",
      "('Epoch Time Cost:', 79.9, 's')\n",
      "('Epoch:', 74, ' done. Loss:', 0.6186405650338295)\n",
      "('Epoch Time Cost:', 79.98, 's')\n",
      "('Epoch:', 75, ' done. Loss:', 0.0520913844336793)\n",
      "('Epoch Time Cost:', 79.7, 's')\n",
      "('Epoch:', 76, ' done. Loss:', 1.192286344033961)\n",
      "('Epoch Time Cost:', 79.89, 's')\n",
      "('Epoch:', 77, ' done. Loss:', 0.038577235680029784)\n",
      "('Epoch Time Cost:', 79.8, 's')\n",
      "('Epoch:', 78, ' done. Loss:', 0.016196710049503193)\n",
      "('Epoch Time Cost:', 79.96, 's')\n",
      "('Epoch:', 79, ' done. Loss:', 0.0156744933325321)\n",
      "('Epoch Time Cost:', 79.73, 's')\n",
      "('Epoch:', 80, ' done. Loss:', 0.7861629335515757)\n",
      "('Epoch Time Cost:', 80.0, 's')\n",
      "('Epoch:', 81, ' done. Loss:', 0.11206201261106172)\n",
      "('Epoch Time Cost:', 79.45, 's')\n",
      "('Epoch:', 82, ' done. Loss:', 0.018725227856798036)\n",
      "('Epoch Time Cost:', 80.08, 's')\n",
      "('Epoch:', 83, ' done. Loss:', 0.014745223349727658)\n",
      "('Epoch Time Cost:', 79.75, 's')\n",
      "('Epoch:', 84, ' done. Loss:', 0.014192780725418637)\n",
      "('Epoch Time Cost:', 79.62, 's')\n",
      "('Epoch:', 85, ' done. Loss:', 1.8599797084650291)\n",
      "('Epoch Time Cost:', 80.06, 's')\n",
      "('Epoch:', 86, ' done. Loss:', 0.5797871311883763)\n",
      "('Epoch Time Cost:', 79.95, 's')\n",
      "('Epoch:', 87, ' done. Loss:', 0.02290827664474763)\n",
      "('Epoch Time Cost:', 79.97, 's')\n",
      "('Epoch:', 88, ' done. Loss:', 0.014948501861398237)\n",
      "('Epoch Time Cost:', 79.91, 's')\n",
      "('Epoch:', 89, ' done. Loss:', 0.20242887799046505)\n",
      "('Epoch Time Cost:', 79.92, 's')\n",
      "('Epoch:', 90, ' done. Loss:', 1.255054491618677)\n",
      "('Epoch Time Cost:', 80.09, 's')\n",
      "('Epoch:', 91, ' done. Loss:', 0.022028167275314973)\n",
      "('Epoch Time Cost:', 79.71, 's')\n",
      "('Epoch:', 92, ' done. Loss:', 0.02250180610152418)\n",
      "('Epoch Time Cost:', 79.98, 's')\n",
      "('Epoch:', 93, ' done. Loss:', 1.5262575251348847)\n",
      "('Epoch Time Cost:', 79.79, 's')\n",
      "('Epoch:', 94, ' done. Loss:', 0.019942523104525812)\n",
      "('Epoch Time Cost:', 79.82, 's')\n",
      "('Epoch:', 95, ' done. Loss:', 0.014799038142683773)\n",
      "('Epoch Time Cost:', 80.11, 's')\n",
      "('Epoch:', 96, ' done. Loss:', 0.014001464379181679)\n",
      "('Epoch Time Cost:', 79.66, 's')\n",
      "('Epoch:', 97, ' done. Loss:', 0.014130797506372748)\n",
      "('Epoch Time Cost:', 79.84, 's')\n",
      "('Epoch:', 98, ' done. Loss:', 1.4020889295412342)\n",
      "('Epoch Time Cost:', 79.68, 's')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch:', 99, ' done. Loss:', 0.01766487509196301)\n",
      "('Epoch Time Cost:', 79.73, 's')\n",
      "('Epoch:', 100, ' done. Loss:', 0.01580293310795801)\n",
      "('Epoch Time Cost:', 80.05, 's')\n",
      "Optimization Finished!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./multi_model/ixmas_fusionft/final_model'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keep training until reach max iterations\n",
    "# start training\n",
    "for epoch in range(n_epochs):\n",
    "#  chose batch randomly\n",
    "    epoch_loss = np.zeros((train_num,1),dtype = float)\n",
    "    n_batchs = np.arange(1,train_num+1)\n",
    "    np.random.shuffle(n_batchs)\n",
    "    tStart_epoch = time.time()\n",
    "    k = 0\n",
    "    for batch in n_batchs:\n",
    "        \n",
    "        # load features and labels\n",
    "        file_name = video_list['video'][0][batch][0][0]\n",
    "        batch_data = np.load(train_path_2 + file_name +'.npy').item()\n",
    "        batch_xs = batch_data['feat']\n",
    "        \n",
    "        batch_ys = batch_data['label']\n",
    "        totalframe = batch_xs.shape\n",
    "        totalframe = totalframe[0]\n",
    "        \n",
    "        # reshape input batch \n",
    "        batch_xc = np.zeros((1,n_frames,batch_xs.shape[1]))\n",
    "\n",
    "        batch_yc = np.zeros((1,n_frames,n_classes))                   \n",
    "        batch_xc[0,:,:] = batch_xs\n",
    "  \n",
    "        batch_zc = np.zeros((1,n_frames,2))\n",
    "        \n",
    "        # feed-forward\n",
    "        _,batch_loss = sess.run([optimizer,loss], feed_dict={x: batch_xc, y: batch_ys})\n",
    "\n",
    "        epoch_loss[batch-1] = batch_loss\n",
    "        # Debug per batch\n",
    "        k = k + 1\n",
    "\n",
    "    # print per epoch\n",
    "    print (\"Epoch:\", epoch+1, \" done. Loss:\", np.mean(epoch_loss))\n",
    "    tStop_epoch = time.time()\n",
    "    print (\"Epoch Time Cost:\", round(tStop_epoch - tStart_epoch,2), \"s\")\n",
    "    sys.stdout.flush()\n",
    "    if (epoch+1) %10 == 0:\n",
    "        saver.save(sess,save_path+\"model\", global_step = epoch+1)\n",
    "print (\"Optimization Finished!\")\n",
    "saver.save(sess, save_path+\"final_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py:1645: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./multi_model/ixmas_fusionft/final_model\n",
      "INFO:tensorflow:Restoring parameters from ./multi_model/ixmas_fusionft/final_model\n",
      "clare2_01_check-watch_cam0_frames_0030_0103\n",
      "('video number:', 0, ' of ', 179)\n",
      "('current mAP = ', 1.0)\n",
      "amel2_01_check-watch_cam1_frames_0076_0123\n",
      "('video number:', 1, ' of ', 179)\n",
      "('current mAP = ', 1.0)\n",
      "hedlena2_01_check-watch_cam2_frames_0052_0122\n",
      "('video number:', 2, ' of ', 179)\n",
      "('current mAP = ', 1.0)\n",
      "nicolas1_01_check-watch_cam2_frames_0050_0119\n",
      "('video number:', 3, ' of ', 179)\n",
      "('current mAP = ', 1.0)\n",
      "andreas2_01_check-watch_cam1_frames_0043_0107\n",
      "('video number:', 4, ' of ', 179)\n",
      "('current mAP = ', 1.0)\n",
      "florian3_01_check-watch_cam4_frames_0040_0090\n",
      "('video number:', 5, ' of ', 179)\n",
      "('current mAP = ', 1.0)\n",
      "clare1_01_check-watch_cam0_frames_0112_0182\n",
      "('video number:', 6, ' of ', 179)\n",
      "('current mAP = ', 0.8571428571428571)\n",
      "chiara2_01_check-watch_cam3_frames_0033_0087\n",
      "('video number:', 7, ' of ', 179)\n",
      "('current mAP = ', 0.875)\n",
      "hedlena1_01_check-watch_cam4_frames_0058_0138\n",
      "('video number:', 8, ' of ', 179)\n",
      "('current mAP = ', 0.8888888888888888)\n",
      "florian3_01_check-watch_cam3_frames_0040_0090\n",
      "('video number:', 9, ' of ', 179)\n",
      "('current mAP = ', 0.9)\n",
      "daniel2_01_check-watch_cam1_frames_0015_0080\n",
      "('video number:', 10, ' of ', 179)\n",
      "('current mAP = ', 0.9090909090909091)\n",
      "nicolas1_01_check-watch_cam4_frames_0050_0119\n",
      "('video number:', 11, ' of ', 179)\n",
      "('current mAP = ', 0.9166666666666666)\n",
      "amel3_01_check-watch_cam4_frames_0083_0138\n",
      "('video number:', 12, ' of ', 179)\n",
      "('current mAP = ', 0.9230769230769231)\n",
      "andreas2_01_check-watch_cam2_frames_0043_0107\n",
      "('video number:', 13, ' of ', 179)\n",
      "('current mAP = ', 0.9285714285714286)\n",
      "andreas3_01_check-watch_cam2_frames_0029_0097\n",
      "('video number:', 14, ' of ', 179)\n",
      "('current mAP = ', 0.9333333333333333)\n",
      "nicolas3_02_cross-arms_cam4_frames_0128_0192\n",
      "('video number:', 15, ' of ', 179)\n",
      "('current mAP = ', 0.9375)\n",
      "clare2_02_cross-arms_cam0_frames_0124_0200\n",
      "('video number:', 16, ' of ', 179)\n",
      "('current mAP = ', 0.9411764705882353)\n",
      "julien2_02_cross-arms_cam4_frames_0136_0179\n",
      "('video number:', 17, ' of ', 179)\n",
      "('current mAP = ', 0.9444444444444444)\n",
      "chiara3_02_cross-arms_cam3_frames_0101_0154\n",
      "('video number:', 18, ' of ', 179)\n",
      "('current mAP = ', 0.9473684210526315)\n",
      "daniel3_02_cross-arms_cam4_frames_0098_0150\n",
      "('video number:', 19, ' of ', 179)\n",
      "('current mAP = ', 0.95)\n",
      "daniel1_02_cross-arms_cam3_frames_0111_0183\n",
      "('video number:', 20, ' of ', 179)\n",
      "('current mAP = ', 0.9523809523809523)\n",
      "andreas2_02_cross-arms_cam1_frames_0108_0178\n",
      "('video number:', 21, ' of ', 179)\n",
      "('current mAP = ', 0.9545454545454546)\n",
      "chiara1_02_cross-arms_cam2_frames_0108_0163\n",
      "('video number:', 22, ' of ', 179)\n",
      "('current mAP = ', 0.9565217391304348)\n",
      "daniel2_02_cross-arms_cam3_frames_0101_0161\n",
      "('video number:', 23, ' of ', 179)\n",
      "('current mAP = ', 0.9583333333333334)\n",
      "clare3_02_cross-arms_cam3_frames_0096_0158\n",
      "('video number:', 24, ' of ', 179)\n",
      "('current mAP = ', 0.96)\n",
      "florian1_02_cross-arms_cam3_frames_0126_0186\n",
      "('video number:', 25, ' of ', 179)\n",
      "('current mAP = ', 0.9615384615384616)\n",
      "clare1_02_cross-arms_cam3_frames_0294_0347\n",
      "('video number:', 26, ' of ', 179)\n",
      "('current mAP = ', 0.9629629629629629)\n",
      "florian3_02_cross-arms_cam3_frames_0101_0168\n",
      "('video number:', 27, ' of ', 179)\n",
      "('current mAP = ', 0.9642857142857143)\n",
      "florian1_02_cross-arms_cam1_frames_0126_0186\n",
      "('video number:', 28, ' of ', 179)\n",
      "('current mAP = ', 0.9655172413793104)\n",
      "alba1_02_cross-arms_cam2_frames_0115_0191\n",
      "('video number:', 29, ' of ', 179)\n",
      "('current mAP = ', 0.9666666666666667)\n",
      "daniel3_03_scratch-head_cam0_frames_0151_0237\n",
      "('video number:', 30, ' of ', 179)\n",
      "('current mAP = ', 0.967741935483871)\n",
      "chiara2_03_scratch-head_cam1_frames_0145_0202\n",
      "('video number:', 31, ' of ', 179)\n",
      "('current mAP = ', 0.96875)\n",
      "nicolas3_03_scratch-head_cam3_frames_0208_0270\n",
      "('video number:', 32, ' of ', 179)\n",
      "('current mAP = ', 0.9696969696969697)\n",
      "daniel2_03_scratch-head_cam0_frames_0162_0218\n",
      "('video number:', 33, ' of ', 179)\n",
      "('current mAP = ', 0.9705882352941176)\n",
      "chiara2_03_scratch-head_cam2_frames_0145_0202\n",
      "('video number:', 34, ' of ', 179)\n",
      "('current mAP = ', 0.9714285714285714)\n",
      "florian1_03_scratch-head_cam2_frames_0187_0264\n",
      "('video number:', 35, ' of ', 179)\n",
      "('current mAP = ', 0.9722222222222222)\n",
      "andreas2_03_scratch-head_cam2_frames_0179_0238\n",
      "('video number:', 36, ' of ', 179)\n",
      "('current mAP = ', 0.972972972972973)\n",
      "hedlena3_03_scratch-head_cam2_frames_0220_0296\n",
      "('video number:', 37, ' of ', 179)\n",
      "('current mAP = ', 0.9736842105263158)\n",
      "andreas3_03_scratch-head_cam1_frames_0150_0226\n",
      "('video number:', 38, ' of ', 179)\n",
      "('current mAP = ', 0.9743589743589743)\n",
      "daniel3_03_scratch-head_cam2_frames_0151_0237\n",
      "('video number:', 39, ' of ', 179)\n",
      "('current mAP = ', 0.975)\n",
      "andreas2_03_scratch-head_cam0_frames_0179_0238\n",
      "('video number:', 40, ' of ', 179)\n",
      "('current mAP = ', 0.975609756097561)\n",
      "clare3_03_scratch-head_cam4_frames_0159_0238\n",
      "('video number:', 41, ' of ', 179)\n",
      "('current mAP = ', 0.9761904761904762)\n",
      "nicolas1_03_scratch-head_cam3_frames_0206_0300\n",
      "('video number:', 42, ' of ', 179)\n",
      "('current mAP = ', 0.9767441860465116)\n",
      "amel3_03_scratch-head_cam2_frames_0318_0372\n",
      "('video number:', 43, ' of ', 179)\n",
      "('current mAP = ', 0.9772727272727273)\n",
      "daniel3_03_scratch-head_cam1_frames_0151_0237\n",
      "('video number:', 44, ' of ', 179)\n",
      "('current mAP = ', 0.9777777777777777)\n",
      "clare1_04_sit-down_cam3_frames_0450_0520\n",
      "('video number:', 45, ' of ', 179)\n",
      "('current mAP = ', 0.9782608695652174)\n",
      "florian2_04_sit-down_cam4_frames_0254_0335\n",
      "('video number:', 46, ' of ', 179)\n",
      "('current mAP = ', 0.9787234042553191)\n",
      "julien1_04_sit-down_cam4_frames_0247_0315\n",
      "('video number:', 47, ' of ', 179)\n",
      "('current mAP = ', 0.9791666666666666)\n",
      "florian3_04_sit-down_cam4_frames_0239_0290\n",
      "('video number:', 48, ' of ', 179)\n",
      "('current mAP = ', 0.9795918367346939)\n",
      "daniel2_04_sit-down_cam1_frames_0233_0306\n",
      "('video number:', 49, ' of ', 179)\n",
      "('current mAP = ', 0.98)\n",
      "andreas2_04_sit-down_cam0_frames_0254_0327\n",
      "('video number:', 50, ' of ', 179)\n",
      "('current mAP = ', 0.9803921568627451)\n",
      "alba2_04_sit-down_cam4_frames_0299_0354\n",
      "('video number:', 51, ' of ', 179)\n",
      "('current mAP = ', 0.9807692307692307)\n",
      "florian1_04_sit-down_cam4_frames_0265_0352\n",
      "('video number:', 52, ' of ', 179)\n",
      "('current mAP = ', 0.9811320754716981)\n",
      "chiara2_04_sit-down_cam3_frames_0203_0261\n",
      "('video number:', 53, ' of ', 179)\n",
      "('current mAP = ', 0.9814814814814815)\n",
      "clare2_04_sit-down_cam2_frames_0283_0348\n",
      "('video number:', 54, ' of ', 179)\n",
      "('current mAP = ', 0.9818181818181818)\n",
      "chiara1_04_sit-down_cam3_frames_0247_0339\n",
      "('video number:', 55, ' of ', 179)\n",
      "('current mAP = ', 0.9821428571428571)\n",
      "florian2_04_sit-down_cam3_frames_0254_0335\n",
      "('video number:', 56, ' of ', 179)\n",
      "('current mAP = ', 0.9824561403508771)\n",
      "chiara1_04_sit-down_cam0_frames_0247_0339\n",
      "('video number:', 57, ' of ', 179)\n",
      "('current mAP = ', 0.9827586206896551)\n",
      "amel3_04_sit-down_cam4_frames_0373_0460\n",
      "('video number:', 58, ' of ', 179)\n",
      "('current mAP = ', 0.9830508474576272)\n",
      "alba3_04_sit-down_cam4_frames_0262_0338\n",
      "('video number:', 59, ' of ', 179)\n",
      "('current mAP = ', 0.9833333333333333)\n",
      "clare2_05_get-up_cam3_frames_0371_0446\n",
      "('video number:', 60, ' of ', 179)\n",
      "('current mAP = ', 0.9836065573770492)\n",
      "nicolas3_05_get-up_cam3_frames_0378_0443\n",
      "('video number:', 61, ' of ', 179)\n",
      "('current mAP = ', 0.9838709677419355)\n",
      "florian1_05_get-up_cam1_frames_0353_0412\n",
      "('video number:', 62, ' of ', 179)\n",
      "('current mAP = ', 0.9841269841269841)\n",
      "nicolas3_05_get-up_cam0_frames_0378_0443\n",
      "('video number:', 63, ' of ', 179)\n",
      "('current mAP = ', 0.984375)\n",
      "nicolas3_05_get-up_cam4_frames_0378_0443\n",
      "('video number:', 64, ' of ', 179)\n",
      "('current mAP = ', 0.9692307692307692)\n",
      "julien1_05_get-up_cam0_frames_0316_0403\n",
      "('video number:', 65, ' of ', 179)\n",
      "('current mAP = ', 0.9696969696969697)\n",
      "hedlena2_05_get-up_cam2_frames_0375_0438\n",
      "('video number:', 66, ' of ', 179)\n",
      "('current mAP = ', 0.9701492537313433)\n",
      "hedlena3_05_get-up_cam2_frames_0372_0460\n",
      "('video number:', 67, ' of ', 179)\n",
      "('current mAP = ', 0.9705882352941176)\n",
      "amel1_05_get-up_cam4_frames_0441_0523\n",
      "('video number:', 68, ' of ', 179)\n",
      "('current mAP = ', 0.9710144927536232)\n",
      "andreas3_05_get-up_cam2_frames_0316_0400\n",
      "('video number:', 69, ' of ', 179)\n",
      "('current mAP = ', 0.9714285714285714)\n",
      "florian2_05_get-up_cam0_frames_0336_0393\n",
      "('video number:', 70, ' of ', 179)\n",
      "('current mAP = ', 0.971830985915493)\n",
      "alba1_05_get-up_cam1_frames_0350_0424\n",
      "('video number:', 71, ' of ', 179)\n",
      "('current mAP = ', 0.9722222222222222)\n",
      "alba3_05_get-up_cam4_frames_0348_0400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('video number:', 72, ' of ', 179)\n",
      "('current mAP = ', 0.9726027397260274)\n",
      "chiara3_05_get-up_cam4_frames_0293_0386\n",
      "('video number:', 73, ' of ', 179)\n",
      "('current mAP = ', 0.972972972972973)\n",
      "julien1_05_get-up_cam3_frames_0316_0403\n",
      "('video number:', 74, ' of ', 179)\n",
      "('current mAP = ', 0.9733333333333334)\n",
      "hedlena1_06_turn-around_cam3_frames_0501_0564\n",
      "('video number:', 75, ' of ', 179)\n",
      "('current mAP = ', 0.9736842105263158)\n",
      "chiara1_06_turn-around_cam0_frames_0439_0519\n",
      "('video number:', 76, ' of ', 179)\n",
      "('current mAP = ', 0.974025974025974)\n",
      "chiara2_06_turn-around_cam1_frames_0384_0453\n",
      "('video number:', 77, ' of ', 179)\n",
      "('current mAP = ', 0.9743589743589743)\n",
      "daniel2_06_turn-around_cam1_frames_0396_0472\n",
      "('video number:', 78, ' of ', 179)\n",
      "('current mAP = ', 0.9746835443037974)\n",
      "amel2_06_turn-around_cam1_frames_0474_0549\n",
      "('video number:', 79, ' of ', 179)\n",
      "('current mAP = ', 0.975)\n",
      "amel2_06_turn-around_cam3_frames_0474_0549\n",
      "('video number:', 80, ' of ', 179)\n",
      "('current mAP = ', 0.9753086419753086)\n",
      "julien2_06_turn-around_cam1_frames_0412_0469\n",
      "('video number:', 81, ' of ', 179)\n",
      "('current mAP = ', 0.975609756097561)\n",
      "amel1_06_turn-around_cam1_frames_0536_0617\n",
      "('video number:', 82, ' of ', 179)\n",
      "('current mAP = ', 0.9759036144578314)\n",
      "hedlena1_06_turn-around_cam4_frames_0501_0564\n",
      "('video number:', 83, ' of ', 179)\n",
      "('current mAP = ', 0.9761904761904762)\n",
      "florian1_06_turn-around_cam3_frames_0429_0497\n",
      "('video number:', 84, ' of ', 179)\n",
      "('current mAP = ', 0.9764705882352941)\n",
      "clare1_06_turn-around_cam0_frames_0738_0819\n",
      "('video number:', 85, ' of ', 179)\n",
      "('current mAP = ', 0.9767441860465116)\n",
      "andreas2_06_turn-around_cam2_frames_0449_0537\n",
      "('video number:', 86, ' of ', 179)\n",
      "('current mAP = ', 0.9770114942528736)\n",
      "julien3_06_turn-around_cam0_frames_0410_0473\n",
      "('video number:', 87, ' of ', 179)\n",
      "('current mAP = ', 0.9772727272727273)\n",
      "florian3_06_turn-around_cam2_frames_0390_0479\n",
      "('video number:', 88, ' of ', 179)\n",
      "('current mAP = ', 0.9775280898876404)\n",
      "hedlena2_06_turn-around_cam2_frames_0459_0533\n",
      "('video number:', 89, ' of ', 179)\n",
      "('current mAP = ', 0.9777777777777777)\n",
      "alba2_07_walk_cam3_frames_0591_0760\n",
      "('video number:', 90, ' of ', 179)\n",
      "('current mAP = ', 0.978021978021978)\n",
      "hedlena1_07_walk_cam0_frames_0571_0717\n",
      "('video number:', 91, ' of ', 179)\n",
      "('current mAP = ', 0.9782608695652174)\n",
      "clare3_07_walk_cam2_frames_0486_0694\n",
      "('video number:', 92, ' of ', 179)\n",
      "('current mAP = ', 0.978494623655914)\n",
      "nicolas3_07_walk_cam2_frames_0561_0738\n",
      "('video number:', 93, ' of ', 179)\n",
      "('current mAP = ', 0.9787234042553191)\n",
      "clare1_07_walk_cam4_frames_0880_1160\n",
      "('video number:', 94, ' of ', 179)\n",
      "('current mAP = ', 0.9789473684210527)\n",
      "daniel1_07_walk_cam3_frames_0491_0671\n",
      "('video number:', 95, ' of ', 179)\n",
      "('current mAP = ', 0.9791666666666666)\n",
      "julien2_07_walk_cam4_frames_0470_0587\n",
      "('video number:', 96, ' of ', 179)\n",
      "('current mAP = ', 0.979381443298969)\n",
      "chiara2_07_walk_cam2_frames_0454_0599\n",
      "('video number:', 97, ' of ', 179)\n",
      "('current mAP = ', 0.9795918367346939)\n",
      "amel3_07_walk_cam0_frames_0642_0780\n",
      "('video number:', 98, ' of ', 179)\n",
      "('current mAP = ', 0.9797979797979798)\n",
      "julien2_07_walk_cam2_frames_0470_0587\n",
      "('video number:', 99, ' of ', 179)\n",
      "('current mAP = ', 0.98)\n",
      "hedlena2_07_walk_cam2_frames_0534_0682\n",
      "('video number:', 100, ' of ', 179)\n",
      "('current mAP = ', 0.9801980198019802)\n",
      "nicolas1_07_walk_cam4_frames_0645_0789\n",
      "('video number:', 101, ' of ', 179)\n",
      "('current mAP = ', 0.9803921568627451)\n",
      "amel1_07_walk_cam3_frames_0618_0781\n",
      "('video number:', 102, ' of ', 179)\n",
      "('current mAP = ', 0.9805825242718447)\n",
      "nicolas1_07_walk_cam2_frames_0645_0789\n",
      "('video number:', 103, ' of ', 179)\n",
      "('current mAP = ', 0.9807692307692307)\n",
      "clare2_07_walk_cam2_frames_0545_0714\n",
      "('video number:', 104, ' of ', 179)\n",
      "('current mAP = ', 0.9809523809523809)\n",
      "amel2_08_wave_cam1_frames_0684_0773\n",
      "('video number:', 105, ' of ', 179)\n",
      "('current mAP = ', 0.9811320754716981)\n",
      "chiara1_08_wave_cam3_frames_0669_0704\n",
      "('video number:', 106, ' of ', 179)\n",
      "('current mAP = ', 0.9813084112149533)\n",
      "florian3_08_wave_cam0_frames_0633_0696\n",
      "('video number:', 107, ' of ', 179)\n",
      "('current mAP = ', 0.9814814814814815)\n",
      "andreas1_08_wave_cam0_frames_0773_0843\n",
      "('video number:', 108, ' of ', 179)\n",
      "('current mAP = ', 0.981651376146789)\n",
      "hedlena1_08_wave_cam3_frames_0756_0816\n",
      "('video number:', 109, ' of ', 179)\n",
      "('current mAP = ', 0.9818181818181818)\n",
      "alba2_08_wave_cam2_frames_0761_0825\n",
      "('video number:', 110, ' of ', 179)\n",
      "('current mAP = ', 0.9819819819819819)\n",
      "daniel1_08_wave_cam0_frames_0672_0727\n",
      "('video number:', 111, ' of ', 179)\n",
      "('current mAP = ', 0.9732142857142857)\n",
      "chiara2_08_wave_cam4_frames_0600_0635\n",
      "('video number:', 112, ' of ', 179)\n",
      "('current mAP = ', 0.9646017699115044)\n",
      "alba3_08_wave_cam0_frames_0661_0721\n",
      "('video number:', 113, ' of ', 179)\n",
      "('current mAP = ', 0.9649122807017544)\n",
      "florian1_08_wave_cam4_frames_0667_0726\n",
      "('video number:', 114, ' of ', 179)\n",
      "('current mAP = ', 0.9652173913043478)\n",
      "daniel3_08_wave_cam3_frames_0602_0663\n",
      "('video number:', 115, ' of ', 179)\n",
      "('current mAP = ', 0.9655172413793104)\n",
      "andreas2_08_wave_cam4_frames_0729_0788\n",
      "('video number:', 116, ' of ', 179)\n",
      "('current mAP = ', 0.9658119658119658)\n",
      "julien3_08_wave_cam2_frames_0642_0724\n",
      "('video number:', 117, ' of ', 179)\n",
      "('current mAP = ', 0.9661016949152542)\n",
      "amel2_08_wave_cam3_frames_0684_0773\n",
      "('video number:', 118, ' of ', 179)\n",
      "('current mAP = ', 0.9663865546218487)\n",
      "andreas3_08_wave_cam1_frames_0690_0758\n",
      "('video number:', 119, ' of ', 179)\n",
      "('current mAP = ', 0.9666666666666667)\n",
      "julien1_09_punch_cam1_frames_0649_0698\n",
      "('video number:', 120, ' of ', 179)\n",
      "('current mAP = ', 0.9669421487603306)\n",
      "nicolas3_09_punch_cam0_frames_0826_0869\n",
      "('video number:', 121, ' of ', 179)\n",
      "('current mAP = ', 0.9672131147540983)\n",
      "chiara2_09_punch_cam2_frames_0661_0710\n",
      "('video number:', 122, ' of ', 179)\n",
      "('current mAP = ', 0.967479674796748)\n",
      "andreas3_09_punch_cam0_frames_0759_0819\n",
      "('video number:', 123, ' of ', 179)\n",
      "('current mAP = ', 0.967741935483871)\n",
      "andreas3_09_punch_cam1_frames_0759_0819\n",
      "('video number:', 124, ' of ', 179)\n",
      "('current mAP = ', 0.968)\n",
      "florian2_09_punch_cam0_frames_0708_0759\n",
      "('video number:', 125, ' of ', 179)\n",
      "('current mAP = ', 0.9682539682539683)\n",
      "clare3_09_punch_cam0_frames_0769_0818\n",
      "('video number:', 126, ' of ', 179)\n",
      "('current mAP = ', 0.968503937007874)\n",
      "florian3_09_punch_cam1_frames_0697_0754\n",
      "('video number:', 127, ' of ', 179)\n",
      "('current mAP = ', 0.96875)\n",
      "daniel1_09_punch_cam4_frames_0734_0789\n",
      "('video number:', 128, ' of ', 179)\n",
      "('current mAP = ', 0.9689922480620154)\n",
      "florian1_09_punch_cam2_frames_0727_0791\n",
      "('video number:', 129, ' of ', 179)\n",
      "('current mAP = ', 0.9692307692307692)\n",
      "clare2_09_punch_cam0_frames_0868_0919\n",
      "('video number:', 130, ' of ', 179)\n",
      "('current mAP = ', 0.9694656488549618)\n",
      "andreas1_09_punch_cam0_frames_0844_0908\n",
      "('video number:', 131, ' of ', 179)\n",
      "('current mAP = ', 0.9696969696969697)\n",
      "amel3_09_punch_cam4_frames_0870_0892\n",
      "('video number:', 132, ' of ', 179)\n",
      "('current mAP = ', 0.9699248120300752)\n",
      "hedlena3_09_punch_cam3_frames_0809_0868\n",
      "('video number:', 133, ' of ', 179)\n",
      "('current mAP = ', 0.9701492537313433)\n",
      "florian2_09_punch_cam2_frames_0708_0759\n",
      "('video number:', 134, ' of ', 179)\n",
      "('current mAP = ', 0.9703703703703703)\n",
      "daniel1_10_kick_cam3_frames_0790_0866\n",
      "('video number:', 135, ' of ', 179)\n",
      "('current mAP = ', 0.9705882352941176)\n",
      "clare1_10_kick_cam4_frames_1356_1411\n",
      "('video number:', 136, ' of ', 179)\n",
      "('current mAP = ', 0.9708029197080292)\n",
      "amel1_10_kick_cam0_frames_0918_1008\n",
      "('video number:', 137, ' of ', 179)\n",
      "('current mAP = ', 0.9710144927536232)\n",
      "amel2_10_kick_cam0_frames_0825_0892\n",
      "('video number:', 138, ' of ', 179)\n",
      "('current mAP = ', 0.9712230215827338)\n",
      "nicolas1_10_kick_cam3_frames_0914_0999\n",
      "('video number:', 139, ' of ', 179)\n",
      "('current mAP = ', 0.9714285714285714)\n",
      "amel2_10_kick_cam1_frames_0825_0892\n",
      "('video number:', 140, ' of ', 179)\n",
      "('current mAP = ', 0.9716312056737588)\n",
      "amel1_10_kick_cam1_frames_0918_1008\n",
      "('video number:', 141, ' of ', 179)\n",
      "('current mAP = ', 0.971830985915493)\n",
      "clare1_10_kick_cam1_frames_1356_1411\n",
      "('video number:', 142, ' of ', 179)\n",
      "('current mAP = ', 0.972027972027972)\n",
      "florian3_10_kick_cam2_frames_0755_0799\n",
      "('video number:', 143, ' of ', 179)\n",
      "('current mAP = ', 0.9722222222222222)\n",
      "florian1_10_kick_cam2_frames_0792_0850\n",
      "('video number:', 144, ' of ', 179)\n",
      "('current mAP = ', 0.9724137931034482)\n",
      "chiara3_10_kick_cam4_frames_0689_0757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('video number:', 145, ' of ', 179)\n",
      "('current mAP = ', 0.9726027397260274)\n",
      "andreas2_10_kick_cam3_frames_0822_0899\n",
      "('video number:', 146, ' of ', 179)\n",
      "('current mAP = ', 0.9727891156462585)\n",
      "alba1_10_kick_cam1_frames_0794_0871\n",
      "('video number:', 147, ' of ', 179)\n",
      "('current mAP = ', 0.972972972972973)\n",
      "hedlena3_10_kick_cam0_frames_0869_0953\n",
      "('video number:', 148, ' of ', 179)\n",
      "('current mAP = ', 0.9731543624161074)\n",
      "andreas3_10_kick_cam0_frames_0820_0861\n",
      "('video number:', 149, ' of ', 179)\n",
      "('current mAP = ', 0.9733333333333334)\n",
      "clare2_11_point_cam0_frames_1089_1146\n",
      "('video number:', 150, ' of ', 179)\n",
      "('current mAP = ', 0.9668874172185431)\n",
      "clare2_11_point_cam3_frames_1089_1146\n",
      "('video number:', 151, ' of ', 179)\n",
      "('current mAP = ', 0.9671052631578947)\n",
      "alba1_11_point_cam4_frames_0872_0967\n",
      "('video number:', 152, ' of ', 179)\n",
      "('current mAP = ', 0.9673202614379085)\n",
      "florian2_11_point_cam3_frames_0839_0888\n",
      "('video number:', 153, ' of ', 179)\n",
      "('current mAP = ', 0.9675324675324676)\n",
      "nicolas1_11_point_cam4_frames_1009_1056\n",
      "('video number:', 154, ' of ', 179)\n",
      "('current mAP = ', 0.967741935483871)\n",
      "hedlena1_11_point_cam4_frames_1007_1069\n",
      "('video number:', 155, ' of ', 179)\n",
      "('current mAP = ', 0.967948717948718)\n",
      "daniel3_11_point_cam0_frames_0795_0836\n",
      "('video number:', 156, ' of ', 179)\n",
      "('current mAP = ', 0.9681528662420382)\n",
      "andreas1_11_point_cam4_frames_1009_1067\n",
      "('video number:', 157, ' of ', 179)\n",
      "('current mAP = ', 0.9620253164556962)\n",
      "julien3_11_point_cam4_frames_0909_0964\n",
      "('video number:', 158, ' of ', 179)\n",
      "('current mAP = ', 0.9622641509433962)\n",
      "clare3_11_point_cam0_frames_0907_0971\n",
      "('video number:', 159, ' of ', 179)\n",
      "('current mAP = ', 0.9625)\n",
      "chiara3_11_point_cam4_frames_0758_0820\n",
      "('video number:', 160, ' of ', 179)\n",
      "('current mAP = ', 0.9565217391304348)\n",
      "florian1_11_point_cam0_frames_0930_0996\n",
      "('video number:', 161, ' of ', 179)\n",
      "('current mAP = ', 0.9567901234567902)\n",
      "hedlena2_11_point_cam2_frames_0929_1001\n",
      "('video number:', 162, ' of ', 179)\n",
      "('current mAP = ', 0.9570552147239264)\n",
      "amel1_11_point_cam3_frames_1070_1152\n",
      "('video number:', 163, ' of ', 179)\n",
      "('current mAP = ', 0.9573170731707317)\n",
      "andreas3_11_point_cam0_frames_0884_0940\n",
      "('video number:', 164, ' of ', 179)\n",
      "('current mAP = ', 0.9575757575757575)\n",
      "andreas1_12_pick-up_cam2_frames_1107_1169\n",
      "('video number:', 165, ' of ', 179)\n",
      "('current mAP = ', 0.9578313253012049)\n",
      "florian2_12_pick-up_cam4_frames_0909_0972\n",
      "('video number:', 166, ' of ', 179)\n",
      "('current mAP = ', 0.9580838323353293)\n",
      "nicolas3_12_pick-up_cam3_frames_1034_1098\n",
      "('video number:', 167, ' of ', 179)\n",
      "('current mAP = ', 0.9583333333333334)\n",
      "andreas3_12_pick-up_cam2_frames_0959_1014\n",
      "('video number:', 168, ' of ', 179)\n",
      "('current mAP = ', 0.9585798816568047)\n",
      "julien2_12_pick-up_cam3_frames_0832_0918\n",
      "('video number:', 169, ' of ', 179)\n",
      "('current mAP = ', 0.9588235294117647)\n",
      "daniel3_12_pick-up_cam4_frames_0884_0967\n",
      "('video number:', 170, ' of ', 179)\n",
      "('current mAP = ', 0.9590643274853801)\n",
      "julien3_12_pick-up_cam3_frames_0965_1016\n",
      "('video number:', 171, ' of ', 179)\n",
      "('current mAP = ', 0.9593023255813954)\n",
      "amel2_12_pick-up_cam4_frames_0983_1012\n",
      "('video number:', 172, ' of ', 179)\n",
      "('current mAP = ', 0.9595375722543352)\n",
      "daniel1_12_pick-up_cam0_frames_0971_1030\n",
      "('video number:', 173, ' of ', 179)\n",
      "('current mAP = ', 0.9597701149425287)\n",
      "alba2_12_pick-up_cam3_frames_1137_1198\n",
      "('video number:', 174, ' of ', 179)\n",
      "('current mAP = ', 0.96)\n",
      "amel3_12_pick-up_cam4_frames_1032_1112\n",
      "('video number:', 175, ' of ', 179)\n",
      "('current mAP = ', 0.9602272727272727)\n",
      "daniel3_12_pick-up_cam1_frames_0884_0967\n",
      "('video number:', 176, ' of ', 179)\n",
      "('current mAP = ', 0.96045197740113)\n",
      "alba2_12_pick-up_cam1_frames_1137_1198\n",
      "('video number:', 177, ' of ', 179)\n",
      "('current mAP = ', 0.9606741573033708)\n",
      "daniel2_12_pick-up_cam2_frames_0897_0996\n",
      "('video number:', 178, ' of ', 179)\n",
      "('current mAP = ', 0.9608938547486033)\n",
      "chiara3_12_pick-up_cam1_frames_0821_0882\n",
      "('video number:', 179, ' of ', 179)\n",
      "('current mAP = ', 0.9611111111111111)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "att_res = {}\n",
    "x,y,optimizer,loss,soft_pred,fc_m_variables,multi_att,matt= build_model()\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.5)\n",
    "sess = tf.InteractiveSession(config=tf.ConfigProto(allow_soft_placement=True,gpu_options=gpu_options))\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# restore model\n",
    "print(default_model_path)\n",
    "saver.restore(sess, default_model_path)\n",
    "top1_list = []\n",
    "for num_batch in range(0,test_num+1): # 0, test_num = 0 : test_num-1\n",
    "    file_name = test_list['video'][0][num_batch][0][0]\n",
    "    test_all_data = np.load(test_path_2 + file_name+'.npy').item()\n",
    "  \n",
    "    print(file_name)\n",
    "    test_data = test_all_data['feat']\n",
    "   \n",
    "    #test_data_final = np.concatenate ((test_data,test_data_2),axis = 2)\n",
    "    test_labels = test_all_data['label']\n",
    "        \n",
    "    totalframe = test_data.shape\n",
    "    totalframe = totalframe[0]\n",
    "    att_res['name'] = file_name\n",
    "    #print(totalframe)\n",
    "    pred_list= np.zeros((n_frames,n_classes))\n",
    "    batch_xc = np.zeros((1,n_frames,test_data.shape[1]))\n",
    "    batch_xc[0,:,:] = test_data\n",
    "                 \n",
    "    [pred,matte] = sess.run([soft_pred,matt], feed_dict={x: batch_xc, y:test_labels})\n",
    "   \n",
    "    pred_list    = pred\n",
    "    matt_list    = matte\n",
    "    \n",
    "\n",
    "    label_index  = np.where(test_labels==1)[1][0]\n",
    "    \n",
    "    avg_pre = np.mean(pred_list, axis=0).tolist()\n",
    "    top1 = (avg_pre.index(max(avg_pre))==label_index)\n",
    "    top1_list.append(top1)\n",
    "    att_res['pred_list'] = avg_pre\n",
    "    att_res['label'] = test_labels \n",
    "    att_res['matt'] = matt_list\n",
    "  \n",
    "    print('video number:' ,num_batch, ' of ', test_num)\n",
    "    print('current mAP = ',np.mean(top1_list))\n",
    "    \n",
    "    sio.savemat(output_path+file_name+'.mat',att_res, do_compression=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(test_labels==1)[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "173"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(top1_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
